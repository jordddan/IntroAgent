{"introduction": "Neural sequence models (Sutskever et al., 2014; Cho et al.,\n2014) have been successfully applied to many applications,\nincluding machine translation (Bahdanau et al., 2015; Lu-\nong et al., 2015), speech recognition (Bahdanau et al., 2016;\nChan et al., 2016), speech synthesis (Oord et al., 2016a;\nWang et al., 2017), image captioning (Vinyals et al., 2015b;\nXu et al., 2015) and image generation (Oord et al., 2016b;c).\nThese models have a common theme: they rely on the chain-\nrule factorization and have an autoregressive left-to-right\nstructure. This formulation bestows many advantages in\nboth training and inference. Log-likelihood computation is\ntractable, allowing for ef\ufb01cient maximum likelihood learn-\ning. Ef\ufb01cient approximate inference is also made possible\nthrough beam search decoding. However, the autoregressive\nframework does not easily accommodate for parallel token\ngeneration or more elaborate generation orderings (e.g., tree\norders).\n\nMore recently, there has been work on non-autoregressive\nsequence models such as the Non-Autoregressive Trans-\nformer (NAT) (Gu et al., 2018) and the Iterative Re\ufb01nement\nmodel (Lee et al., 2018). In both of these models, the de-\ncoder is seeded with an initial input derived from the source\nsequence, then produces the entire target sequence in paral-\nlel. Lee et al. (2018) adds an iterative re\ufb01nement stage to the\ndecoder in which a new hypothesis is produced conditioning\non the input and the previous output.\n\nWhile allowing for highly parallel generation, there are a\nfew drawbacks to such approaches. The \ufb01rst is that the target\nsequence length needs to be chosen up front, preventing the\noutput from growing dynamically as generation proceeds.\nThis can be problematic if the chosen length is too short\nto accommodate the desired target, or can be wasteful if\nit is too long.\nIn the case of Gu et al. (2018), there is\nalso a strong conditional independence assumption between\noutput tokens, limiting the model\u2019s expressive power. Lee\net al. (2018) relaxes this assumption but in turn requires two\nseparate decoders for the initial hypothesis generation and\nthe iterative re\ufb01nement stage.\n\nIn this work, we present a \ufb02exible sequence generation\nframework based on insertion operations. The Insertion\nTransformer is an iterative, partially autoregressive model\nwhich can be trained in a fully end-to-end fashion. Gen-\neration is accomplished by repeatedly making insertions\ninto an initially-empty output sequence until a termination\ncondition is met. Our approach bypasses the problem of\nneeding to predict the target sequence length ahead of time\nby allowing the output to grow dynamically, and also per-\nmits deviation from classic left-to-right generation, allowing\nfor more exotic orderings like balanced binary trees.\n\nDuring inference, the Insertion Transformer can be used\nin an autoregressive manner for serial decoding, with one\ninsertion operation being applied at a time, or in a partially\nautoregressive manner for parallel decoding, with insertions\nat multiple locations being applied simultaneously. This\nallows for the target sequence to grow exponentially in\nlength. In the case of a balanced binary tree order, our\nmodel can use as few as (cid:98)log2 n(cid:99) + 1 operations to produce\na sequence of length n, which we \ufb01nd achievable in practice\nusing an appropriately chosen loss function during training.\n", "contribution": "The main contribution of this paper is the presentation of a flexible sequence generation framework based on insertion operations, called the Insertion Transformer. This model is an iterative, partially autoregressive model that can be trained in a fully end-to-end fashion. It allows for dynamic output sequence growth and permits deviation from classic left-to-right generation, allowing for more exotic orderings like balanced binary trees. During inference, the Insertion Transformer can be used in an autoregressive or partially autoregressive manner for serial or parallel decoding, respectively. This model can use as few as (cid:98)log2 n(cid:99) + 1 operations to produce a sequence of length n, which is achievable in practice using an appropriately chosen loss function during training.", "title": "Insertion Transformer: Flexible Sequence Generation via Insertion Operations", "words": 782, "abstract": "\nWe present the Insertion Transformer, an itera-\ntive, partially autoregressive model for sequence\ngeneration based on insertion operations. Unlike\ntypical autoregressive models which rely on a\n\ufb01xed, often left-to-right ordering of the output,\nour approach accommodates arbitrary orderings\nby allowing for tokens to be inserted anywhere\nin the sequence during decoding. This \ufb02exibil-\nity confers a number of advantages: for instance,\nnot only can our model be trained to follow spe-\nci\ufb01c orderings such as left-to-right generation or\na binary tree traversal, but it can also be trained\nto maximize entropy over all valid insertions for\nrobustness. In addition, our model seamlessly ac-\ncommodates both fully autoregressive generation\n(one insertion at a time) and partially autoregres-\nsive generation (simultaneous insertions at multi-\nple locations). We validate our approach by ana-\nlyzing its performance on the WMT 2014 English-\nGerman machine translation task under various\nsettings for training and decoding. We \ufb01nd that\nthe Insertion Transformer outperforms many prior\nnon-autoregressive approaches to translation at\ncomparable or better levels of parallelism, and\nsuccessfully recovers the performance of the orig-\ninal Transformer while requiring only logarithmi-\ncally many iterations during decoding.\n"}
{"introduction": "Most state-of-the-art neural machine translation (NMT) sys- tems are autoregressive, where they predict one word after another to generate the target sequence (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017). However, autoregressive NMT is slow in inference, which sometimes does not meet the ef\ufb01ciency requirement from the industry. Gu et al. (2018) propose non-autoregressive neural ma- chine translation (NAT), which is 15.6 times faster than its autoregressive counterpart. While NAT achieves fast infer- ence by generating target tokens in parallel, it assumes that the generated words are conditionally independent given the input. Such an independence assumption, however, weakens the power of sequence modeling, and results in worse per- formance than autoregressive translation. In recent studies, researchers propose partially (non-)autoregressive models by progressively generat- ing several words at a time (Ghazvininejad et al. 2019) or iteratively editing with the Levenshtein Transformer (Gu, Wang, and Zhao 2019). However, their inference is signif- icantly slower than a full NAT model. As shown by Kasai * Work partially done during an internship at ByteDance AI Lab. 1 Our code, training/evaluation scripts, and output are available at https://github.com/chenyangh/DSLP. 0 3 6 9 12 15 6SeeduS 21 22 23 24 25 26 27 BLEU 0 3 6 9 12 15 6SeeGXS 21 22 23 24 25 26 27 B/EU AXtRUegUeVVLve 7UDnVIRUPeU InVeUtLRn 1A7 D&5F AXE /evenVhteLn 1A7 G/A7 G/A7 w/ D6/3 9DnLllD 1A7 9DnLllD 1A7 w/ D6/3 9DnLllD 1A7 w/ D6/3 & 07 &7& &7& w/ D6/3 &7& w/ D6/3 & 07 Figure 1: Quality\u2013ef\ufb01ciency trade-off for state-of-the-art NAT models and our DSLP on the WMT\u201914 EN\u2013DE dataset. A cross \u201c \u00d7 \u201d represents our DSLP variants, and a star \u201c \u22c6 \u201d refers a DSLP model that is enhanced with our mixed train- ing (MT) technique. Its base model is shown in the same color, and its correspondence is represented by an arrow. et al. (2021), a comparable-sized autoregressive model with a shallow decoder is able to outperform these approaches with similar latency. Therefore, these approaches do not achieve a desired quality\u2013ef\ufb01ciency trade-off. In this work, we propose a simple yet effective approach to non-autoregressive generation with a Deeply Supervised, Layer-wise Prediction-aware (DSLP) Transformer. It is no- ticed that a traditional NAT Transformer (Vaswani et al. 2017) only makes predictions at the last layer, where all the words are generated in parallel. Thus, the prediction of a word is unaware of other time steps, which typically leads to inconsistent sentences. For example, the English phrase thank you has two translations in German: danke sch\u00a8on and vielen dank . Instead of generating either of the phrases, a non-autoregressive model may predict danke dank , which is absurd. Therefore, we propose a layer-wise prediction-aware Transformer that predicts the output at every decoder layer. The prediction is fed to the next Transformer layer for fur- ther processing. Hence, our decoder model is aware of dif- ferent steps\u2019 predictions and can calibrate the NAT output through its decoder layers. We further introduce deep super- vision for training our prediction-aware decoder. This is es- sential to our model, because otherwise the intermediate pre- dictions are not grounded to the target output and the calibra- tion would be less meaningful. During training, we also pro- pose to mix the intermediate predictions with groundtruth tokens, as the layer-wise predictions may still be meaning- less after deep supervision. Our DSLP is a generic framework that can be com- bined with different base NAT models. In our experi- ments, we evaluated DSLP on WMT\u201914 English\u2013German and WMT\u201916 English\u2013Romanian pairs, and considered the translation of both directions. Results show that our DSLP consistently improves the NAT performance for every trans- lation language pair and every base model, including vanilla NAT, CMLM, GLAT, and CTC (see experiments section for details), demonstrating the generality of our approach. In addition, we show that mixing the layer-wise predictions with groundtruth during training further improves the per- formance. Remarkably, our best model, CTC with DSLP and mixed training, achieves better performance compared with its autoregressive teacher model on three of the four datasets with a 14.8x speedup. Figure 1 positions DSLP in prior work in terms of quality\u2013ef\ufb01ciency trade-off.", "contribution": "The main contributions of this paper are:\n\n1. Proposing a Deeply Supervised, Layer-wise Prediction-aware (DSLP) Transformer for non-autoregressive generation that predicts the output at every decoder layer, making the model aware of different steps' predictions and able to calibrate the NAT output through its decoder layers.\n\n2. Introducing deep supervision for training the prediction-aware decoder, which is essential to ground the intermediate predictions to the target output and make the calibration more meaningful.\n\n3. Proposing to mix the intermediate predictions with groundtruth tokens during training to further improve the performance.\n\n4. Demonstrating the generality of the DSLP approach by evaluating it on different base NAT models and language pairs, and showing that the best model with DSLP and mixed training achieves better performance than its autoregressive teacher model on three of the four datasets with a 14.8x speedup.", "title": "Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision Chenyang Huang Introduction Related Work Methodology Experiments Conclusion References", "words": 1125, "abstract": "Abstract How do we perform ef\ufb01cient inference while retaining high translation quality? Existing neural machine translation mod- els, such as Transformer, achieve high performance, but they decode words one by one, which is inef\ufb01cient. Re- cent non-autoregressive translation models speed up the in- ference, but their quality is still inferior. In this work, we pro- pose DSLP, a highly ef\ufb01cient and high-performance model for machine translation. The key insight is to train a non- autoregressive Transformer with D eep S upervision and feed additional L ayer-wise P redictions. We conducted extensive experiments on four translation tasks (both directions of WMT\u201914 EN\u2013DE and WMT\u201916 EN\u2013RO). Results show that our approach consistently improves the BLEU scores com- pared with respective base models. Speci\ufb01cally, our best vari- ant outperforms the autoregressive model on three translation tasks, while being 14.8 times more ef\ufb01cient in inference. 1"}
{"introduction": "\nVarious pretraining methods (Song et al., 2019; Lewis et al.,\n2019; Qi et al., 2020; Raffel et al., 2020; Zhang et al., 2019a)\nhave been successfully applied in natural language gener-\nation. Most of the pretraining works are based on Trans-\nformer and designed with autoregressive (AR) language\nmodel. Transformer based pretraining models show consis-\n\n*Equal contribution 1University of Science and Technology of\nChina, Hefei, China 2During Internship at MSRA 3Microsoft Re-\nsearch Asia, Beijing, China 4Microsoft, Redmond, USA 5Sichuan\nUniversity, Chengdu, China. Correspondence to: Yeyun Gong\n<yegong@microsoft.com>.\n\ntent improvements with larger model size and larger pretrain-\ning corpus. Although the autoregressive generation method\nachieves high-quality results in many tasks, its latency is a\nwell-known limitation for online real-time usage.\n\nNon-autoregressive (NAR) models (Gu et al., 2017; Lee\net al., 2018; Ghazvininejad et al., 2019; Raffel et al., 2020;\nZhang et al., 2019a) are proposed to reduce generation la-\ntency. Different from AR models which generate tokens\nsequentially, NAR models generate tokens in parallel. Com-\npared to AR models, NAR models generally come with a\nmuch lower inference latency, but a decrease in accuracy. In\norder to balance latency and accuracy, semi-NAR generation\nmodels (Stern et al., 2019; Lee et al., 2018; Gu et al., 2019;\nGhazvininejad et al., 2019) are proposed. However, most of\nthe NAR and semi-NAR models focus on translation tasks\nrather than general natural langauge generation tasks, which\nare proved to signi\ufb01cantly bene\ufb01t from pretraining (Qi et al.,\n2020; Lewis et al., 2019). Some works (Guo et al., 2020b;\nSu et al., 2021) initialize their NAR models with pretrained\nnatural language understanding model BERT (Devlin et al.,\n2018) for better performance. To the best of our knowledge,\nthis paper proposes the \ufb01rst large-scale pretraining model\ndesigned for NAR and semi-NAR generation.\n\nIn this paper, we propose a new model named BANG 1 to\nbridge the gap between AR and NAR via pretraining a gen-\nerative model. Speci\ufb01cally, we consider pretraining model\nusing AR, semi-NAR and NAR objectives with different\nattention mechanisms, which decide what extent previous\ntokens can be attended to. Precisely, BANG is pretrained to\npredict each token with arbitrary length of previous golden\ntokens replaced with special token [MASK]s. For example,\nwith complete previous golden tokens, BANG predicts the\nnext token in the AR manner. With all previous tokens re-\nplaced by [MASK], BANG predicts the next token in the\nNAR manner.\n\nFor AR models, the training strategy of teacher-forcing is\ncommonly used, which uses the golden tokens as previous\ncontext to predict the next token. For NAR models, [MASK]\ninitialization (Ghazvininejad et al., 2019) or other methods\n\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\n\n1https://github.com/microsoft/BANG\n\n\nlike encoder copy (Gu et al., 2017) and posterior distribu-\ntion approximation (Shu et al., 2020) are applied. In BANG\npretraining, we consider the previous context of golden and\n[MASK] tokens, with arbitrary golden tokens\u2019 length and\n[MASK] tokens\u2019 length. To achieve an ef\ufb01cient implemen-\ntation for multiple arbitrary alternatives in a same output\nsequence, we propose a new structure named cross-stream\nvisible n-stream self-attention, which can be used to train\nBANG with different golden and [MASK] tokens\u2019 combina-\ntions. For usage on downstream tasks, the single pretrained\nBANG model can be directly \ufb01netuned for either vanilla AR\nmodels or vanilla NAR models. Additionally, BANG can\nalso be \ufb01netuned for hybrid semi-NAR models, which sup-\nport predicting tokens with arbitrary previous golden tokens\nor [MASK]. Concretely, for semi-NAR generation, BANG\npredicts the \ufb01rst several tokens one by one as a high-quality\nsub-sequence hint, then produces all the remaining tokens\nsimultaneously.\n\nOur main contributions are: 1) BANG bridges the gap\nbetween AR and NAR by considering arbitrary previous\n[MASK] length during large-scale pretraining. 2) BANG is\npretrained using an ef\ufb01cient cross-stream visible n-stream\ndecoder to realize parallelization. Given multiple arbitrary\nnumber of previous tokens replaced with [MASK], every\ntoken is trained to predict simultaneously at each time step.\n3) BANG supports NAR, semi-NAR and AR \ufb01netuning to\nmeet different requirements with the same pretrained model\nstructure. 4) We pretrain BANG with 16GB English lan-\nguage corpora of Wikipedia and BookCorpus, and \ufb01netune\nit on 3 popular natural language generation tasks in AR,\nsemi-NAR and NAR manners, respectively. For NAR and\nsemi-NAR \ufb01netuning, BANG achieves signi\ufb01cant perfor-\nmance improvements on all the tasks. For AR \ufb01netuning\nwith the comparison to strong AR pretrained models, BANG\ncan attain comparable performance.\n", "contribution": "Main contributions of the paper are:\n\n1. BANG model is proposed to bridge the gap between autoregressive (AR) and non-autoregressive (NAR) models via pretraining a generative model.\n2. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, considering arbitrary previous [MASK] length during large-scale pretraining.\n3. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.\n4. BANG is pretrained with 16GB English language corpora of Wikipedia and BookCorpus, and finetuned on 3 popular natural language generation tasks in AR, semi-NAR, and NAR manners, respectively.\n5. For NAR and semi-NAR finetuning, BANG achieves significant performance improvements on all the tasks.\n6. For AR finetuning with the comparison to strong AR pretrained models, BANG can attain comparable performance.", "title": "BANG: Bridging Autoregressive and Non-autoregressive Generation with", "words": 1247, "abstract": "\nIn this paper, we propose BANG, a new pre-\nto Bridge the gap between\ntraining model\nAutoregressive (AR) and Non-autoregressive\n(NAR) Generation. AR and NAR generation can\nbe uniformly regarded as to what extent previ-\nous tokens can be attended, and BANG bridges\nAR and NAR generation by designing a novel\nmodel structure for large-scale pretraining. The\npretrained BANG model can simultaneously sup-\nport AR, NAR and semi-NAR generation to meet\ndifferent requirements. Experiments on question\ngeneration (SQuAD 1.1), summarization (XSum)\nand dialogue generation (PersonaChat) show that\nBANG improves NAR and semi-NAR perfor-\nmance signi\ufb01cantly as well as attaining compara-\nble performance with strong AR pretrained mod-\nels. Compared with the semi-NAR strong base-\nlines, BANG achieves absolute improvements of\n14.01 and 5.24 in the overall scores of SQuAD\n1.1 and XSum, respectively. In addition, BANG\nachieves absolute improvements of 10.73, 6.39\nand 5.90 in the overall scores of SQuAD, XSUM\nand PersonaChat respectively compared with the\nstrong NAR baselines."}
{"introduction": "et al. , 2014 ; , 2014 ) has achieved impressive performance in recent years, but the au- toregressive decoding process limits the translation speed and restricts low-latency applications. To mitigate this issue, many non-autoregressive (NAR) translation methods have been proposed, including latent space models ( , 2017 ; , 2019 ; , 2019 ), iterative re\ufb01nement meth- ods ( , 2018 ; , 2019 ), and alternative loss functions ( Libovick ` y and Helcl , 2018 ; Wang et al. , 2019 ; Wei et al. , 2019 ; Li et al. , 2019 ; , 2019 ). The decoding speedup for NAR models is typically 2-15 \u00d7 depending on the speci\ufb01c setup (e.g., the number of length can- didates, number of latent samples, etc.), and NAR models can be tuned to achieve different trade-offs between time complexity and decoding quality ( Gu et al. , 2017 ; , 2019 ; , 2019 ; Ma et al. , 2019 ). these methods are based on transformer modules ( Vaswani et al. , 2017 ), and depend on a well-trained AR model to obtain its output translations to cre- ate targets for NAR model training. This training setup is well-suited to leverage external monolin- gual data, since the target side of the NAR train- ing corpus is always generated by an AR model. , 2015a ) are known to improve MT performance us- ing monolingual data alone. However, to the best of our knowledge, monolingual data augmentation for NAR-MT has not been reported in the literature. teacher provides a consistent supervision signal for the NAR model; the source text that was used to train the teacher is decoded by the teacher to create synthetic target text. In this work, we use a large amount of source text from monolingual corpora to generate additional teacher outputs for NAR-MT training. tural changes to perform NAR generation in a non- iterative way, which establishes stronger baselines than most of the previous methods. We demon- strate that generating additional training data with monolingual corpora consistently improves the translation quality of our baseline NAR system on the WMT14 En-De and WMT16 En-Ro transla- tion tasks. Furthermore, our experiments show that NAR models trained with increasing amount of ex- tra monolingual data are less prone to over\ufb01tting and generalize better on longer sentences. \u2192 \u2192 iterative NAR-MT, just by using more monolingual data. 1894 Parallel En Mono. Non-En Mono. En-Ro 608,320 2,197,792 2,261,206 En-De 4,459,186 3,008,621 3,015,110 Table 1: Number of sentences per language arc. \u2018Mono\u2019 refers to the amount of monolingual text available. 2", "contribution": "The main contributions of this paper are:\n\n1. The use of monolingual data augmentation for non-autoregressive machine translation (NAR-MT) to generate additional teacher outputs for NAR-MT training, which consistently improves the translation quality of the baseline NAR system on the WMT14 En-De and WMT16 En-Ro translation tasks.\n\n2. The demonstration that NAR models trained with increasing amounts of extra monolingual data are less prone to overfitting and generalize better on longer sentences.\n\n3. The proposal of a new NAR-MT method that makes structural changes to perform NAR generation in a non-iterative way, which establishes stronger baselines than most of the previous methods.", "title": "Improving Non-autoregressive Neural Machine Translation with Jiawei Zhou Phillip Keung Abstract References", "words": 642, "abstract": "Non-autoregressive (NAR) neural machine translation is usually done via knowledge dis- tillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model\u2019s performance, with the goal of trans- ferring the AR model\u2019s generalization abil- ity while preventing over\ufb01tting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks con\ufb01rm that monolin- gual data augmentation consistently improves the performance of the NAR model to ap- proach the teacher AR model\u2019s performance, yields comparable or better results than the best non-iterative NAR methods in the litera- ture and helps reduce over\ufb01tting in the training process. 1"}
{"introduction": "translation (Bahdanau et al., 2015; Vaswani et al., 2017). As we examine the current frameworks, the most popular autoregressive models generate tokens step-by-step. If not better, recent non- autoregressive approaches (Gu et al., 2018; Kaiser et al., 2018; Lee et al., 2018) have proved it possible to perform generation within a much smaller number of decoding iterations. In this paper, we propose Levenshtein Transformer (LevT), aiming to address the lack of \ufb02exibility of the current decoding models. Notably, in the existing frameworks, the length of generated sequences is either \ufb01xed or monotonically increased as the decoding proceeds. This remains incompatible with human-level intelligence where humans can revise, replace, revoke or delete any part of their generated text. Hence, LevT is proposed to bridge this gap by breaking the in-so-far standardized decoding mechanism and replacing it with two atomic operations \u2014 insertion and deletion . executed in an alternate manner. Empirically, we show that LevT achieves comparable or better results than a standard Transformer model on machine translation and summarization, while maintaining the ef\ufb01ciency advantages bene\ufb01ted from parallel decoding similarly to (Lee et al., 2018). With this model, we argue that the decoding becomes more \ufb02exible. For example, when the decoder is given an empty token, it falls back to a normal sequence generation model. On the other hand, the decoder acts as a re\ufb01nement model when the initial state is a low-quality generated sequence. Indeed, we show that a LevT trained from machine translation is directly applicable to translation post-editing without any change. This would not be possible with any framework in the literature because generation and re\ufb01nement are treated as two different tasks due to the model\u2019s inductive bias. Preprint. Under review. arXiv:1905.11006v1  [cs.CL]  27 May 2019 One crucial component in LevT framework is the learning algorithm. We leverage the characteristics of insertion and deletion \u2014 they are complementary but also adversarial. The algorithm we propose is called \u201cdual policy learning\u201d. The idea is that when training one policy (insertion or deletion), we use the output from its adversary at the previous iteration as input. An expert policy, on the other hand, is drawn to provide a correction signal. Despite that, in theory, this learning algorithm is applicable to other imitation learning scenarios where a dual adversarial policy exists, in this work we primarily focus on a proof-of-concept of this algorithm landing at training the proposed LevT model. To this end, we summarize the contributions as follows: \u2022 We propose Levenshtein Transformer (LevT), a new sequence generation model composed of the insertion and deletion operations. This model achieves comparable or better results than a strong ef\ufb01ciency (up to \u00d7 5 speed-up); \u2022 learning, tackling the complementary and adversarial nature of the dual policies; \u2022 We recognize our model as a pioneer attempt to unify sequence generation and re\ufb01nement, thanks to its built-in \ufb02exibility. With this uni\ufb01cation, we empirically validate the feasibility of applying a LevT model trained by machine translation directly to translation post-editing, without any change. 2", "contribution": "The main contributions of this paper are:\n\n- The proposal of Levenshtein Transformer (LevT), a new sequence generation model that breaks the standardized decoding mechanism and replaces it with insertion and deletion operations executed in an alternate manner, resulting in a more flexible decoding process.\n- The demonstration that LevT achieves comparable or better results than a standard Transformer model on machine translation and summarization, while maintaining efficiency advantages.\n- The development of a learning algorithm called \"dual policy learning\" that leverages the complementary and adversarial nature of the insertion and deletion operations in LevT.\n- The recognition of LevT as a pioneer attempt to unify sequence generation and refinement, allowing for the direct application of a LevT model trained by machine translation to translation post-editing without any change.", "title": "Levenshtein Transformer", "words": 697, "abstract": "step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a \ufb01xed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more \ufb02exible and amenable sequence generation. Unlike previous approaches, the atomic operations of our model are insertion and deletion . The combination of them facilitates not only generation but also sequence re\ufb01nement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other\u2019s learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable performance but much-improved ef\ufb01ciency on both generation (e.g. machine translation, text summarization) and re\ufb01nement tasks (e.g. automatic post-editing). We further con\ufb01rm the \ufb02exibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing. 1"}
{"introduction": "26 , 27 , 5 , 39 ] have received extensive attention in natural language processing communities in recent years. Generally, training of these models consists of two stages. then \ufb01ne-tuned end-to-end on downstream tasks with task-speci\ufb01c loss functions and datasets. In this way, pre-trained language models have achieved great success on various natural language understanding tasks such as reading comprehension and text classi\ufb01cation, and BERT [ 5 ] is one of the most successful models among them. language generation which is one of the core problems in NLP [ 2 , 34 , 24 ], how to incorporate BERT \u2217 Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.06138v1  [cs.CL]  13 Oct 2020 remains substantially challenging. We conclude the main challenges as three-fold considering that the sequence-to-sequence framework [ 33 ] is the backbone model of generation tasks. On the encoder side, as studied in [ 41 ], simply initializing the encoder with a pre-trained BERT will actually hurt the performance. One possible explanation could be that training on a complex task with rich resources (e.g., machine translation) leads to the catastrophic forgetting problem [ 23 ] of the pre-trained model. On the decoder side, which can be treated as a conditional language model, it is naturally non-trivial to marry unconditional pre-training with conditional \ufb01ne-tuning. And the bidirectional nature of BERT also prevents it from being directly applied to common autoregressive text generation. In addition, \ufb01ne-tuning the full model is parameter inef\ufb01cient considering the enormous scale of recent pre-trained language models [28] while being unstable and fragile on small datasets [20]. text generation tasks under the sequence-to-sequence framework. Speci\ufb01cally, we construct our framework based on the following steps. We \ufb01rst choose two pre-trained BERT models from the source/target side respectively, and consider them as the encoder/decoder. For example, on the WMT14 English-German machine translation task, we take bert-base-cased as the encoder and bert-base-german-cased as the decoder. Then, we introduce lightweight neural network components named adapter layers and insert them into each BERT layer to achieve the adaptation to new tasks. While \ufb01ne-tuning on task speci\ufb01c datasets, we freeze the parameters of BERT layers and only tune the adapter layers. We design different architectures for adapters. Speci\ufb01cally, we stack two feed-forward networks as the encoder adapter, mainly inspired from [ 3 ]; and an encoder-decoder attention module is considered as the decoder adapter. Considering that BERT utilizes bi-directional context information and ignores conditional dependency between tokens, we build our framework on a parallel sequence decoding algorithm named Mask-Predict [ 8 ] to make the most of BERT and keep the consistency between training and inference. modules, we decouple the parameters of the pre-trained language model and task-speci\ufb01c adapters, therefore bypassing the catastrophic forgetting problem. And the conditional information can be learned through the cross-attention based adapter on the decoder side; 2) Our model is parameter ef\ufb01cient and robust while tuning as a bene\ufb01t from the lightweight nature of adapter modules. In addition, thanks to parallel decoding, the proposed framework achieves better performance than autoregressive baselines while doubling the decoding speed; 3) Each component in the framework can be considered as a plug-in unit, making the framework very \ufb02exible and task agnostic. For example, our framework can be adapted to autoregressive decoding straightforwardly by only incorporating the source-side BERT encoder and adapters while keeping the original Transformer decoder. We evaluate our framework on various neural machine translation tasks, and the proposed framework achieves 36 . 49 / 33 . 57 BLEU scores on the IWSLT14 German-English/WMT14 German-English translation tasks, achieving 3 . 5 / 0 . 88 improvements over traditional autoregressive baselines with half of the inference latency. When adapting to autoregressive decoding, we achieve 30 . 60 / 43 . 56 BLEU scores on the WMT14 English-German/English-French translation tasks, on par with the state-of-the-art baseline models. 2", "contribution": "The main contributions of this paper are:\n\n1. A framework for text generation tasks under the sequence-to-sequence framework that utilizes pre-trained BERT models as the encoder and decoder, and introduces lightweight neural network components named adapter layers to achieve adaptation to new tasks.\n2. The proposed framework is parameter efficient and robust while tuning, achieves better performance than autoregressive baselines while doubling the decoding speed, and is flexible and task agnostic.\n3. Evaluation of the framework on various neural machine translation tasks, achieving significant improvements over traditional autoregressive baselines with half of the inference latency, and achieving state-of-the-art results when adapting to autoregressive decoding.", "title": "Incorporating BERT into Parallel Sequence Decoding with Adapters", "words": 919, "abstract": "5 ] have achieved great success on various natural language understanding tasks, how to ef\ufb01ciently and effectively incorporate them into sequence-to-sequence models and the cor- responding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and \ufb01ne-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-speci\ufb01c dataset. In this way, we obtain a \ufb02exible and ef\ufb01cient model which is able to jointly leverage the information contained in the source-side and target- side BERT models, while bypassing the catastrophic forgetting problem. Each com- ponent in the framework can be considered as a plug-in unit, making the framework \ufb02exible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict [ 8 ] considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36 . 49 / 33 . 57 BLEU scores on IWSLT14 German-English/WMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30 . 60 / 43 . 56 BLEU scores on WMT14 English-German/English-French translation, on par with the state-of-the-art baseline models. 1"}
{"introduction": "Neural machine translation (NMT) systems based on autoregressive models achieve state-of-the- art (SOTA) performance, where the Trans- former ( Vaswani et al. , 2017 ) encoder-decoder framework is the prevalent architecture. Autore- gressive translation (AT) systems generate target tokens sequentially, i.e., each generation step de- pends on the previously generated tokens, resulting \u2217 Contribution during internship at Machine Intelligence Technology Lab, Alibaba DAMO Academy. \u2020 Corresponding Author 1 We have released our code at https://github. com/zja-nlp/NAT_with_DAD . in high inference latency. Non-autoregressive trans- lation (NAT) ( Gu et al. , 2018 ) models are proposed to speed up the inference of NMT, by generating all the target tokens independently and simultane- ously. However, this independence assumption pre- vents NAT models to properly learn the target-side dependency in real data distribution. Therefore, while NAT models signi\ufb01cantly reduce the infer- ence latency, they suffer from accuracy degradation compared to AT models. The mainstream NAT models fall into two cate- gories, iterative NAT models and fully NAT models. Iterative NAT models ( Gu et al. , 2019 ; Ghazvinine- jad et al. , 2019 ; Lee et al. , 2018 ) employ an it- erative procedure to re\ufb01ne translations, achieving improved translation accuracy at the expanse of the decoding speed. In fact, iterative NAT mod- els have similar latency and translation accuracy with AT models with deep encoder and shallow decoder ( Kasai et al. , 2020b ). In contrast, fully NAT models maintain the latency advantage by making parallel predictions with a single decoding round, but produce outputs with poor translation ac- curacy. Due to their high ef\ufb01ciency, there has been increased research effort to improve the translation accuracy of fully NAT models while maintaining their latency advantage ( Guo et al. , 2019 , 2020a , b ; Saharia et al. , 2020 ; Ma et al. , 2019 ; Ran et al. , 2020 ). Recently, Gu and Kong ( 2021 ) achieves competitive results for fully NAT models on three translation benchmarks. They argue that the key to successfully training a fully NAT model is to per- form dependency reduction in the learning space of output tokens. While easing the dif\ufb01culty of generating out- put tokens may be practical, it limits the perfor- mance upper bound of fully NAT models. Mod- els trained with these methods would struggle for generating complex sentences. Hence, different from Gu and Kong ( 2021 ) to perform dependency arXiv:2203.16266v1  [cs.CL]  30 Mar 2022 Figure 1: The illustration of our proposed approach DAD . S denotes the source text (in German). P F B denotes the prediction from the NAT model with both forward and backward dependency while P F denotes the prediction of the NAT model with only forward de- pendency. space S and space T denote the source rep- resentation space and the target representation space, respectively. reduction in the learning space of output tokens, we focus on enhancing the learning capacity of fully NAT models by helping the model learn these complex dependencies among target tokens. De- pendencies generally include forward dependen- cies and backward dependencies. Previous works explore bidirectional decoding to improve model- ing of both forward and backward dependencies in phrase-based statistical machine translation ( Finch and Sumita , 2009 ) and RNN-based machine trans- lation ( Zhang et al. , 2018 ). We \ufb01nd that the less- investigated backward dependencies are critical for target generation in NAT. As shown in Fig- ure 1 , \u201cwoher\u201d means both \u201cwhere\u201d and \u201chow\u201d in German. Without considering the backward depen- dency, the model would be confused of the selec- tion of these two possible translations, leading to a multi-modality error ( Ran et al. , 2020 ). Multi- modality is the main problem that NAT models suffer from, i.e., the target tokens may be gener- ated based on different possible translations, often causing repeating or missing tokens. Hence, we propose approaches for fully NAT models to model both forward and backward target dependencies. On the other hand, decoder input is a crucial fac- tor in target dependency modeling since the \ufb01nal output is directly conditioned on them. We hypoth- esize that accurate modeling of target dependencies may require the decoder input to be in the same representation space of the target output. Prior NAT models ( Gu et al. , 2018 ; Wang et al. , 2019 ) mostly use a copy of source text embedding as decoder input, i.e., z in Figure 2 , which is target- independent. Various methods have been proposed to make the NAT decoder input similar to the target space. Qian et al. ( 2021 ), Guo et al. ( 2020a ) and Guo et al. ( 2020b ) choose a number of positions in z and substitute them with the corresponding target embedding. Guo et al. ( 2019 ) uses linear mapping to make z closer to target embedding. Unfortu- nately, these methods cannot guarantee to have the decoder input in the exact target space and still lead to a difference from real target distribution. In this work, we design a novel Dependency- Aware Decoder ( DAD ) for fully NAT models so that the model can learn complex target-side depen- dencies. Firstly, we enhance the target dependency within the NAT decoder from the perspective of decoder input . The initial decoder input (see z in Figure 1 ) is obtained by copying the source text embedding with uniform copy ( Gu et al. , 2018 ) or soft copy ( Wei et al. , 2019 ), which is dif\ufb01cult to build the target-side dependency. Our \ufb01rst innova- tion is to transform the initial decoder input z from the source language space to the target language space through a novel attentive transformation pro- cess. For brevity, we denote this decoder input transformation method by IT . We utilize the output embedding matrix of the decoder as a knowledge base in the target representation space. We take the original input as the query and apply the attention mechanism on the output embedding matrix, so that we can directly extract information from the target space and reform them as the new decoder input. In this way, we can construct the decoder input from the target space without creating a gap between training and inference. Secondly, we enhance the target dependency within the NAT decoder from the perspective of decoder self-attention . Guo et al. ( 2020a ) incor- porates the forward dependency modeled by AT models through curriculum learning to develop NAT models. As discussed earlier, forward depen- dency only is inadequate for NAT models. Hence, our second innovation is to introduce an effec- tive forward-backward pre-training phase before NAT training. We adopt different triangle attention masks to force the model to only attend to previous or future information, learning the forward or back- ward dependencies respectively. We denote this method of forward-backward dependency model- ing by FBD . We also investigate different curricula for optimizing initialization for fully NAT models (Section 4.3 ). Our contributions can be summarized as follows: 1) we propose a novel dependency-aware decoder (DAD) for non-autoregressive translation models to model the target-side dependencies. The proposed approach is model-agnostic and can be applied to other NAT models. In this design, we propose a novel decoder input transformation approach and a novel approach for incorporating backward depen- dency modeling into the decoder of NAT models, through which the target-side dependencies can be better modeled. 2) Experimental results on multiple machine translation benchmarks demonstrate that our proposed approach signi\ufb01cantly improves the representative vanilla NAT model ( Gu et al. , 2018 ) and the highly competitive fully NAT model GLAT ( Qian et al. , 2021 ) and DSLP ( Huang et al. , 2021 ), by up to +1.88 BLEU score on GLAT and +2.2 BLEU score on vanilla NAT, while adding only mi- nor latency to these models and overall maintaining comparable inference latency to other fully NAT models. 2", "contribution": "The paper proposes a novel Dependency-Aware Decoder (DAD) for non-autoregressive translation models to model the target-side dependencies. The proposed approach includes a novel decoder input transformation approach and a novel approach for incorporating backward dependency modeling into the decoder of NAT models, through which the target-side dependencies can be better modeled. Experimental results on multiple machine translation benchmarks demonstrate that the proposed approach significantly improves the representative vanilla NAT model and the highly competitive fully NAT model, by up to +1.88 BLEU score on GLAT and +2.2 BLEU score on vanilla NAT, while adding only minor latency to these models and overall maintaining comparable inference latency to other fully NAT models.", "title": "Non-autoregressive Translation with Dependency-Aware Decoder", "words": 1753, "abstract": "Non-autoregressive translation (NAT) models suffer from inferior translation quality due to removal of dependency on previous target to- kens from inputs to the decoder. In this pa- per, we propose a novel and general approach to enhance the target dependency within the NAT decoder from two perspectives: decoder input and decoder self-attention . First, we transform the initial decoder input from the source language space to the target language space through a novel attentive transforma- tion process. The transformation reassem- bles the decoder input based on target token embeddings and conditions the \ufb01nal output on the target-side information. Second, be- fore NAT training, we introduce an effective forward-backward pre-training phase, imple- mented with different triangle attention masks. This pre-training phase enables the model to gradually learn bidirectional dependencies for the \ufb01nal NAT decoding process. Experimen- tal results demonstrate that the proposed ap- proaches consistently improve highly compet- itive NAT models on four WMT translation di- rections by up to 1.88 BLEU score, while over- all maintaining inference latency comparable to other fully NAT models. 1 1"}
{"introduction": "\nUnlike autoregressive generation (AG) that gener-\nates tokens step-by-step, non-autoregressive gener-\nation (NAG) parallelly generates all tokens in one\ntime step and thus the inference could be signi\ufb01-\ncantly speeded up (Ma et al., 2019; Ran et al., 2020;\nSusanto et al., 2020). Despite the computational\nadvantage of NAG, it has faced the multimodality\nproblem (Gu et al., 2018) caused by the condition-\nally independent decoding. A typical example of\nthe problem is illustrated in Figure 1, where either\n\n\u2217 Correspondence to Wenqiang Lei.\n\nFigure 1: An example to explain \u201cmultimodality prob-\nlem\u201d. The German sentence \u201cVielen Dank.\u201d can be\ntranslated into \u201cMany Thanks.\u201d and \u201cThank you.\u201d.\n\nof \u201cThank you.\u201d and \u201cMany Thanks.\u201d is the correct\ntranslation (i.e., generation modes). In this exam-\nple, a mixed mode \u201cMany you.\u201d / \u201cThank Thanks.\u201d\nwill be generated by NAG. It is because the con-\nditional dependence among target words will be\nbroken in parallel decoding. A typical manifesta-\ntion is that words are usually missing (e.g., \u201cMany\nyou.\u201d) and repeating (e.g., \u201cThank Thanks.\u201d) in\nNAG\u2019s sentences. To solve this problem, the key is\nhelping NAG models to deal with various genera-\ntion modes.\n\nTo date, one of the most widely used solutions\nis sequence-level knowledge distillation (Kim and\nRush, 2016) which aims to reduce the generation\nmodes of the raw data (Zhou et al., 2019). Tak-\ning machine translation as an example, the knowl-\nedge distillation based methods rebuild the target\nsequence in the training set by employing an AG\nmodel to translate the training samples. The as-\nsumption is that the target sentences generated by\none AG model tend to have less modality. De-\nspite the success of the above studies, there are\nstill two major limitations: (1) Most existing works\nmainly focus on machine translation where the per-\nformance of AG is generally assumed to be better\n\n\fthan NAG. Clearly, such a solution will degrade\nthe performance of NAG on the task where the AG\nmodel cannot obtain a better result. As demon-\nstrated in our experiments (See \u00a7 4.5), there are a\nnumber of such tasks beyond the assumption like\ntext summarization and story ending generation.\n(2) The knowledge distillation based methods may\ncost a tremendous amount of time to rebuild a large-\nscale training set with AG, which runs counter to\nthe initial goal of NAG to improve the speed.\n\nTo overcome the aforementioned limitations,\nwe explore to alleviate the multimodality problem\nIn short, we aim to con-\nin a different manner.\nstrain NAG generation modes in the inference stage,\nrather than directly reducing generation modes in\nthe training stage. More speci\ufb01cally, our basic idea\nis that the linguistic structure of the target sentence\ncould be helpful to alleviate the multimodality prob-\nlem. In this paper, we show that the Part-of-Speech\n(POS) sequence, one of most simple solutions in\nmodeling the linguistic structure (Cutting et al.,\n1992), could effectively verify our idea and show\npromising performance in four different tasks. In\nmore details, the proposed POS-constrained Paral-\nlel Decoding (POSPD) trains a POS predictor to ob-\ntain POS tags of target sequences. In the inference\nstage, POSPD constrains NAG models to choose\nthe \ufb01nal outputs that satisfy the pre-speci\ufb01ed POS\nsequence. As the POS predictor with a shallow\ndecoder is separately trained, our POSPD could act\nas a plug-and-play method to assistant NAG mod-\nels with negligible extra time. Meanwhile, it also\nshows the speed advantage of our method even con-\nsidering the time cost in building the POS dataset,\nsince POS tagging is much faster than sentence\ngenerating due to the small POS dictionary.\n\nTo conduct a comprehensive empirical evalua-\ntion, we examine the generalizability of POSPD by\napplying it to two widely-used NAG models (i.e.,\nCMLM and DisCo) over four text generation tasks,\nincluding text summarization, story ending genera-\ntion, question generation, and machine translation.\nExperiments demonstrate that POSPD signi\ufb01cantly\nand consistently improves the two NAG models\nand beats the sequence-level knowledge distillation\nwith a considerable performance gap. The main\ncontributions of this work could be summarized as\nfollows:\n\n\u2022 For the \ufb01rst time, we experimentally reveal\nthat the implicit assumption of knowledge dis-\ntillation does not always hold for the tasks\n\n(e.g., text summarization, story ending gener-\nation, as demonstrated in our experiments). In\nother words, AG cannot guarantee better per-\nformance than NAG, thus resulting in the un-\ndesirable performance of NAG if using knowl-\nedge distillation to alleviate the multimodal-\nity problem. This empirical result could pro-\nvide novel insight to revisiting the role of the\nknowledge distillation in NAG.\n\n\u2022 To alleviate the multimodality problem in var-\nious tasks, we propose POSPD by employing\nPOS sequences to constrain the NAG gener-\nation modes in the inference stage. It is sim-\nple but effective, being able to act as a plug-\nand-play assistant for NAG models. Such a\nlinguistic structure based solution shows an\neffective and ef\ufb01cient alternative to the knowl-\nedge distillation paradigm in alleviating the\nmultimodality problem1.", "contribution": "The main contributions of this paper are:\n\n1. Experimentally revealing that the assumption of knowledge distillation does not always hold for tasks such as text summarization and story ending generation, and proposing a new approach to alleviate the multimodality problem in various tasks.\n\n2. Proposing POS-constrained Parallel Decoding (POSPD) as a simple but effective solution to constrain non-autoregressive generation (NAG) generation modes in the inference stage, which acts as a plug-and-play assistant for NAG models and shows an effective and efficient alternative to the knowledge distillation paradigm in alleviating the multimodality problem. \n\n3. Conducting a comprehensive empirical evaluation of POSPD by applying it to two widely-used NAG models over four text generation tasks, including text summarization, story ending generation, question generation, and machine translation, and demonstrating that POSPD significantly and consistently improves the two NAG models and beats the sequence-level knowledge distillation with a considerable performance gap.", "title": "POS-Constrained Parallel Decoding for Non-autoregressive Generation Kexin Yang Abstract Acknowledgments References", "words": 1218, "abstract": "The multimodality problem has become a ma- jor challenge of existing non-autoregressive generation (NAG) systems. A common solu- tion often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as \u201cteacher AG\u201d). The success of such methods may largely depend on a latent as- sumption, i.e., the teacher AG is superior to the NAG model. However, in this work, we exper- imentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending genera- tion. To provide a feasible solution to the mul- timodality problem of NAG, we propose incor- porating linguistic structure (Part-of-Speech sequence in particular) into NAG inference in- stead of relying on teacher AG. More specif- ically, the proposed POS-constrained Parallel Decoding (POSPD) method aims at provid- ing a speci\ufb01c POS sequence to constrain the NAG model during decoding. Our experi- ments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowl- edge distillation. This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation. 1"}
{"introduction": "Open-domain neural dialogue generation ( Vinyals and Le , 2015 ; Sordoni et al. , 2015 ; Li et al. , 2016a ; Mou et al. , 2016 ; Serban et al. , 2016a ; Asghar et al. , 2016 ; Mei et al. , 2016 ; Serban et al. , 2016e , b , d ; Baheti et al. , 2018 ; Wang et al. , 2018 ; Ghazvininejad et al. , 2018 ; Zhang et al. , 2018 ; 1 Qinghong and Yuxian contribute equally to this work. Gao et al. , 2019 ) treats dialog contexts ( x ) as sources,and responses ( y ) as targets and uses the encoder-decoder model ( Sutskever et al. , 2014 ; Vaswani et al. , 2017b ) as the backbone to generate responses. S EQ 2S EQ models offer the promise of scalability and language-independence, along with the capacity to capture contextual dependencies semantic and syntactic relations between sources and targets. One of key issues with the S EQ 2S EQ structure is that it exhibits a strong tendency to generate dull, trivial or non-committal responses (e.g., I don\u2019t know or I\u2019m OK ) regardless of the input, which has been observed by many recent works ( Li et al. , 2016a ; Sordoni et al. , 2015 ; Serban et al. , 2016c ; Niu and Bansal , 2020 ). Various strategies ( Li et al. , 2016a ; Vijayakumar et al. , 2016 ; Baheti et al. , 2018 ; Niu and Bansal , 2020 ) have been proposed to address this issue, , one of the most widely used of which is to replace the MLE objective in the S EQ 2S EQ training with the maximum mutual information objective (MMI for short) ( Li et al. , 2016a ). MMI models the bidirectional dependency between responses ( y ) and contexts ( x ). It takes the form of the linear combination of the forward probability log p ( y | x ) and the backward probability log p ( x | y ) . The intuition behind MMI is straightforward: it is easy to predict a dull response given any context, but hard to predict the context given a dull response since the context that corresponds to a dull response could be anything. Unfortunately, under the framework of the S EQ 2S EQ model, direct decoding from log p ( y | x ) + log p ( x | y ) is infeasible since the second part (i.e., p ( x | y ) ) requires the completion of target generation before p ( x | y ) can be computed, and the search space for y is huge. Empirically, an N-best list is \ufb01rst generated given p ( y | x ) , and p ( x | y ) is then used to rerank the arXiv:2002.04250v2  [cs.CL]  13 Feb 2020 N-best list. Due to the fact that beam search lacks for diversity in the beam: candidates often differ only by punctuation or minor morphological variations, with most of the words overlapping, this reranking strategy inevitably results in non-globally-optimal solutions. Some strategies have been proposed to alleviate this non-global-optimality issue, such as generating a more diverse N-best list ( Li et al. , 2016c ; Gu et al. , 2017 ; Vijayakumar et al. , 2016 ), or using reinforcement learning to estimate the future score of p ( x | y ) ( Li et al. , 2017a ), which help alleviate the non-globally-optimal issue, but cannot fully address it. Non-autoregressive (non-AR) generation ( Gu et al. , 2018 ; Ma et al. , 2019 ; Lee et al. , 2018 ) provides resolution to the non-global-optimality issue. generation, Under the formalization of non-AR target tokens y t are generated independently, which enables p ( x | y t ) to be computed as soon as y t is generated. This naturally resolves the non-global optimal issue in decoding. We conduct experiments on the widely used Opensubtitle dataset and experimental results demonstrate that the proposed strategy produces more diverse, coherent, and appropriate responses, yielding substantive gains in BLEU scores and in human evaluations. The rest of this paper is organized as follows: Section 2 and section 3 present related work and background knowledge respectively. The propose model is described in Section 4. Experimental results and ablation studies are detailed in Section 5 and 6, followed by a brief conclusion in Section 7. 2", "contribution": "The paper proposes a non-autoregressive generation strategy to address the issue of non-global-optimality in open-domain neural dialogue generation. The proposed strategy produces more diverse, coherent, and appropriate responses, yielding substantive gains in BLEU scores and in human evaluations. The main contribution of the paper is the introduction of a non-autoregressive generation strategy that resolves the non-global optimal issue in decoding.", "title": "Non-Autoregressive Neural Dialogue Generation", "words": 1027, "abstract": "Maximum Mutual information (MMI), which models the bidirectional dependency between responses ( y ) and contexts ( x ), i.e., the forward probability log p ( y | x ) and the backward probability log p ( x | y ) , has been widely used as the objective in the S EQ 2S EQ model to address the dull-response issue in open-domain dialog generation. Unfortunately, under the framework of the S EQ 2S EQ model, direct decoding from log p ( y | x ) + log p ( x | y ) is infeasible since the second part (i.e., p ( x | y ) ) requires the completion of target generation before it can be computed, and the search space for y is enormous. Empirically, an N-best list is \ufb01rst generated given p ( y | x ) , and p ( x | y ) is then used to rerank the N-best list, which inevitably results in non-globally-optimal solutions. In this paper, we propose to use non-autoregressive (non-AR) generation model to address this non-global optimality issue. Since target tokens are generated independently in non-AR generation, p ( x | y ) for each target word can be computed as soon as it\u2019s generated, and does not have to wait for the completion of the whole sequence. This naturally resolves the non-global optimal issue in decoding. Experimental results demonstrate that the proposed non-AR strategy produces more diverse, coherent, and appropriate responses, yielding substantive gains in BLEU scores and in human evaluations. 1 1"}
{"introduction": "Traditional neural machine translation (NMT) mod-\nels (Sutskever et al., 2014; Cho et al., 2014; Bah-\ndanau et al., 2014; Gehring et al., 2017; Vaswani\net al., 2017) commonly make predictions in an\nincremental token-by-token way, which is called\nautoregressive translation (AT). Although this strat-\negy can capture the full translation history, it has\nrelatively high decoding latency. To make the de-\ncoding more ef\ufb01cient, non-autoregressive transla-\ntion (NAT) (Gu et al., 2018) is introduced to gener-\nate multiple tokens at once instead of one-by-one.\nHowever, with the conditional independence prop-\nerty (Gu et al., 2018), NAT models do not directly\nconsider the dependencies among output tokens,\nwhich may cause errors of repeated translation and\n\n\u2217 Zhisong and Xiang contributed equally for this paper\n\nFigure 1: An example of the LAT mechanism. For each\ndecoding position, a short sequence of tokens is gener-\nated in an autoregressive way. (cid:104)sop(cid:105) is the special start-\nof-piece symbol. \u2018pos*\u2019 denotes the hidden state from\nthe decoder at that position.\n\nincomplete translation (Wang et al., 2019). There\nhave been various methods in previous work (Stern\net al., 2019; Gu et al., 2019; Ma et al., 2018; Wei\net al., 2019; Ma et al., 2019; Tu et al., 2020) to miti-\ngate this problem, including iterative decoding (Lee\net al., 2018; Ghazvininejad et al., 2019).\n\nIn this work, we introduce a novel mechanism,\ni.e., local autoregressive translation (LAT), to take\nlocal target dependencies into consideration. For a\ndecoding position, instead of generating one token,\nwe predict a short sequence of tokens (which we\ncall a translation piece) for the current and next\nfew positions in an autoregressive way. A simple\nexample is shown in Figure 1.\n\nWith this mechanism, there can be overlapping\ntokens between nearby translation pieces. We take\nadvantage of these redundancies, and apply a sim-\nple algorithm to align and merge all these pieces\nto obtain the full translation output. Speci\ufb01cally,\nour algorithm builds the output by incrementally\naligning and merging adjacent pieces, based on the\nhypothesis that each local piece is \ufb02uent and there\nare overlapping tokens between adjacent pieces\nas aligning points. Moreover, the \ufb01nal output se-\n\n \n \n \n \n \n \n\fquence is dynamically decided through the merging\nalgorithm, which makes the decoding process more\n\ufb02exible.\n\nWe integrate our mechanism into the conditional\nmasked language model (CMLM) (Ghazvinine-\njad et al., 2019) and similarly adopt iterative\ndecoding, where tokens with low con\ufb01dence\nscores are masked for prediction in more itera-\ntions. With evaluations on \ufb01ve translation tasks,\ni.e., WMT\u201914 EN\u2194DE, WMT\u201916 EN\u2194RO and\nIWSLT\u201914 DE\u2192EN, we show that our method\ncould achieve similar or better performance com-\npared with CMLM and AT models while gaining\nnearly 2.5 and 7 times speedups, respectively. Fur-\nthermore, our method is shown to effectively re-\nduce repeated translations and perform better at\nlonger sentences.\n", "contribution": "The paper introduces a novel mechanism called Local Autoregressive Translation (LAT) to take local target dependencies into consideration in non-autoregressive machine translation. The mechanism predicts a short sequence of tokens (translation piece) for the current and next few positions in an autoregressive way, which can capture the dependencies among output tokens and reduce errors of repeated translation and incomplete translation. The paper integrates the mechanism into the Conditional Masked Language Model (CMLM) and adopts iterative decoding to achieve similar or better performance compared with CMLM and Autoregressive Translation (AT) models while gaining nearly 2.5 and 7 times speedups, respectively. The method is also shown to effectively reduce repeated translations and perform better at longer sentences.", "title": "Incorporating a Local Translation Mechanism", "words": 776, "abstract": "toregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among tar- get outputs. Speci\ufb01cally, for each target decod- ing position, instead of only one token, we pre- dict a short sequence of tokens in an autore- gressive way. We further design an ef\ufb01cient merging algorithm to align and merge the out- put pieces into one \ufb01nal output sequence. We integrate LAT into the conditional masked lan- guage model (CMLM; , 2019 ) and similarly adopt iterative decod- ing. Empirical results on \ufb01ve translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and per- forms better at longer sentences. The code for our model is available at https://github. com/shawnkx/NAT-with-Local-AT . 1"}
{"introduction": "\nNeural machine translation (NMT) models based on the Transformer architecture have achieved\nleading performance (Vaswani et al., 2017; Barrault et al., 2019; Huang et al., 2020). Majority\nof the proposed approaches are based on the autoregressive (AR) principle, where translation is\ndone one token at a time conditioning on already translated tokens. AR inference scales linearly\nwith the number of tokens and full forward pass through the decoder is required for each translated\ntoken. This can be prohibitively expensive for long sequences, particularly as leading models are\nbecoming increasingly larger in size. To mitigate this problem, recent works have explored the\nnon-autoregressive (NAR) approach where subsets of tokens are translated in parallel (Gu et al., 2018;\nGhazvininejad et al., 2019; Kasai et al., 2020). NAR models achieve signi\ufb01cantly faster inference\nspeed that no longer depends on sequence length. However, despite considerable progress, leading\nNAR models still require sequence-level knowledge distillation (Kim & Rush, 2016) to achieve\ncompetitive accuracy. In practice, a large AR Transformer model trained on the raw data is used\nas the teacher for distillation (Ghazvininejad et al., 2019). This process is expensive, as every new\nlanguage pair requires training a new teacher. It is also non-standard, and raises questions to the\nnecessity and the underlying problems solved by distillation (Zhou et al., 2020; Ding et al., 2021).\n\nIn this work we focus on one of the leading NAR approaches, the Conditional Masked Language\nModel (CMLM) (Ghazvininejad et al., 2019). CMLM achieved leading NAR performance on\nmultiple NMT datasets - especially when combined with semi-autoregressive training (Ghazvininejad\net al., 2020b) - but only when the model is trained on distilled data. Without distillation, CMLM\nperformance drops signi\ufb01cantly below AR benchmarks. The need for distillation indicates that\nCMLM alone is unable to fully leverage the information available in the raw training data (Ding et al.,\n2021). Here, we identify two shortcomings of CMLM that, when addressed, signi\ufb01cantly improve\nNAR translation quality and narrow the gap between raw and distilled performance.\n\nFirst, input token representations in CMLM can become nearly indistinguishable, especially for\nadjacent positions. In AR models this problem is avoided by a combination of causal masked attention,\nsequential inference, and learned positional encodings (PEs). However, unmasked attention and\nsimultaneous translation of token blocks in CMLM loses most of the information that distinguishes\ntokens. This problem is particularly severe during the \ufb01rst inference step, where the input is fully\nmasked. The model thus only relies on learned PEs to distinguish tokens, which is not suf\ufb01cient.\nPoor token separation can cause signi\ufb01cant translation errors, including the identi\ufb01ed phenomenon of\ntoken repetition stemming from the related multi-modality problem (Zhou et al., 2020).\n\n1\n\n\fPublished as a conference paper at ICLR 2022\n\nSecond, there is a misalignment between CMLM\u2019s training and inference procedures. During training\nCMLM is optimized with a masked loss analogous to language model training in popular models\nsuch as BERT (Devlin et al., 2019). However, CMLM inference always starts with a fully masked\nsentence and translates all tokens simultaneously. Iterative re\ufb01nement is then applied where subsets\nof low con\ufb01dence tokens are masked and re-translated at each iteration. During training the model\nrarely sees a fully masked sentence, and is not trained to self-correct from the initial fully masked\ntranslation that can contain signi\ufb01cant errors. The misalignment between the two procedures can\ncause a disconnect, where optimization of the training loss does not transfer to improvements in\ntranslation quality.\n\nIn this work we propose the Conditional Masked Language Model with Correction (CMLMC). Our\nmodel builds on the CMLM architecture and addresses the aforementioned problems. We modify the\ndecoder structure by exposing the positional encodings and incorporating causal attention layers to\ndifferentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to\ncorrect translation mistakes made in early decoding iterations from the fully masked sentence. With\nthese improvements, CMLMC achieves new state-of-the-art undistilled NAR results and approaches\nAR performance on multiple NMT benchmarks.\n", "contribution": "The main contributions of this paper are:\n\n1. Proposing the Conditional Masked Language Model with Correction (CMLMC) which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in non-autoregressive (NAR) machine translation.\n2. Modifying the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens.\n3. Proposing a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence.\n4. Achieving new state-of-the-art undistilled NAR results and approaching autoregressive (AR) performance on multiple NMT benchmarks.", "title": "IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION", "words": 989, "abstract": "\nTransformer-based autoregressive (AR) machine translation models have achieved\nsigni\ufb01cant performance improvements, nearing human-level accuracy on some\nlanguages. The AR framework translates one token at a time which can be time\nconsuming, especially for long sequences. To accelerate inference, recent work has\nbeen exploring non-autoregressive (NAR) approaches that translate blocks of tokens\nin parallel. Despite signi\ufb01cant progress, leading NAR models still lag behind their\nAR counterparts, and only become competitive when trained with distillation. In\nthis paper we investigate possible reasons behind this performance gap, namely, the\nindistinguishability of tokens, and mismatch between training and inference. We\nthen propose the Conditional Masked Language Model with Correction (CMLMC)\nthat addresses these problems. Empirically, we show that CMLMC achieves state-\nof-the-art NAR performance when trained on raw data without distillation, and\napproaches AR performance on multiple datasets. Code for this work is available\nhere: https://github.com/layer6ai-labs/CMLMC."}
{"introduction": "Most machine translation systems use sequen- tial decoding strategies where words are predicted one-by-one. In this paper, we present a model and a parallel decoding algorithm which, for a rela- tively small sacri\ufb01ce in performance, can be used to generate translations in a constant number of decoding iterations. We introduce conditional masked language models (CMLMs), which are encoder-decoder ar- chitectures trained with a masked language model objective ( Devlin et al. , 2018 ; Lample and Con- neau , 2019 ). This change allows the model to learn to predict, in parallel, any arbitrary subset of masked words in the target translation. We use transformer CMLMs, where the decoder\u2019s self at- tention ( Vaswani et al. , 2017 ) can attend to the \u2217 Equal contribution, sorted alphabetically. 1 Our code is publicly available at: https://github.com/facebookresearch/Mask-Predict entire sequence (left and right context) to pre- dict each masked word. We train with a simple masking scheme where the number of masked tar- get tokens is distributed uniformly, presenting the model with both easy (single mask) and dif\ufb01cult (completely masked) examples. Unlike recently proposed insertion models ( Gu et al. , 2019 ; Stern et al. , 2019 ), which treat each token as a separate training instance, CMLMs can train from the en- tire sequence in parallel, resulting in much faster training. We also introduce a new decoding algorithm, mask-predict , which uses the order-agnostic na- ture of CMLMs to support highly parallel decod- ing. Mask-predict repeatedly masks out and re- predicts the subset of words in the current trans- lation that the model is least con\ufb01dent about, in contrast to recent parallel decoding translation approaches that repeatedly predict the entire se- quence ( Lee et al. , 2018 ). Decoding starts with a completely masked target text, to predict all of the words in parallel, and ends after a constant num- ber of mask-predict cycles. This overall strategy allows the model to repeatedly reconsider word choices within a rich bi-directional context and, as we will show, produce high-quality translations in just a few cycles. Experiments on benchmark machine translation datasets show the strengths of mask-predict de- coding for transformer CMLMs. With just 4 it- erations, BLEU scores already surpass the perfor- mance of the best non-autoregressive and parallel decoding models. 2 With 10 iterations, the approach outper- forms the current state-of-the-art parallel decod- 2 We use the term \u201cparallel decoding\u201d to refer to the family of approaches that can generate the entire target sequence in parallel. These are often referred to as \u201cnon-autoregressive\u201d approaches, but both iterative re\ufb01nement ( Lee et al. , 2018 ) and our mask-predict approach condition on the model\u2019s past predictions. 6113 ing model ( Lee et al. , 2018 ) by gaps of 4-5 BLEU points on the WMT\u201914 English-German transla- tion benchmark, and up to 3 BLEU points on WMT\u201916 English-Romanian, but with the same model complexity and decoding speed. When compared to standard autoregressive transformer models, CMLMs with mask-predict offer a trade- off between speed and performance, trading up to 2 BLEU points in translation quality for a 3x speed-up during decoding. 2", "contribution": "The paper presents a model and a parallel decoding algorithm for machine translation that can generate translations in a constant number of decoding iterations. The authors introduce conditional masked language models (CMLMs) that allow the model to predict, in parallel, any arbitrary subset of masked words in the target translation. They use transformer CMLMs and a new decoding algorithm called mask-predict, which repeatedly masks out and re-predicts the subset of words in the current translation that the model is least confident about. The approach outperforms the current state-of-the-art parallel decoding model by gaps of 4-5 BLEU points on the WMT\u201914 English-German translation benchmark and up to 3 BLEU points on WMT\u201916 English-Romanian, but with the same model complexity and decoding speed. CMLMs with mask-predict offer a trade-off between speed and performance, trading up to 2 BLEU points in translation quality for a 3x speed-up during decoding.", "title": "Mask-Predict: Parallel Decoding of Marjan Ghazvininejad Abstract Acknowledgements References", "words": 741, "abstract": "Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling ob- jective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target trans- lation. This approach allows for ef\ufb01cient it- erative decoding, where we \ufb01rst predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least con\ufb01- dent about. By applying this strategy for a con- stant number of iterations, our model improves state-of-the-art performance levels for non- autoregressive and parallel decoding transla- tion models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding signi\ufb01cantly faster. 1 1"}
{"introduction": "Neural machine translation (NMT) models ( Cho et al. , 2014 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2014 ) solve the machine translation problem with the Encoder-Decoder framework and achieve im- pressive performance on translation quality. Re- cently, the Transformer model ( Vaswani et al. , 2017 ) further enhances the translation perfor- mance on multiple language pairs, while suffer- ing from the slow decoding procedure, which re- Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. \u22c6 Corresponding Author Src und noch tragischer ist , dass es Oxford war \u00b7 \u00b7 \u00b7 Ref even more tragic is that it was Oxford \u00b7 \u00b7 \u00b7 NAT and more more more more that it was Oxford \u00b7 \u00b7 \u00b7 AR and , more tragic , Oxford was \u00b7 \u00b7 \u00b7 Table 1: A fragment of a long sentence translation. AR stands for the translation of the autoregressive Trans- former. The output of the NAT model contains re- peated translations of word \u2018more\u2019 and misses the word \u2018tragic\u2019. stricts its application scenarios. The slow decod- ing problem of the Transformer model is caused by its autoregressive nature, which means that the target sentence is generated word by word accord- ing to the source sentence representations and the target translation history. Non-autoregressive Transformer model ( Gu et al. , 2017a ) is proposed to accelerate the de- coding process, which can simultaneously gen- erate target words by discarding the autoregres- sive mechanism. Since the generation of target words is independent, NAT models utilize alter- native information such as encoder inputs ( Gu et al. , 2017a ), translation results from other sys- tems ( Lee et al. , 2018 ; Guo et al. , 2018 ) and la- tent variables ( Kaiser et al. , 2018 ) as decoder in- puts. Without considering the target translation history, NAT models are weak to exploit the tar- get words collocation knowledge and tend to gen- erate repeated target words at adjacent time steps ( Wang et al. , 2019 ). Over-translation and under- translation problems are aggravated and often oc- cur due to the above reasons. Table 1 shows an inferior translation example generated by a NAT model. Compared to the autoregressive Trans- former, NAT models achieve signi\ufb01cant speedup while suffering from a large gap in translation quality due to the lack of target sequential infor- mation. 3014 In this paper, we present two approaches to re- trieve the target sequential information for NAT models to enhance their translation ability and meanwhile preserve the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT ( Reinforce-NAT ) to reduce the variance and stabilize the training procedure. We leverage the sequence-level objectives (e.g., BLEU ( Pap- ineni et al. , 2002 ), GLEU ( Wu et al. , 2017 ), TER ( Snover et al. , 2006 )) instead of the cross-entropy objective to encourage NAT model to generate high quality sentences rather than the correct to- ken for each position. Secondly, we propose an in- novative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. The bottom layers of the FS-decoder run in parallel to keep the decoding speed and the top layer of the FS-decoder can ex- ploit target sequential information to guide the tar- get words generation procedure. We conduct experiments on three machine translation tasks (IWSLT16 En \u2192 De, WMT14 En \u2194 De, WMT16 En \u2192 Ro) to validate our pro- posed approaches. Experimental results show that the Reinforce-NAT surpasses the baseline NAT system by a signi\ufb01cant margin on the translation quality without decelerating the decoding speed, and the FS-decoder achieves comparable trans- lation capacity to the autoregressive Transformer with considerable speedup. 2", "contribution": "The paper proposes two approaches to enhance the translation ability of non-autoregressive Transformer (NAT) models while preserving their fast-decoding property. The first approach is a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce variance and stabilize the training procedure. The second approach is an innovative Transformer decoder named FS-decoder to fuse target sequential information into the top layer of the decoder. Experimental results show that Reinforce-NAT surpasses the baseline NAT system by a significant margin on translation quality without decelerating the decoding speed, and the FS-decoder achieves comparable translation capacity to the autoregressive Transformer with considerable speedup.", "title": "Retrieving Sequential Information for Non-Autoregressive Chenze Shao Abstract References", "words": 852, "abstract": "Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential informa- tion. Over-translation and under-translation errors often occur for the above reason, espe- cially in the long sentence translation scenario. In this paper, we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. Sec- ondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a sig- ni\ufb01cant margin on BLEU without decelerat- ing the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with consid- erable speedup. 1"}
{"introduction": "generate each target word conditioned on previously generated ones, non-autoregressive translation (Gu et al., 2018, NAT) models break the autoregressive factorization and produce the target words in parallel. x , the probability of generating its target sentence y with length T is de\ufb01ned by NAT as: p ( y | x ) = p L ( T | x ; \u03b8 )  \ufffd T t =1  p ( y t | x ; \u03b8 ) , where p L ( \u00b7 ) is a separate conditional distribution to predict the length of target sequence. As NAT models can predict all tokens independently and simultaneously, recent works have fully investigated their superiority on decoding ef\ufb01ciency (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Kasai et al., 2020; Sun et al., 2019; Shu et al., 2020; Ran et al., 2019). However, there still exists a gap between AT and NAT models in terms of effectiveness. information (key) given a target-side token (query) (Yang et al., 2020; Wang and Tu, 2020). Through qualitative and quantitative analyses, we found that it is dif\ufb01cult for the NAT decoder to adequately capture the source context due to the lack of autoregressive factorization. As shown in Table 1, when translating the Chinese word \u201c \u4ea4 \u5f80 \u201d, the source context word \u201c \u5973 \u5b69 \u201d should play a signi\ufb01cant role in predicting the candidate word \u201cdating\u201d. However, the NAT model inappropriately generates \u201csocializing with\u201d, resulting in lexical choice errors. As seen, the AT model gives relatively higher attention weights to local contexts on the source side while the NAT model pays less attention on them (0.15 vs 0.04). We make further statistical analysis in Section 2 to prove the universality of this localness perception problem. Similar to our \ufb01ndings, Li et al. (2019) showed that distributions of cross-attention in NAT models are more ambiguous than those in AT ones. to model both local and global contexts simultaneously. For local attention, we limit the scope of cross-attention to adjacent tokens surrounding the source word with the maximum alignment probability. Section 3). \u2217 Work done when interning at Tencent AI Lab. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ . 4397 Input \u5f17 \u5170\u514b \u627e \u5230 \u4e00 \u95f4 \u516c \u5bd3 \uff0c \u540c \u65f6 \u5728 \u8ddf \u4e00\u4e2a \u5973 \u5b69 \u4ea4 \u4ea4 \u4ea4 \u5f80 \u5f80 \u5f80 \u3002 Reference Frank found an apartment and was dating a girl at the same time. NAT Output Frank found an apartment and was socializing with a girl. Attention \u4ea4 \u5f80 0 . 68 \u5f17 \u5170\u514b 0 . 18 \u5973 \u5b69 0 . 04 AT Output Frank found an apartment and was dating a girl. Attention \u4ea4 \u5f80 0 . 81 \u5973 \u5b69 0 . 15 \u5f17 \u5170\u514b 0 . 03 Ours Output Frank found an apartment and was dating a girl. Attention \u4ea4 \u5f80 0 . 69 \u5973 \u5b69 0 . 11 \u5f17 \u5170\u514b 0 . 09 NAT and AT models, respectively. \u201cAttention\u201d shows top-3 cross-attention probabilities when generating the target word \u201cdating\u201d or other equivalents. are conducted on four commonly-cited datasets on translation task (i.e. WMT16 Romanian \u21d2 \u21d2 \u21d2 \u21d2 0.5 BLEU point over advanced NAT models (in Section 4). Further analyses reveal that our method can enhance abilities of NAT to learn syntactic and semantic information as well as phrase patterns (in Section 5). 2", "contribution": "Main contributions of this paper are:\n\n1. Proposing a novel approach to address the localness perception problem in non-autoregressive translation (NAT) models by incorporating both local and global contexts simultaneously.\n\n2. Conducting qualitative and quantitative analyses to demonstrate the effectiveness of the proposed approach in improving the accuracy of NAT models in capturing source context and generating target words.\n\n3. Showing that the proposed approach outperforms state-of-the-art NAT models on four commonly-cited datasets on translation task, achieving up to 0.5 BLEU point improvement.\n\n4. Providing further analyses to demonstrate that the proposed approach can enhance the ability of NAT models to learn syntactic and semantic information as well as phrase patterns.", "title": "Context-Aware Cross-Attention for Non-Autoregressive Translation Liang Ding Abstract Acknowledgements References", "words": 829, "abstract": "Non-autoregressive translation (NAT) signi\ufb01cantly accelerates the inference process by predicting the entire target sequence. However, due to the lack of target dependency modelling in the decoder, the conditional generation process heavily depends on the cross-attention. In this paper, we reveal a localness perception problem in NAT cross-attention, for which it is dif\ufb01cult to adequately capture source context. To alleviate this problem, we propose to enhance signals of neighbour source tokens into conventional cross-attention. Experimental results on several representative datasets show that our approach can consistently improve translation quality over strong NAT baselines. source contexts by leveraging both local and global information. 1"}
{"introduction": ", 2018 ), which aim to enable the parallel genera- tion of output tokens without sacri\ufb01cing translation quality, have attracted much attention. Although the non-autoregressive models have considerably sped up the inference process for real-time neural machine translation (NMT) ( Gu et al. , 2018 ), their performance is considerably worse than that of autoregressive counterparts. Most previous works attribute the poor performance to the inevitable con- ditional independence issue when predicting target tokens, and many variants have been proposed to solve it. For example, several techniques in non- autoregressive models are investigated to mitigate the trade-off between speedup and performance, including iterative re\ufb01nement ( , 2018 ), insertion-based models ( , 2019 ; et al. , 2019 ), latent variable based models ( et al. , 2018 ; , 2020 ), CTC models ( bovick \u00b4 y and Helcl , 2018 ; , 2020 ), al- ternative loss function based models ( , 2019 ; , 2019 ; , 2020 ), and masked language models ( , 2019 , 2020 ). performance gap between autoregressive and non- autoregressive models, and have achieved some improvements on machine translation, the non- autoregressive models still suffer from syntactic and semantic limitations. That is, the translations of non-autoregressive models tend to contain inco- herent phrases (e.g., repetitive words), and some informative tokens on the source side are absent. token in the target sentence is generated indepen- dently. Consequently, it will cause the multimodal- ity issue, i.e., the non-autoregressive models can- not model the multimodal distribution of target sequences properly ( Gu et al. , 2018 ). One key observation to mitigate the syntactic and semantic error is that source and target translated sentences follow the same structure, which can be re\ufb02ected from Part-Of-Speech (POS) tags and POS, which aims to assign labels to words to in- dicate their categories by considering the long- distance structure of sentences, can help the model learn the syntactic structure to avoid generating the repetitive words. Likewise, NER, which dis- covers the proper nouns and verbs of sentences, naturally helps the model recognize some meaning- ful semantic tokens that may improve translation quality. This observation motivates us to leverage the syntactic as well as semantic structures of nat- ural language to improve the performance of non- 1236 Table 1: A motivating example on WMT14 En \u2192 De dataset. English with POS | NER and its corresponding German translation with POS | NER. The Blue labels show the same tags, while the Red labels show the different tags in two languages. EN: A republican strategy to counter the rel-election of Obama . | | | | | | | | | | EN POS: DET ADJ NOUN PART VERB DET NOUN ADP PROPN PUNCT EN NER: O B NORP O O O O O O B PERSON O DE: Eine republikanische strategie gegen die wiederwahl Obama . | | | | | | | | DE POS: DET ADJ NOUN ADP DET NOUN PROPN PUNCT EN NER: O B NORP O O O O B PERSON O autoregressive NMT. We present a motivating ex- ample in Table 1 to better illustrate our idea. From this table, we can \ufb01nd that although the words are altered dramatically from the English sentence to its German translation, the corresponding POS and NER tags still remain similar. For example, most POS tags are identical and follow the same pattern, except that PART, VERB, and ADP in the English do not match the German ADP, while the NER tags are exactly the same in both sentences. S yntactic and semantic structure-aware N on- A utoregressive T ransformer model ( SNAT ) for NMT. We take the structure labels and words as inputs of the model. With the guidance of extra sentence structural information, the model greatly mitigates the multimodality issue\u2019s negative impact. The core contributions of this paper can be summa- rized as that we propose 1) a syntax and semantic structure-aware Transformer which takes sequen- tial texts and the structural labels as input and gen- erates words conditioned on the predicted structural labels, and 2) an intermediate alignment regular- ization which aligns the intermediate decoder layer with the target to capture coarse target informa- tion. We conduct experiments on four benchmark tasks over two datasets, including WMT14 En \u2192 De and WMT16 En \u2192 cate that our proposed method achieves competitive results compared with existing state-of-the-art non- autoregressive and autoregressive neural machine translation models, as well as signi\ufb01cantly reduces the decoding time. 2", "contribution": "The paper proposes a Syntax and Semantic Structure-Aware Non-Autoregressive Transformer Model (SNAT) for Neural Machine Translation (NMT) that takes sequential texts and structural labels as input and generates words conditioned on predicted structural labels. The paper also introduces an intermediate alignment regularization that aligns the intermediate decoder layer with the target to capture coarse target information. The proposed method achieves competitive results compared to existing state-of-the-art non-autoregressive and autoregressive NMT models and significantly reduces decoding time.", "title": "Enriching Non-Autoregressive Transformer with Syntactic and Semantic Ye Liu Abstract Acknowledgements References", "words": 1018, "abstract": "The non-autoregressive models have boosted the ef\ufb01ciency of neural machine translation through parallelized decoding at the cost of effectiveness, when comparing with the au- toregressive counterparts. In this paper, we claim that the syntactic and semantic struc- tures among natural language are critical for non-autoregressive machine translation and can further improve the performance. How- ever, these structures are rarely considered in existing non-autoregressive models. Inspired by this intuition, we propose to incorporate the explicit syntactic and semantic structures of languages into a non-autoregressive Trans- former, for the task of neural machine transla- tion. Moreover, we also consider the interme- diate latent alignment within target sentences to better learn the long-term token dependen- cies. Experimental results on two real-world datasets (i.e., WMT14 En-De and WMT16 En- Ro) show that our model achieves a signi\ufb01- cantly faster speed, as well as keeps the transla- tion quality when compared with several state- of-the-art non-autoregressive models. 1"}
{"introduction": "026 027 with the development of deep learning. 028 traditional neural machine translation mod- 029 els ( , 2014 ; , 2015 ; 030 , 2016 ; , 2017 ) are autore- 031 gressive (AT), which means that they predict target 032 tokens one by one based on source tokens and pre- 033 viously predicted tokens. This dependence leads 034 to the limitation of translation speed, and the time 035 required for translation is directly proportional to 036 the sentence length. 037 038 tion (NAT) becomes a research hotspot. The non- 039 autoregressive generation mode eliminates token 040 dependency in the target sentence and generates all 041 ative pairs. (a) Contrastive Common Mask. (b) Con- trastive Dropout. tokens in parallel, considerably improving trans- 042 lation speed. However, the increase in speed is 043 accompanied with a decrease in translation qual- 044 ity. Many iterative models have been developed 045 to make a trade-off between translation speed and 046 quality. The iterative model improves translation 047 quality by continually and iteratively optimizing 048 the generated target sentence. The iterative model 049 is usually to predict the masked token in the target 050 sentence, such as BERT ( , 2019 ). 051 052 A sentence can be masked in a variety of ways. In 053 different masked sequences of the same sentence, 054 the representation of the same masked token should 055 be similar because they are from the same token 056 and have the same semantics in a similar context 057 (the same source sentence and the different masked 058 results of the same target sentence). We think about 059 how to make these different representations of the 060 same token more similar. Inspired by the success- 061 ful use of contrastive learning in NLP pre-trained 062 models (e.g., , 2021 ), We explore combin- 063 1 ing contrastive learning and the conditional masked 064 language model, treating different representations 065 of the same masked token as positive pairs and rep- 066 resentations of different tokens as negative pairs. 067 068 using contrastive learning. 069 1 , we propose two strate- 070 gies for constructing positive pairs in this paper. 071 072 lizes representations of the same token in differ- 073 ent masked sequences of the same sentence. As 074 shown in Figure 1 (a), \"fell\" is masked both in 075 \"he [mask] asleep almost [mask]\" and \"he [mask] 076 asleep [mask] instantly\" , which are different ran- 077 domly masked results of \"he fell asleep almost in- 078 stantly\" . The other is inspired by ( 2021 ), 079 where we feed the same input to the decoder twice 080 and get two different representations due to the 081 dropout setting, which we call Contrastive Dropout. 082 083 be similar, as shown in Figure 1 (b). 084 085 pairs to calculate the contrastive loss and jointly 086 optimize it with the cross-entropy loss. We verify 087 the effectiveness of our model in four translation 088 directions of two standard datasets with varying 089 data sizes. Experiments show that our model beats 090 CMLM with 0.80-1.04 BLEU margins at the same 091 translation speed. It also outperforms other CMLM- 092 based models and beats the state-of-the-art NAT 093 model on WMT\u201916 Ro-En (34.18 BLEU). We will 094 make our code publicly available. 095 096 cluded as follows: 097 \u2022 098 \ufb01rst effort to combine token-level contrastive 099 learning and the conditional masked language 100 model. 101 \u2022 102 pairs for the contrastive conditional masked 103 language model: Contrastive Common Mask 104 and Contrastive Dropout. 105 \u2022 106 signi\ufb01cant improvement with margins rang- 107 ing from 0.80-1.04 BLEU in four translation 108 directions and is state-of-the-art on WMT\u201916 109 110 2", "contribution": "The main contributions of this academic paper are:\n\n1. The proposal of a new model that combines token-level contrastive learning and the conditional masked language model to improve translation speed and quality.\n2. The introduction of two strategies for constructing positive pairs in the contrastive conditional masked language model: Contrastive Common Mask and Contrastive Dropout.\n3. The demonstration of the effectiveness of the proposed model in four translation directions of two standard datasets with varying data sizes, outperforming other CMLM-based models and beating the state-of-the-art NAT model on WMT\u201916 Ro-En (34.18 BLEU).\n4. The release of the code for the proposed model to the public.", "title": "Contrastive Conditional Masked Language Model for Non-autoregressive Anonymous ACL submission Abstract References", "words": 888, "abstract": "001 ing in natural language processing, we in- 002 corporate contrastive learning into the condi- 003 tional masked language model which is exten- 004 sively used in non-autoregressive neural ma- 005 chine translation (NAT) that we term Con- 006 trastive Conditional Masked Language Model 007 (CCMLM). CCMLM optimizes the similar- 008 ity of several different representations of the 009 same token in the same sentence, resulting in a 010 richer and more robust representation. We pro- 011 pose two methods to obtain various represen- 012 tations: Contrastive Common Mask and Con- 013 trastive Dropout. Positive pairs are various dif- 014 ferent representations of the same token, while 015 negative pairs are representations of different 016 tokens. In the feature space, the model with 017 contrastive loss pulls positive pairs together 018 and pushes negative pairs away. We conduct 019 extensive experiments on four translation di- 020 rections with different data sizes. The results 021 demonstrate that CCMLM showed a consis- 022 tent and signi\ufb01cant improvement with margins 023 ranging from 0.80-1.04 BLEU and is state-of- 024 the-art on WMT\u201916 Ro-En (34.18 BLEU). 025 1"}
{"introduction": ", 2017 ; Gu et al. , 2017 ; Chen et al. , 2019 ; Ren et al. , 2019 ), which generate all the tokens in a target sequence in parallel and can speed up inference, are widely explored in natural language and speech processing tasks such as neural machine transla- tion (NMT) ( Gu et al. , 2017 ; Lee et al. , 2018 ; Guo et al. , 2019a ; , 2019 ; , 2019b ; , 2019b ), automatic speech recognition (ASR) ( Chen et al. , 2019 ) and text to speech (TTS) synthesis ( Oord et al. , 2017 ; Ren et al. , 2019 ). How- ever, NAR models usually lead to lower accuracy than their autoregressive (AR) counterparts since the inner dependencies among the target tokens are explicitly removed. Several techniques have been proposed to allevi- ate the accuracy degradation, including 1) knowl- edge distillation ( Oord et al. , 2017 ; Gu et al. , 2017 ; , 2019a , b ; , 2019 ), 2) impos- ing source-target alignment constraint with fertil- ity ( , 2017 ), word mapping ( , 2019a ), attention distillation ( , 2019b ) and duration prediction ( , 2019 ). With the help of those techniques, it is observed that NAR models can match the accuracy of AR models for some tasks ( Ren et al. , 2019 ), but the gap still exists for some other tasks ( , 2017 ; , 2019 ). Therefore, several questions come out natu- rally: (1) Why the gap still exists for some tasks? than others? (2) Why the techniques like knowl- edge distillation and source-target alignment can help NAR generation? 150 The main difference between AR and NAR mod- els is that NAR models do not consider the depen- dency among target tokens, which is also the root cause of accuracy drop of NAR models. Thus, to better understand NAR sequence generation and an- swer the above questions, we need to characterize and quantify the target-token dependency, which turns out to be non-trivial since the sequences could be of different modalities (i.e., speech or text). COnditional Masked prediction model with Mix- Attention (CoMMA), inspired by the mix-attention in He et al. ( 2018 ) and the masked language mod- eling in ( 2018 ): in CoMMA, (1) the prediction of one target token can attend to all the source and target tokens with mix-attention, and 2) target tokens are randomly masked with vary- ing probabilities. CoMMA can help us to measure target-token dependency using the ratio of the atten- tion weights on target context over that on full (both source and target) context when predicting a tar- get token: bigger ratio, larger dependency among target tokens. We conduct a comprehensive study in this work and obtain several interesting discoveries that can answer previous questions. the rank of the target-token dependency among the three tasks is ASR > NMT > TTS: ASR has the largest dependency while TTS has the smallest. This \ufb01nding is consistent with the accuracy gap be- tween AR and NAR models and demonstrates the dif\ufb01culty of NAR generation across tasks. Second, we replace the target sequence of original training data with the sequence generated by an AR model (i.e., through knowledge distillation) and use the new data to train CoMMA; we \ufb01nd that the target- token dependency is reduced. Smaller target-token dependency makes NAR training easier and thus improves the accuracy. Third, source-target align- ment constraint such as explicit duration predic- tion ( Ren et al. , 2019 ) or implicit attention distilla- tion ( Li et al. , 2019b ) also reduces the target-token dependency, thus helping the training of NAR mod- els. lows: \u2022 conditional masked prediction model with mix-attention (CoMMA), to measure the token dependency for sequence generation. \u2022 three tasks, ASR is the most dif\ufb01cult and TTS is the least for NAR generation; 2) both knowl- edge distillation and imposing source-target alignment constraint reduce the target-token dependency, and thus reduce the dif\ufb01culty of training NAR models. 2", "contribution": "The main contributions of this paper are:\n\n1. The development of a Conditional Masked Prediction Model with Mix-Attention (CoMMA) to measure the token dependency for sequence generation.\n2. A comprehensive study of three tasks (ASR, NMT, and TTS) that shows ASR is the most difficult and TTS is the least for NAR generation.\n3. The finding that both knowledge distillation and imposing source-target alignment constraint reduce the target-token dependency, and thus reduce the difficulty of training NAR models.", "title": "A Study of Non-autoregressive Model for Sequence Generation Yi Ren Zhejiang University Jinglin Liu Zhejiang University Xu Tan Zhou Zhao Zhejiang University Sheng Zhao Tie-Yan Liu Abstract Acknowledgments References", "words": 951, "abstract": "Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, re- sulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different tech- niques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR mod- els in various tasks such as neural machine translation (NMT), automatic speech recogni- tion (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the dif\ufb01culty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge dis- tillation and source-target alignment can help NAR models. Since the main difference be- tween AR and NAR models is that NAR mod- els do not use dependency among target tokens while AR models do, intuitively the dif\ufb01culty of NAR sequence generation heavily depends on the strongness of dependency among tar- get tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the dif\ufb01culty of different NAR se- quence generation tasks. We have several inter- esting \ufb01ndings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token de- pendency while TTS has the least. 2) Knowl- edge distillation reduces the target-token de- pendency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency \u2217 Equal contribution. \u2020 Corresponding author of a target token on source tokens and thus eases the training of NAR models. 1"}
{"introduction": "popularity lately ( , 2018a ; et al. , 2019 ; Lee et al. , 2020 ; Qian et al. , 2021 ). Au- toregressive (AR) generation models ( et al. , 2014 ) need to iterate computations of for- ward propagation from the input to output layers of the decoder multiple times for all generated to- kens, whereas NAR models predict multiple tokens simultaneously. Hence, NAR decoding is faster as only one forward propagation computation is required. However, due to the inherent dif\ufb01culty in capturing the dependencies between generated tokens, the generation quality of NAR models is worse than that of AR models. The primary solution to this problem is to repeat decoding processes including edit operations, e.g., token substitution ( Ghazvininejad et al. , 2019 ; Qian et al. , 2021 ), insertion ( , 2019a ), and deletion/insertion ( Gu et al. , 2019 ). Repeating gen- eration processes decreases the dependencies be- tween output tokens by learning the conditional dis- tribution over the generated tokens ( , 2021 ). However, during the inference, the decoder must start the generation from scratch (or special tokens), which leads to a low-quality sentence gen- erated in the \ufb01rst iteration. Moreover, ( 2022 ) have demonstrated that if the initial sen- tence generated is of low quality, it is dif\ufb01cult to recover even after several iterations. Therefore, it is preferable to begin the generation process with a high-quality sentence to improve performance. Therefore, this paper proposes using the nearest neighbor as the initial state of the NAR decoder. The proposed method, NeighborEdit , retrieves the nearest neighbor of an input sentence and edits it to generate the output sentence. Thus, Neigh- borEdit can start the generation with a sentence that is close to that of the output. As generating from a neighbor example is easier than generating from scratch, we anticipate an improved generation quality with fewer iterations. Consider the follow- ing example where the English sentence \u201cI have an apple.\u201d is translated into the German sentence \u201cIch habe einen Apfel.\u201d NeighborEdit \ufb01rst retrieves the nearest neighbor of the English sentence from the training data, \u201cI have a banana. \u2013 Ich habe eine Banane.\u201d The method then starts with the retrieved German sentence, deletes the words \u2018eine\u2019 and \u2018Ba- nane\u2019 and inserts the words \u2018einen\u2019 and \u2018Apfel,\u2019 to complete the translation. lized in various tasks, including part-of-speech tag- ging ( , 1996 ) and example-based machine translation ( Nagao , 1984 ). Recently, near- est neighbors have been successfully applied to AR models, particularly to neural machine trans- lation ( , 2018b ; , 2020 ; arXiv:2208.12496v1  [cs.CL]  26 Aug 2022 wal et al. , 2021 ), text summarization ( , 2018 ; , 2019 ), and data-to-text genera- tion ( Wiseman et al. , 2021 ). We adapt the pioneer- ing idea to re\ufb01ne and improve NAR models. NeighborEdit is regarded as one of the edit-based NAR models that have been actively studied in recent years. In reality, it is not always possible to retrieve a neighbor close to the output sentence, and learning and applying editing operations to a distant sentence can be challenging. Therefore, we also propose a training strategy based on the lexical difference between the neighbor and the output sentences by mixing two policies, neighbor- and target-centric policies (described in Section 3.1.1 ). For the base architecture, we adopt the Levenshtein transformer ( , 2019 ), which permits two edit operations on input sentences: token deletion and token insertion. This model does not require that the output length be \ufb01xed, whereas most NAR models require that the output length be predicted. method (NeighborEdit) achieves higher transla- tion quality (1.69 points higher than the vanilla eighteenth fewer iterations) on the JRC-Acquis En- De dataset, the common benchmark dataset for ma- chine translation using nearest neighbors. We also con\ufb01rm the effectiveness of the proposed method on a data-to-text task (WikiBio). In addition, the proposed method outperforms an NAR baseline on the WMT\u201914 En-De dataset. Lastly, we report anal- ysis on neighbor examples utilized by the proposed method. 2", "contribution": "The main contributions of this paper are: proposing a method called NeighborEdit that uses the nearest neighbor as the initial state of the NAR decoder to improve generation quality with fewer iterations; introducing a training strategy based on the lexical difference between the neighbor and the output sentences by mixing two policies, neighbor- and target-centric policies; adopting the Levenshtein transformer as the base architecture, which permits two edit operations on input sentences; demonstrating the effectiveness of the proposed method on machine translation and data-to-text tasks, outperforming NAR baselines; and providing analysis on neighbor examples utilized by the proposed method.", "title": "Nearest Neighbor Non-autoregressive Text Generation", "words": 950, "abstract": "Non-autoregressive (NAR) models can gener- ate sentences with less computation than au- toregressive models but sacri\ufb01ce generation quality. Previous studies addressed this is- sue through iterative decoding. This study proposes using nearest neighbors as the ini- tial state of an NAR decoder and editing them iteratively. We present a novel train- ing strategy to learn the edit operations on neighbors to improve NAR text generation. Experimental results show that the proposed method (NeighborEdit) achieves higher trans- lation quality (1.69 points higher than the vanilla Transformer) with fewer decoding it- erations (one-eighteenth fewer iterations) on the JRC-Acquis En-De dataset, the common benchmark dataset for machine translation us- ing nearest neighbors. We also con\ufb01rm the ef- fectiveness of the proposed method on a data- to-text task (WikiBio). In addition, the pro- posed method outperforms an NAR baseline on the WMT\u201914 En-De dataset. We also re- port analysis on neighbor examples used in the proposed method. 1"}
{"introduction": "\nFast, accurate machine translation is a fundamental goal with a wide range of applications both in\nresearch and production. State-of-the-art neural machine translation systems generate translations\nautoregressively where words are predicted one-by-one conditioned on all previous words (Kalch-\nbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani\net al., 2017). This sequential property limits parallelization, since multiple tokens in each sentence\ncannot be generated in parallel. A \ufb02urry of recent work developed ways to (partially) parallelize\nthe decoder with non-autoregressive machine translation (NAR; Gu et al., 2018), thereby speeding\nup decoding during inference. NAR tends to suffer in translation quality because parallel decoding\nassumes conditional independence between the output tokens and prevents the model from properly\ncapturing the highly multimodal distribution of target translations (Gu et al., 2018).\n\nRecent work proposed methods to mitigate this multimodality issue, including iterative re\ufb01nement\n(e.g., Lee et al., 2018; Ghazvininejad et al., 2019), and modeling with latent variables (e.g., Ma\net al., 2019; Shu et al., 2020). These approaches modify the decoder transformer to \ufb01nd a balance\nbetween decoding parallelism and translation quality. In this work, however, we adopt a different\nspeed-quality tradeoff. Recent work by Kim et al. (2019) in autoregressive machine translation (AR)\nsuggests that better speed-quality tradeoffs can be achieved by having different depths in the encoder\nand the decoder. Here, we make a formal argument in favor of deep encoder, shallow decoder\ncon\ufb01gurations and empirically demonstrate better speed-quality tradeoffs for the AR baselines.\n\n\u2217 Work partially done at Facebook AI.\n\n1\n\n \n \n \n \n \n \n\fPublished as a conference paper at ICLR 2021\n\nWe provide extensive speed-quality comparisons between iterative NAR models and AR models\nwith varying numbers of encoder and decoder layers.\nIn particular, we use two types of speed\nmeasures for translation and discuss their relation to computational complexity. The two measures\nre\ufb02ect two different application scenarios: feeding one sentence at a time, and feeding as many\nwords as possible into the GPU memory. The \ufb01rst scenario is designed to simulate, for example,\ninstantaneous machine translation that translates text (or even speech) input from users. This is\nwhere current NAR models shine\u2014we can make full use of parallelism across decoding positions\nin a GPU. For this reason, much prior work in NAR only measures speed using this metric (e.g.,\nGu et al., 2018; 2019b; Kasai et al., 2020; Li et al., 2020). The second scenario aims at a situation\nwhere we want to translate a large amount of text as quickly as possible. In this case, we see that\nAR models run faster than NAR models by a large margin. Computation at each time step is large\nenough to exploit parallelism in a GPU, which cancels out the bene\ufb01t from parallel NAR decoding.\nFurther, AR models can cache all previous hidden states (Ott et al., 2019) and compute each step in\nlinear time complexity with respect to the sequence length. In contrast, NAR models necessitate a\nfresh run of quadratic self and cross attention in every decoding iteration.\n\nInterestingly, using a deep encoder and a shallow decoder in NAR models fails to retain the original\ntranslation accuracy by using 6 layers each (\u00a74.1). This suggests that departure from AR decoding\nnecessitates more capacity in the decoder; the strategy is effective speci\ufb01cally for AR models. In\nparticular, our analysis demonstrates that an NAR decoder requires more layers to learn target word\nordering (\u00a75). In summary, our contributions are the following:\n\n\u2022 We challenge three conventional assumptions in NAR evaluation: suboptimal layer alloca-\n\ntion, lack of distillation for AR baselines, and insuf\ufb01ciently general speed measures.\n\n\u2022 We provide a complexity analysis and identify an optimal layer allocation strategy that\n\nleads to better speed-quality tradeoffs, namely a deep-shallow con\ufb01guration.\n\n\u2022 We perform extensive analyses and head-to-head comparisons of AR and strong NAR mod-\nels on seven standard translation directions. We demonstrate that the accuracy gap between\nthe two model families is much wider than previously thought and that NAR models are\nunable to capture target word order well without suf\ufb01ciently deep decoders.", "contribution": "The main contributions of this paper are:\n\n1. Challenging three conventional assumptions in non-autoregressive (NAR) evaluation: suboptimal layer allocation, lack of distillation for autoregressive (AR) baselines, and insufficiently general speed measures.\n2. Providing a complexity analysis and identifying an optimal layer allocation strategy that leads to better speed-quality tradeoffs, namely a deep-shallow configuration.\n3. Performing extensive analyses and head-to-head comparisons of AR and strong NAR models on seven standard translation directions.\n4. Demonstrating that the accuracy gap between the two model families is much wider than previously thought and that NAR models are unable to capture target word order well without sufficiently deep decoders.", "title": "DEEP ENCODER, SHALLOW DECODER:\nREEVALUATING NON-AUTOREGRESSIVE MACHINE\nTRANSLATION", "words": 993, "abstract": "\nMuch recent effort has been invested in non-autoregressive neural machine trans-\nlation, which appears to be an ef\ufb01cient alternative to state-of-the-art autoregres-\nsive machine translation on modern GPUs. In contrast to the latter, where gen-\neration is sequential, the former allows generation to be parallelized across target\ntoken positions. Some of the latest non-autoregressive models have achieved im-\npressive translation quality-speed tradeoffs compared to autoregressive baselines.\nIn this work, we reexamine this tradeoff and argue that autoregressive baselines\ncan be substantially sped up without loss in accuracy. Speci\ufb01cally, we study au-\ntoregressive models with encoders and decoders of varied depths. Our extensive\nexperiments show that given a suf\ufb01ciently deep encoder, a single-layer autore-\ngressive decoder can substantially outperform strong non-autoregressive models\nwith comparable inference speed. We show that the speed disadvantage for au-\ntoregressive baselines compared to non-autoregressive methods has been overes-\ntimated in three aspects: suboptimal layer allocation, insuf\ufb01cient speed measure-\nment, and lack of knowledge distillation. Our results establish a new protocol for\nfuture research toward fast, accurate machine translation. Our code is available at\nhttps://github.com/jungokasai/deep-shallow."}
{"introduction": "Non-autoregressive translation (NAT) models achieve signi\ufb01cant decoding speedup in neural machine translation [NMT, 1 , 47 ] by generating target words simultaneously [ 17 ]. This advantage usually comes at the cost of translation quality due to the mismatch of training objectives. NAT models are typically trained with the cross-entropy loss, which forces the model outputs to be aligned verbatim with the target sentence and will highly penalize small shifts in word positions. The explicit alignment required by the cross-entropy loss cannot be guaranteed due to the multi-modality problem that there exist many possible translations for the same sentence [ 17 ], making the cross-entropy loss intrinsically mismatched with NAT. to designing better training objectives for NAT [ 29 , 40 \u2013 42 , 38 , 13 , 36 , 9 , 43 ]. Among them, latent alignment models [ 29 , 36 ] relax the alignment restriction by marginalizing out all monotonic latent alignments with the connectionist temporal classi\ufb01cation loss [CTC, 15 ], which receive much attention for the ability to generate variable length translation. model output and the target sentence. As illustrated in Figure 1a, the monotonic assumption holds in \u2217 Corresponding author: Yang Feng 2 Source code: https://github.com/ictnlp/NMLA-NAT. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2210.03953v1  [cs.CL]  8 Oct 2022 (a) CTC for ASR (b) CTC for NAT Figure 1: Illustration of the monotonic alignment assumption of CTC: (a) CTC for ASR where there is a natural monotonic mapping between the speech input and ASR target, (b) CTC for NAT where there exists global word reordering that induces non-monotonic alignment. \u03f5 is the blank token that means \u2018output nothing\u2019. classic application scenarios of CTC like automatic speech recognition (ASR) as there is a natural monotonic mapping between the speech input and ASR target. However, non-monotonic alignments are non-negligible in machine translation as there is typically global word reordering, which is a common source of the multi-modality problem. As Figure 1b shows, when the target sentence is \u201cI ate pizza this afternoon\u201d but the model produces a translation with a different but correct word ordering \u201cthis afternoon I ate pizza\u201d, the CTC loss cannot handle this non-monotonic alignment between output and target and wrongly penalizes the model. In this paper, we propose to model non-monotonic latent alignments for non-autoregressive machine translation. We \ufb01rst extend the alignment space from monotonic alignments to non-monotonic alignments to allow for the global word reordering in machine translation. Without the monotonic structure, we have to optimize the best alignment found by the Hungarian algorithm [ 46 , 28 , 7 , 9 ] since it becomes dif\ufb01cult to marginalize out all non-monotonic alignments with dynamic programming. sentence. In practice, it is not necessary to have the translation include exact words as contained in the target sentence, but it would be favorable to have a large overlap between them. Therefore, we further extend the alignment space by considering all alignments that overlap with the target sentence. metrics (e.g., BLEU). We accumulate n-grams from all alignments regardless of their positions and non-monotonically match them to target n-grams. The latent alignment model is trained to maximize the F1 score of n-gram matching, which re\ufb02ects the translation quality to a certain extent [32]. We conduct experiments on major WMT benchmarks for NAT (WMT14 En \u2194 De, WMT16 En \u2194 Ro), which shows that our method substantially improves the translation performance and achieves comparable performance to autoregressive Transformer with only one-iteration parallel decoding. 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a method to model non-monotonic latent alignments for non-autoregressive machine translation, which extends the alignment space from monotonic to non-monotonic alignments to allow for global word reordering in machine translation.\n\n2. Extending the alignment space further by considering all alignments that overlap with the target sentence, and training the latent alignment model to maximize the F1 score of n-gram matching, which reflects the translation quality to a certain extent.\n\n3. Conducting experiments on major WMT benchmarks for NAT (WMT14 En \u2194 De, WMT16 En \u2194 Ro), which show that the proposed method substantially improves the translation performance and achieves comparable performance to autoregressive Transformer with only one-iteration parallel decoding.", "title": "Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation", "words": 823, "abstract": "entropy loss, which forces the model outputs to be aligned verbatim with the target sentence and will highly penalize small shifts in word positions. Latent alignment models relax the explicit alignment by marginalizing out all monotonic latent alignments with the CTC loss. However, they cannot handle non-monotonic alignments, which is non-negligible as there is typically global word reordering in machine translation. In this work, we explore non-monotonic latent alignments for NAT. We extend the alignment space to non-monotonic alignments to allow for the global word reordering and further consider all alignments that overlap with the target sentence. We non-monotonically match the alignments to the target sentence and train the latent alignment model to maximize the F1 score of non- monotonic matching. Extensive experiments on major WMT benchmarks show that our method substantially improves the translation performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14 En-De with only one- iteration decoding, closing the gap between non-autoregressive and autoregressive models. 2 1"}
{"introduction": "Neural Machine Translation (NMT) has witnessed rapid progress in recent years [Bahdanau et al. , 2015; Gehring et al. , 2017; Vaswani et al. , 2017]. Typically, NMT mod- els adopt the encoder-decoder framework [Bahdanau et al. , 2015], and the decoder generates a target sentence in an au- toregressive manner [Bahdanau et al. , 2015; Vaswani et al. , 2017], where the generation of the current token depends on previous tokens and the source context from the encoder. While the accuracy of NMT models achieve human parity, \u2217 Equal contribution. \u2020 Corresponding author they usually suffer from high inference latency due to autore- gressive generation. Therefore, non-autoregressive transla- tion (NAT) [Gu et al. , 2018; Guo et al. , 2019a; Wang et al. , 2019; Ma et al. , 2019; Ren et al. , 2019] has been proposed to generate target tokens in parallel, which can greatly speed up the inference process. However, the accuracy of NAT models still lag behind that of the autoregressive translation (AT) models, due to the previous target tokens are removed from conditional depen- dency. A variety of works have tried to improve the accu- racy of NAT, including enhanced decoder input with embed- ding mapping [Guo et al. , 2019a], generative \ufb02ow [Ma et al. , 2019], and iterative re\ufb01nement [Ghazvininejad et al. , 2019; Lee et al. , 2018], etc. However, none of these works leverage the task relationship between AT and NAT when designing their methods. As AT models are more accurate and easier to train than NAT models due to the explicit dependency on previous tokens, a natural idea is to \ufb01rst train the model with easier AT, and then continue to train it with harder NAT. AT and NAT can be regarded as two tasks that are far differ- ent from each other, which makes it less bene\ufb01cial to directly shift to NAT training right after AT training. How to smoothly shift the model training from AT to NAT is critical for the \ufb01- nal accuracy. In this paper, we introduce semi-autoregressive translation (SAT) [Wang et al. , 2018], which only generates a part of the tokens in parallel at each decoding step, as in- termediate tasks to bridge the shift process from AT to NAT. Speci\ufb01cally, we de\ufb01ne a parameter k to represent the degree of parallelism for each task, and view different tasks under a uni\ufb01ed perspective: k = 1 represents AT, k = N rep- resents NAT where N is the length of target sentence, and 1 < k < N represents SAT. Intuitively, a task with smaller k is easier to train and achieves higher accuracy, while that with larger k is harder to train and results in worse accu- racy [Wang et al. , 2018], which forms a good curriculum to train the model from easy to hard. Inspired by this, we propose a task-level curriculum learn- ing for non-autoregressive translation (TCL-NAT), which trains the model with sequentially increased k . We divide the training procedure into three phases: AT training ( k = 1 ), SAT training ( 1 < k < N ) and NAT training ( k = N ). SAT training consists of multiple stages, where we shift k gradu- ally and exponentially as k = 2 , 4 , 8 , ..., 16 . To \ufb01nd the best schedule strategy to shift k , we design different pacing func- Proceedings of the Twenty-Ninth International Joint Conference on Arti\ufb01cial Intelligence (IJCAI-20) 3861 tions to control the training steps for each k , including linear, logarithmic and exponential functions. On the other hand, to smooth the shift process and reduce the gap between different stages, we further introduce a parameter called task window w , which represents the number of tasks training at the same time in each stage. For example, when w = 2 , we train the model with k = 1 , 2 for the \ufb01rst stage and k = 2 , 4 for the second stage, and so on. We implement TCL-NAT on Transformer model [Vaswani et al. , 2017]. In order to support different k in the same model, we introduce a causal- k self-attention mechanism in the Transformer decoder. We conduct experiments on four translation datasets including IWSLT14 German-English (De-En), IWSLT16 English-German (En-De), WMT14 English-German (En-De) and WMT14 German-English (De- En) to demonstrate the effectiveness of our method. The ex- periment results show that our method can achieve signi\ufb01cant improvement over NAT baselines and also outperform state of the art NAT models, without sacri\ufb01cing the inference speed. Speci\ufb01cally, we outperform the state of art NAT model [Guo et al. , 2019b] by 1.88 BLEU on the IWSLT14 De-En task, and reduce the accuracy gap between AT and NAT models to nearly 1 BLEU point on IWSLT16 En-De and WMT14 En- De tasks. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing semi-autoregressive translation (SAT) as an intermediate task to bridge the shift process from autoregressive translation (AT) to non-autoregressive translation (NAT).\n2. Proposing a task-level curriculum learning for non-autoregressive translation (TCL-NAT), which trains the model with sequentially increased degree of parallelism (k) for each task.\n3. Designing different pacing functions to control the training steps for each k and introducing a parameter called task window (w) to smooth the shift process and reduce the gap between different stages.\n4. Implementing TCL-NAT on Transformer model and introducing a causal-k self-attention mechanism in the Transformer decoder to support different k in the same model.\n5. Conducting experiments on four translation datasets and demonstrating that TCL-NAT can achieve significant improvement over NAT baselines and also outperform state of the art NAT models, without sacrificing the inference speed.", "title": "Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Jinglin Liu Abstract Acknowledgments References", "words": 1173, "abstract": "Non-autoregressive translation (NAT) achieves faster inference speed but at the cost of worse ac- curacy compared with autoregressive translation (AT). Since AT and NAT can share model struc- ture and AT is an easier task than NAT due to the explicit dependency on previous target-side tokens, a natural idea is to gradually shift the model train- ing from the easier AT task to the harder NAT task. To smooth the shift from AT training to NAT train- ing, in this paper, we introduce semi-autoregressive translation (SAT) as intermediate tasks. SAT con- tains a hyperparameter k , and each k value de\ufb01nes a SAT task with different degrees of parallelism. Specially, SAT covers AT and NAT as its special cases: it reduces to AT when k = 1 and to NAT when k = N ( N is the length of target sentence). We design curriculum schedules to gradually shift k from 1 to N , with different pacing functions and number of tasks trained at the same time. We called our method as task-level curriculum learning for NAT (TCL-NAT). Experiments on IWSLT14 De- En, IWSLT16 En-De, WMT14 En-De and De-En datasets show that TCL-NAT achieves signi\ufb01cant accuracy improvements over previous NAT base- lines and reduces the performance gap between NAT and AT models to 1-2 BLEU points, demon- strating the effectiveness of our proposed method. 1"}
{"introduction": ", 2018 ) introduce a parallel decoding paradigm with higher decoding ef\ufb01ciency (> 10 \u00d7 ) than autore- gressive models ( , 2015 ; et al. , 2017 ; , 2017 ). Unlike autore- gressive models, NAT models impose conditional independence assumptions in words to support par- allel decoding of sentences during inference. It attracts many researchers to explore NAT in ma- chine translation ( Gu et al. , 2018 ; Lee et al. , 2018 ; Kaiser et al. , 2018 ) and text-to-speech tasks ( Chen et al. , 2019 ; Peng et al. , 2020 ). Amount of researchers devoted themselves to im- prove the NATs\u2019 inferior generation quality. Such as modeling word inter-dependencies by curricu- lum learning ( , 2020a ; , 2020 ) or iterative re\ufb01nements mechanism ( Ghazvininejad * Shujian Huang is the corresponding author. \u2020 Work is done while at ByteDance AI Lab. \u2021 The implementation of latent- GLAT will be released at https://github.com/baoy-nlp/Latent-GLAT . et al. , 2019 ; , 2020b ), introducing latent variables to decompose target sentences and serve as the springboard for decoding ( , 2019 ; Ma et al. , 2019 ; Bao et al. , 2021 ), and introduce in- ductive bias for models\u2019 training ( Wei et al. , 2019 ; Li et al. , 2019 ). The most successful method is the glancing transformer (GLAT, , 2021a ), which trains the NAT model by sampling partial tar- get words as inputs to predict the remaining target words, explicitly building dependencies between the observed and unobserved words. ( 2021b ) employ GLAT to achieve impressive re- sults on the translation task of WMT21 1 , even out- performing many strong autoregressive translation systems in BLEU score ( Papineni et al. , 2002 ). Although existing NAT models achieve competi- tive results compared to autoregressive models in translation tasks, it is not negligible that they still need the help of an autoregressive Transformer (AT, , 2017 ) as a teacher for training, i.e., sequence-level knowledge distillation ( , 2016 ). A well-recognized explanation is a multi-modality problem ( Zhou et al. , 2020 ; Sun and , 2020 ): each input may have multiple valid outputs in datasets, which will prevent NAT mod- els from learning to organize consistent outputs. Training with the outputs of an AT can directly by- pass the multi-modal phenomenon in the dataset, effectively improving the models\u2019 performances. distillation are limited. First, it needs to train an extra AT model, which inevitably enlarges the train- ing cost. Second, it is hard to promise that the teacher (or AT) model can be accurate enough in all text generation settings, which will become the bottleneck for its student NAT model. Therefore, training a model from scratch without the help of an AT model is still an open and interesting problem. latent- GLAT, which can directly learn from the raw dataset. It alleviates 1 http://statmt.org/wmt21/ arXiv:2204.02030v1  [cs.CL]  5 Apr 2022 the multi-modality problem following a divide-and- conquer spirit, introducing a small set of discrete latent variables to capture the target word categor- ical information and divide the origin goal into latent variables modeling and sentence reconstruc- tion. First, the categorical information may have fewer multi-modality phenomena than the origi- nal words, thus can be learned directly without the help of knowledge distillation. Second, the word categorical information is informativeness to the sentence reconstruction. We can extend glanc- ing training with these discrete latent variables for modeling the sentence, encouraging the model to build dependencies on word categorical informa- tion rather than words, which works more robustly. Experiment results on WMT14, Quora, and Dai- lyDialog datasets show that latent- GLAT achieves remarkable improvements over several strong base- lines, verifying the effectiveness of latent- GLAT. More impressively, latent- GLAT even outperforms autoregressive models in Quora and DailyDialog datasets, further validating our motivation for re- moving knowledge distillation. In-depth analyses indicate that the introduced discrete latent variables are helpful to alleviate the multi-modality problem and are necessary for performance improvement. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing a new model called latent-GLAT that can learn directly from the raw dataset without the help of an autoregressive Transformer (AT) model as a teacher for training, by introducing a small set of discrete latent variables to capture the target word categorical information and divide the original goal into latent variables modeling and sentence reconstruction.\n\n2. Demonstrating that latent-GLAT achieves remarkable improvements over several strong baselines in machine translation and text-to-speech tasks, and even outperforms autoregressive models in Quora and DailyDialog datasets, validating the effectiveness of the introduced discrete latent variables in alleviating the multi-modality problem and improving performance.", "title": "latent-GLAT: Glancing at Latent Variables for Parallel Text Generation", "words": 964, "abstract": "Recently, parallel text generation has received widespread attention due to its success in gen- eration ef\ufb01ciency. Although many advanced techniques are proposed to improve its gen- eration quality, they still need the help of an autoregressive model for training to overcome the one-to-many multi-modal phenomenon in the dataset, limiting their applications. In this paper, we propose latent -GLAT, which em- ploys the discrete latent variables to capture word categorical information and invoke an advanced curriculum learning technique, alle- viating the multi-modality problem. Exper- iment results show that our method outper- forms strong baselines without the help of an autoregressive model, which further broadens the application scenarios of the parallel decod- ing paradigm.  \u2021 1"}
{"introduction": "Parallelization is the key ingredient for making deep learning models computationally tractable. While the advantages of parallelization are ex- ploited on many levels during training and infer- ence, autoregressive decoders require sequential execution. Training and inference algorithms in sequence- to-sequence tasks with recurrent neural networks (RNNs) such as neural machine translation (NMT) have linear time complexity w.r.t. the target se- quence length, even when parallelized ( Sutskever et al. , 2014 ; Bahdanau et al. , 2014 ). Recent approaches such as convolutional sequence-to-sequence learning ( Gehring et al. , 2017 ) or self-attentive networks a.k.a. the Trans- former ( Vaswani et al. , 2017 ) replace RNNs with parallelizable components in order to reduce the time complexity of the training. In these models, the decoding is still sequential, because the probability of emitting a symbol is conditioned on the previously decoded symbols. In non-autoregressive decoders, the inference algorithm can be parallelized because the decoder does not depend on its previous outputs. The apparent advantage of this approach is the near- constant time complexity achieved by the paral- lelization. On the other hand, the drawback is that the model needs to explicitly determine the target sentence length and reorder the state sequence be- fore it starts generating the output. In the current research contributions on this topic, these parts are trained separately and the inference is done in sev- eral steps. In this paper, we propose an end-to-end non- autoregressive model for NMT using Connec- tionist Temporal Classi\ufb01cation (CTC; Graves et al. 2006 ). The proposed technique achieves promising results on translation between English- Romanian and English-German on the WMT News task datasets. The paper is organized as follows. In Sec- tion 2 , we summarize the related work on non- autoregressive NMT. Section 3 describes the ar- chitecture of our proposed model. Section 4 presents details of the conducted experiments. The results are discussed in Section 5 . We conclude and present ideas for future work in Section 6 . 2", "contribution": "The main contribution of this paper is the proposal of an end-to-end non-autoregressive model for neural machine translation (NMT) using Connectionist Temporal Classification (CTC), which achieves promising results on translation between English-Romanian and English-German on the WMT News task datasets. The paper also summarizes related work on non-autoregressive NMT, describes the architecture of the proposed model, presents details of the conducted experiments, discusses the results, and presents ideas for future work.", "title": "End-to-End Non-Autoregressive Neural Machine Translation Jind\u02c7rich Libovick\u00b4y Abstract Acknowledgments References", "words": 467, "abstract": "Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols inde- pendently in parallel. We present a novel non- autoregressive architecture based on connec- tionist temporal classi\ufb01cation and evaluate it on the task of neural machine translation. Un- like other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English- German datasets. Our models achieve a signif- icant speedup over the autoregressive models, keeping the translation quality comparable to other non-autoregressive models. 1"}
{"introduction": "Neural machine translation (NMT) (Bahdanau, Cho, and Bengio 2014; Gehring et al. 2017; Shen et al. 2018; Vaswani et al. 2017; He et al. 2018; Hassan et al. 2018) has made rapid progress in recent years. The dominant approaches for NMT are based on autoregressive translation (AT), where the generation of the current token in the target sentence depends on the previously generated tokens as well as the source sentence. The conditional distribution of sentence generation in AT models can be formulated as: P ( y | x ) = T y \ufffd t =1 P ( y t | y <t , x ) , (1) where T y is the length of the target sentence which is im- plicitly decided by predicting the [ EOS ] token, and y <t rep- resents all generated target tokens before y t and x represents \u2217 Corresponding Author. Copyright c \u20dd 2020, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. the source sentence. Since AT model generates the target to- kens sequentially, the inference speed is a natural bottleneck for real-world machine translation systems. Recently, non-autoregressive translation (NAT) mod- els (Gu et al. 2017; Kaiser et al. 2018; Lee, Mansimov, and Cho 2018; Guo et al. 2019; Wang et al. 2019; Li et al. 2019) are proposed to reduce the inference latency by generat- ing all target tokens independently and simultaneously. In- stead of conditioning on previously generated target tokens, NAT models generate target tokens by taking other target- independent signals as the decoder input. In this way, the generation of y can be written as: P ( y | x ) = P ( T y | x ) \u00b7 T y \ufffd t =1 P ( y t | z, x ) , (2) where P ( T y | x ) is the explicit length prediction process for NAT models, and z represents the decoder input which is generated conditionally independent of y . As a result, the inference speed can be signi\ufb01cantly boosted. However, the context dependency within the target sentence is sacri\ufb01ced at the same time, which leads to a large degradation of the translation quality of NAT models. Therefore, improving the accuracy of NAT models becomes a critical research prob- lem. Considering that 1) NAT is a harder task than AT due to that the decoder in the NAT model has to handle the trans- lation task conditioned on less and weaker target-side in- formation; 2) AT models are of higher accuracy than NAT models; 3) NAT models (Gu et al. 2017; Guo et al. 2019; Wang et al. 2019) usually share the same encoder-decoder framework with AT models (Vaswani et al. 2017), it is very natural to \ufb01ne-tune a well-trained AT model for NAT, in or- der to transfer the knowledge learned in the AT model, espe- cially the ability of target language modeling and generation in the decoder. However, AT and NAT models differ a lot in training, and thus directly \ufb01ne-tuning a well-trained AT model does not lead to a good NAT model in general. To effectively transfer an AT model and obtain a good NAT model, we \ufb01rst note that there are two major differ- ences between NAT and AT models, as shown in Figure 1. \u2022 Decoder input : The decoder in AT models leverages the previous tokens as input while the decoder in NAT models 7839 Emb Emb Emb Emb Emb Soft Max Soft Max Soft Max Soft Max Soft Max y 1 y 2 y 3 y 4 EOS SOS y 1 y 2 y 3 y 4 AT Decoder \ufffd \ufffd Emb Emb Emb Emb Emb Soft Max Soft Max Soft Max Soft Max Soft Max y 1 y 2 y 3 y 4 EOS NAT Decoder \ufffd \ufffd z 1 z 2 z 3 z 4 z 5 Multi-Head Attention Multi-Head Attention Figure 1: The comparison between the decoders of AT models and NAT models. The red dashed line indicates the attention mask, and we only draw the masks of the \ufb01rst three tokens for simplicity. The blue dashed box indicates the decoder input. Best view in color. takes target-independent signals as input. Speci\ufb01cally, Gu et al. (2017) and Wang et al. (2019) take a copy of the source sentence x as the decoder input. \u2022 Attention mask : Each token can only attend to the tokens in its previous positions in AT models, while each token can attend to the tokens in all positions in NAT models. In order to handle the differences between the AT and NAT models during the \ufb01ne-tuning process, we introduce the idea of curriculum learning (Bengio et al. 2009) to make the transfer smooth and progressive. Speci\ufb01cally, we pro- pose two kinds of curriculums for the transfer from an AT model to an NAT model: \u2022 Curriculum for the decoder input : We \ufb01rst feed the tar- get sentence as AT models do, and then randomly substi- tute a number of tokens by the tokens in the copied source sentence, where the number of substituted tokens depends on a probability that is monotonically increasing w.r.t the training step. \u2022 Curriculum for the attention mask : We \ufb01rst train the model with the attention mask of AT models and switch to that of NAT models entirely after a pre-de\ufb01ned training step. In this way, we \ufb01rst train the translation model in an easier autoregressive generation, and gradually transfer to a harder non-autoregressive generation. We conduct ex- periments on four translation datasets including WMT14 English-German, WMT14 German-English and IWSLT14 German-English to verify the effectiveness of the proposed method, and our model outperforms all non-autoregressive baselines on these tasks. Speci\ufb01cally, we outperform the best NAT baseline (Wang et al. 2019) by 1 . 87 BLEU on the IWSLT14 De-En task and 1 . 14 BLEU on the WMT14 En-De task. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing the idea of curriculum learning to transfer knowledge from an autoregressive translation (AT) model to a non-autoregressive translation (NAT) model in a smooth and progressive manner.\n2. Proposing two kinds of curriculums for the transfer process: one for the decoder input and one for the attention mask.\n3. Conducting experiments on four translation datasets and showing that the proposed method outperforms all non-autoregressive baselines on these tasks, including the best NAT baseline by 1.87 BLEU on the IWSLT14 De-En task and 1.14 BLEU on the WMT14 En-De task.", "title": "Fine-Tuning by Curriculum Learning for Junliang Guo, Acknowledgements References", "words": 1355, "abstract": "Abstract Non-autoregressive translation (NAT) models remove the de- pendence on previous target tokens and generate all target tokens in parallel, resulting in signi\ufb01cant inference speedup but at the cost of inferior translation accuracy compared to autoregressive translation (AT) models. Considering that AT models have higher accuracy and are easier to train than NAT models, and both of them share the same model con- \ufb01gurations, a natural idea to improve the accuracy of NAT models is to transfer a well-trained AT model to an NAT model through \ufb01ne-tuning. However, since AT and NAT mod- els differ greatly in training strategy, straightforward \ufb01ne- tuning does not work well. In this work, we introduce curricu- lum learning into \ufb01ne-tuning for NAT. Speci\ufb01cally, we de- sign a curriculum in the \ufb01ne-tuning process to progressively switch the training from autoregressive generation to non- autoregressive generation. Experiments on four benchmark translation datasets show that the proposed method achieves good improvement (more than 1 BLEU score) over previous NAT baselines in terms of translation accuracy, and greatly speed up (more than 10 times) the inference process over AT baselines. 1"}
{"introduction": ", 2018 ) has received increasing attention in the \ufb01eld of neural machine translation for the property of parallel decoding. Despite the signi\ufb01cant speedup, NAT suffers from the performance degradation compared to autoregressive models ( et al. , 2015 ; , 2017 ) due to the multi- modality problem: the source sentence may have \u2217 Corresponding author: Yang Feng 1 Source code: https://github.com/ictnlp/DDRS-NAT. multiple correct translations, but the loss is cal- culated only according to the reference sentence. racy of the loss function since NAT has no prior knowledge about the reference sentence during the generation, where the teacher forcing algorithm ( , 1989 ) makes autoregressive models less affected by feeding the golden context. has been a central focus in recent efforts for im- proving NAT models ( , 2019 , 2020 , 2021 ; , 2020 ; , 2020 ; jad et al. , 2020 ; , 2021 ). A standard ap- proach is to use sequence-level knowledge distilla- tion ( , 2016 ), which attacks the multi- modality problem by replacing the target-side of the training set with the output from an autoregres- sive model. The distilled dataset is less complex and more deterministic ( , 2020 ), which becomes a default con\ufb01guration of NAT. How- ever, the multi-modality problem in the distilled dataset is still nonnegligible ( , 2020 ). to imitate the behavior of a speci\ufb01c autoregressive teacher, which limits the upper bound of the model capability and restricts the potential of developing stronger NAT models. enough and propose diverse distillation with ref- erence selection (DDRS) for NAT. Diverse distil- lation generates a dataset containing multiple ref- erence translations for each source sentence, and reference selection \ufb01nds the reference translation that best \ufb01ts the model output for the training. As illustrated in Figure 1 , diverse distillation provides candidate references \u201cI must leave tomorrow\" and \u201cTomorrow I must leave\", and reference selection selects the former which \ufb01ts better with the model output. More importantly, NAT with DDRS does not imitate the behavior of a speci\ufb01c teacher but learns selectively from multiple references, which arXiv:2205.14333v1  [cs.CL]  28 May 2022 I must  leave tomorrow I must  leave tomorrow I must leave tomorrow 0.3 0.5 0.1 0.1 0.1 0.1 0.7 0.1 0.1 0.1 0.2 0.6 0.6 0.1 0.1 0.2 Figure 1: Illustration of diverse distillation and reference selection. Diverse distillation provides multiple refer- ences and reference selection selects the one that best \ufb01ts the model output for the training. improves the upper bound of the model capability and allows for developing stronger NAT models. the task of diverse machine translation, which aims to generate diverse translations with high transla- tion quality ( , 2016 ; , 2018 ; , 2019 ; , 2020 ; , 2021 ). We propose a simple yet effective method called SeedDiv, which directly uses the random- ness in model training controlled by random seeds to produce diverse reference translations without losing translation quality. For reference selection, we compare the model output with all references and select the one that best \ufb01ts the model output, which can be ef\ufb01ciently conducted without extra neural computations. The model learns from all ref- erences indiscriminately in the beginning, and grad- ually focuses more on the selected reference that provides accurate training signals for the model. reinforcement learning, where we encourage the model to move towards the selected reference that gives the maximum reward to the model output. chine translation benchmarks to demonstrate the effectiveness of our method. On the competitive task WMT14 En-De, DDRS achieves 27.60 BLEU with 14 . 7 \u00d7 speedup and 28.33 BLEU with 5 . 0 \u00d7 speedup, outperforming the autoregressive Trans- former while maintaining considerable speedup. DDRS even achieves 29.82 BLEU with only one decoding pass, improving the state-of-the-art per- formance level for NAT by over 1 BLEU. 2", "contribution": "The main contributions of this academic paper are:\n\n1. Proposing a method called diverse distillation with reference selection (DDRS) for improving the performance of neural machine translation models with parallel decoding. DDRS generates a dataset containing multiple reference translations for each source sentence and selects the reference that best fits the model output for training. This method improves the upper bound of the model capability and allows for developing stronger NAT models.\n\n2. Proposing a simple yet effective method called SeedDiv for diverse machine translation, which directly uses the randomness in model training controlled by random seeds to produce diverse reference translations without losing translation quality.\n\n3. Introducing a reinforcement learning approach to encourage the model to move towards the selected reference that gives the maximum reward to the model output.\n\n4. Achieving state-of-the-art performance levels for NAT on competitive machine translation benchmarks, including WMT14 En-De, with considerable speedup. DDRS achieves 27.60 BLEU with 14.7\u00d7 speedup and 28.33 BLEU with 5.0\u00d7 speedup, while maintaining considerable speedup. DDRS even achieves 29.82 BLEU with only one decoding pass, improving the state-of-the-art performance level for NAT by over 1 BLEU.", "title": "One Reference Is Not Enough: Diverse Distillation with Reference", "words": 942, "abstract": "Non-autoregressive neural machine translation (NAT) suffers from the multi-modality prob- lem: the source sentence may have multi- ple correct translations, but the loss func- tion is calculated only according to the ref- erence sentence. Sequence-level knowledge distillation makes the target more determin- istic by replacing the target with the output from an autoregressive model. However, the multi-modality problem in the distilled dataset is still nonnegligible. Furthermore, learning from a speci\ufb01c teacher limits the upper bound of the model capability, restricting the poten- tial of NAT models. In this paper, we ar- gue that one reference is not enough and pro- pose diverse distillation with reference selec- tion (DDRS) for NAT. Speci\ufb01cally, we \ufb01rst propose a method called SeedDiv for diverse machine translation, which enables us to gener- ate a dataset containing multiple high-quality reference translations for each source sentence. During the training, we compare the NAT out- put with all references and select the one that best \ufb01ts the NAT output to train the model. Ex- periments on widely-used machine translation benchmarks demonstrate the effectiveness of DDRS, which achieves 29.82 BLEU with only one decoding pass on WMT14 En-De, improv- ing the state-of-the-art performance for NAT by over 1 BLEU. 1 1"}
{"introduction": ", 2019 ), pre-trained language models (PLMs) have achieved state-of-the-art performance across text generation tasks, which aim to generate human-like texts on demand ( , 2020 ; , 2022c ). auto-regressive (AR) fashion to generate texts token-by-token: the next token is predicted based on all previously gener- ated tokens. A major limitation of this approach is \u2217 Corresponding author that it is hard to be parallelized for the inference process, thus leading to a relatively high inference latency ( , 2018 ). Such a limitation prevents AR models from wide deployment in online real- time applications, such as query rewriting in search engines and online chat-bot. Moreover, AR models are prone to suffering from the exposure bias prob- lem since there is a gap between AR training and in- ference ( , 2021 ). These concerns have sparked extensive interests in non-autoregressive (NAR) models for text generation ( , 2018 ). target tokens in all positions simultaneously and independently ( , 2018 ). This full paral- lelism leads to an ef\ufb01cient and low-latency infer- ence process. However, the independence assump- tion prevents NAR models from learning the depen- dency among target tokens, resulting in accuracy degradation ( , 2022 ). One widely-used solution to improve the NAR generation quality is to iteratively re\ufb01ne outputs ( , 2019 ; , 2019 ), which however leads to the loss in the speed-up advantage. In addition, many studies aim to learn the input-output mapping for more accurate generation via embedding map- ping ( , 2019 ), latent alignment ( and Helcl , 2018 ), and discrete variables ( , 2019 ). While easing the dif\ufb01culty of NAR gener- ation to some extent, these methods still struggle for generating complex sentences. Therefore, in- spired by ( 2022 ), we argue that the key to NAR text generation is to enhance the learning of token dependency\u2014each token should be gener- ated depending on forward and backward generated tokens. E LMER : an E f\ufb01cient and Effective P LM for NAR t E xt gene R ation, to explicitly learn the bi-directional token dependency. taneously only at the last layer, thus making the token prediction unaware of tokens generated in arXiv:2210.13304v2  [cs.CL]  28 Oct 2022 other positions. To address this issue, we propose to generate tokens at different layers and the upper- layer token generation can depend on lower-layer generated tokens from both left and right. In this way, our model can explicitly learn the dependency between tokens from different layers while enjoy- ing full parallelism in NAR decoding, as shown in 1 . To this end, we propose to extend the early exit technique ( , 2021c ) to NAR text generation: if there is suf\ufb01cient con\ufb01dence to gen- erate a token at a lower layer, the model is allowed to exit at this layer and make the prediction without passing through the upper layers. for a token, we aim to predict each token at differ- ent layers for learning diverse token dependencies in NAR text generation. Thus, inspired by XLNet ( , 2019 ), we further propose a novel pre- training objective based on early exit, i.e., E LMER learn complex token dependencies. Given a sequence, LPLM will permute the exit layer (from 1 to the maximum layer) for each token and maxi- mize the NAR text generation probability w.r.t. all possible exit layer permutations of the sequence. ferent layers and attend to all other tokens from both forward and backward positions. In this way, LPLM could effectively capture diverse token de- pendencies from large-scale corpora. Pre-trained with the general LPLM, E LMER can adapt to down- stream text generation tasks and datasets by using speci\ufb01c early exit strategies. to introduce the idea of early exit to NAR text generation. We \ufb01ne-tune E LMER on three popu- lar text generation tasks. Experiments show that E LMER signi\ufb01cantly improves the best NAR mod- els by +4.71 ROUGE-1 on XSUM, +1.79 ME- TEOR on SQuAD v1.1, and +2.26 Distinct-2 on with auto-regressive PLMs ( e.g., E LMER (29.92) vs BART (30.61) ROUGE-L on XSUM) while achiev- ing over 10x faster inference. 2", "contribution": "The main contributions of this academic paper are:\n\n1. Introducing the idea of early exit to non-autoregressive (NAR) text generation to enhance the learning of token dependency and improve the generation quality.\n2. Proposing a novel pre-training objective based on early exit, called ELMER, to explicitly learn the bi-directional token dependency and capture diverse token dependencies from large-scale corpora.\n3. Developing an efficient and effective pre-trained language model (PLM) for NAR text generation, called LPLM, that generates tokens at different layers and allows upper-layer token generation to depend on lower-layer generated tokens from both left and right.\n4. Fine-tuning ELMER on three popular text generation tasks and achieving significant improvements over the best NAR models while achieving over 10x faster inference.", "title": "ELMER: A Non-Autoregressive Pre-trained Language Model for Ef\ufb01cient and Effective Text Generation", "words": 978, "abstract": "We study the text generation task under the approach of pre-trained language models (PLMs). Typically, an auto-regressive (AR) method is adopted for generating texts in a token-by-token manner. Despite many ad- vantages of AR generation, it usually suffers from inef\ufb01cient inference. Therefore, non- autoregressive (NAR) models are proposed to generate all target tokens simultaneously. However, NAR models usually generate texts of lower quality due to the absence of token dependency in the output text. In this paper, we propose E LMER : an E f\ufb01cient and effec- tive P LM for NAR t E xt gene R ation to explic- itly model the token dependency during NAR generation. By leveraging the early exit tech- nique, E LMER enables the token generations at different layers, according to their predic- tion con\ufb01dence (a more con\ufb01dent token will exit at a lower layer). Besides, we propose a novel pre-training objective, Layer Permuta- tion Language Modeling, to pre-train E LMER by permuting the exit layer for each token in sequences. Experiments on three text gen- eration tasks show that E LMER signi\ufb01cantly outperforms NAR models and further narrows the performance gap with AR PLMs ( e.g., E LMER (29.92) vs BART (30.61) ROUGE-L in XSUM) while achieving over 10 times in- ference speedup. 1"}
{"introduction": "Autoregressive translation (AT) models based on Transformer ( Vaswani et al. , 2017 ; So et al. , 2019 ; Sun et al. , 2022 ; Zhu et al. , 2021a ), where each gen- eration step depends on the previously generated to- kens, achieve state-of-the-art (SOTA) performance on most datasets for machine translation tasks. AT model can better model the process of translation generation but leads to a massive limitation of its inference speed. Therefore, the non-autoregressive translation (NAT) ( Gu et al. , 2018 ) model is proposed, which is 15.6 times faster than AT model. NAT assumes that the generated tokens are conditionally independent 1 Our code is released at https://github.com/ boom-R123/Candidate_Soups . Figure 1: Efficiency (Speedup) and Translation quality (BLEU) of NAT models in the WMT\u201914 EN-DE trans- lation dataset. A cross \u201c\u00d7\u201d represents our Candidate Soups (CDS) variants. Its base model is shown in the shape \u201c \u2022 \u201d, and its correspondence is represented by an arrow. CDS (mE-nD) refers to the AT model for re- scoring that has m encoder layers and n decoder layers. given the source sentence, so the translation can be generated in parallel, significantly improving its in- ference speed compared to AT. However, due to the strong independence assumption, the ability of the NAT model modeling sequence generation is weak- ened. So NAT model usually has multimodality problem ( Gu et al. , 2018 ) in the inference process, resulting in its performance worse than AT models. Several methods have been proposed to alleviate the multimodality problem and improve the perfor- mance of the NAT model, such as the iteration- based NAT model ( Ghazvininejad et al. , 2019 ; Gu et al. , 2019 ; Kasai et al. , 2020 ) and the semi- autoregressive translation model ( Wang et al. , 2018 ; Ran et al. , 2020 ). Most of the previous methods are modified from the model\u2019s perspective, either modifying the struc- ture of the model ( Shu et al. , 2020 ; Huang et al. , 2021 ; Zhu et al. , 2021b ) or modifying the training method of the model ( Du et al. , 2021 ; Qian et al. , 2021 ). Different from the previous methods, in this paper, we propose a simple but effective method: 4811 Src Die Beschaffung des erforderlichen Personalausweises kostet oft \u00fcber hundert Dollar. Candidate 1 It often costs over a hundred dollars to obtain the require identity card . Candidate 2 It often cost over a hundred dollars to obtain the required identity card . NPD It often cost over a hundred dollars to obtain the required identity card . CDS It often costs over a hundred dollars to obtain the required identity card . Figure 2: Comparison between NPD and Candidate Soups (CDS). Red fonts represent mistranslated tokens. Compared with NPD, Candidate Soups does not preserve the erroneous parts in the candidate results. Candidate Soups, which can significantly improve the translation quality without any modification to the model. Moreover, Candidate Soups is a general approach that can be used by any NAT model that can generate multiple candidate results, such as Vanilla NAT ( Gu et al. , 2018 ), GLAT ( Qian et al. , 2021 ), etc. The conventional recipe for maximizing trans- lation quality through candidate results is noisy parallel decoding (NPD) ( Gu et al. , 2018 ), which regards each candidate translation as an indepen- dent individual and ultimately only selects one of them as the final result and discards others. There- fore NPD can not utilize the valuable information in all the candidate translations. For example, there are a total of two candidate translations. The first candidate translation has the wrongly translated word in the second half, and the second candidate translation has the wrongly translated word in the first half. Using the NPD algorithm, in this case, can not get the correct translations (Figure 2 ). However, Candidate Soups will effectively use the valuable information of all the candidate trans- lations to fuse the different candidate results and ob- tain a higher-quality translation (Figure 2 ). Specifi- cally, Candidate Soups first finds the common sub- sequence among all candidate results. Based on the uncertainty of the model, we consider the com- mon subsequence to be the most confident part of the model\u2019s predictions, so we make it part of the final translation and use it to align the candi- date results. For the remaining parts, we select the part with the highest average log-probability among all candidate results to add to the final translation. Candidate Soups can be regarded as an implicit model ensemble method, which generates multiple different results by introducing uncertainty in the inference process, and further enhances the transla- tion quality by making full use of the information of multiple results. We conduct extensive experiments in two datasets commonly used in machine translation, WMT\u201914 EN\u2013DE and WMT\u201916 EN\u2013RO. The re- sults demonstrate that our proposed method can significantly improve the base models\u2019 translation quality on different tasks while maintaining the fast inference speed of the NAT model. Remarkably, our best variant achieves better performance than the AT teacher model on three translation tasks with 7.6 \u00d7 speedup. Figure 1 demonstrates the quality- speed trade-off compared with AT and recent NAT models. And relevant background knowledge is introduced in Appendix A . 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a simple and effective method called Candidate Soups to improve the translation quality of non-autoregressive translation (NAT) models without modifying the model's structure or training method.\n\n2. Demonstrating that Candidate Soups can be used by any NAT model that can generate multiple candidate results.\n\n3. Introducing a new approach to maximizing translation quality through candidate results that is more effective than the conventional noisy parallel decoding (NPD) algorithm.\n\n4. Conducting extensive experiments on two commonly used machine translation datasets and showing that Candidate Soups significantly improves the translation quality of NAT models while maintaining fast inference speed.\n\n5. Achieving better performance than the autoregressive translation (AT) teacher model on three translation tasks with 7.6 \u00d7 speedup.", "title": "Candidate Soups: Fusing Candidate Results Improves Translation Quality Huanran Zheng Abstract Limitations Ethics Statement Acknowledgements References", "words": 1197, "abstract": "Non-autoregressive translation (NAT) model achieves a much faster inference speed than the autoregressive translation (AT) model be- cause it can simultaneously predict all tokens during inference. However, its translation qual- ity suffers from degradation compared to AT. And existing NAT methods only focus on im- proving the NAT model\u2019s performance but do not fully utilize it. In this paper, we propose a simple but effective method called \u201cCandi- date Soups,\u201d which can obtain high-quality translations while maintaining the inference speed of NAT models. Unlike previous ap- proaches that pick the individual result and dis- card the remainders, Candidate Soups (CDS) can fully use the valuable information in the different candidate translations through model uncertainty. Extensive experiments on two benchmarks (WMT\u201914 EN\u2013DE and WMT\u201916 EN\u2013RO) demonstrate the effectiveness and gen- erality of our proposed method, which can sig- nificantly improve the translation quality of var- ious base models. More notably, our best vari- ant outperforms the AT model on three transla- tion tasks with 7.6 \u00d7 speedup. 1 1"}
{"introduction": "As a dominant pre-training paradigm in natural lan- guage processing (NLP), masked language models such as BERT and its variants ( Devlin et al. , 2019 ; , 2019 ), were initially proposed and have achieved state-of-the-art performance on various natural language understanding tasks. For gener- ation tasks, previous studies either leverage the pre-trained BERT as an external component for representation fusing ( , 2019 ) or simply initialize the generation models with a pre-trained model ( Ma et al. , 2020 ). Although straightforward, these methods suffer from either heavy compu- tation costs during inference or only supporting \u2217 Work done while at Microsoft Corporation. \u2020 Corresponding authors. autoregressive generation with sequential depen- dency. Additionally, these approaches result in inconsistency from two perspectives: 1) mismatch of architectures between pre-trained models and the generation model; 2) disagreement of training objectives between pre-training and autoregressive generation tasks. the problem from another point of view. We \ufb01nd that the training objective in pre-trained MLM can align well with the one in non-autoregressive gen- eration, which is an emergent generation paradigm due to its excellent inference speed-up ( , 2018 ; , 2018 ). Intuitively, both train- ing objectives are formalized as a series of inde- pendent token predictions in the output sequence. trained MLM into the non-autoregressive genera- tion. Speci\ufb01cally, in this work, we focus on the classic non-autoregressive machine translation task. formed very well, lagging behind the autoregres- sive translation counterpart. training model (XLMR) into NAT models with a lightweight, effective and user-con\ufb01gurable dec- orator. Following Occam\u2019s razor principle and to better leverage the capability of pre-training mod- els, we design the decorator component based on two key criteria: 1) involve additional trainable pa- rameters as few as possible, e.g., parameter-free; 2) keep the model intermediate representation consis- tent after using the decorator. Guided by the two criteria, the decorator consists of a distance-based latent transformation module and a position-wise add and scale module, which contains only one trainable scalar parameter. Moreover, the decorator can be \ufb02exibly incorporated into a user-speci\ufb01ed layer of XLMR to balance the translation perfor- mance and inference speed. We use the connec- tionist temporal classi\ufb01cation (CTC) ( Graves et al. , 6934 2006 ) loss as the training objective. we systematically conduct the evaluation on sev- eral widely-used translation datasets. Extensive experiments demonstrate that our proposed model signi\ufb01cantly and consistently improves the trans- lation performance and achieves new state-of-the- art results in both single-step and iterative NAT models. Results show that our single-step XLM- D model achieves 27 . 46 / 34 . 70 BLEU points on WMT14 En \u21d2 \u21d2 tion tasks with 19 . 9 \u00d7 speed-up. Encouragingly, our iterative XLM-D model obtains 29 . 80 / 35 . 65 points on both tasks and outperforms previous well- performed models (CMLM) ( Ghazvininejad et al. , 2019 ) by a large margin, i.e., 2 . 77 / 2 . 57 points. Fur- ther analyses reveal that our approach enhances the capability in long sentence translation and can be user-con\ufb01gured to balance the trade-off between the translation quality and inference speed. 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a new approach to non-autoregressive generation that aligns the training objective of pre-trained masked language models (MLM) with non-autoregressive generation, resulting in improved inference speed and consistency between pre-training and generation tasks.\n\n2. Introducing a lightweight, effective, and user-configurable decorator component that can be incorporated into a user-specified layer of the pre-trained MLM to improve translation performance and achieve state-of-the-art results in both single-step and iterative non-autoregressive machine translation models.\n\n3. Demonstrating the effectiveness of the proposed approach through extensive experiments on several widely-used translation datasets, showing significant and consistent improvements in translation performance and the ability to balance the trade-off between translation quality and inference speed.", "title": "XLM-D: Decorate Cross-lingual Pre-training Model as Yong Wang Abstract Limitations Acknowledgement References", "words": 763, "abstract": "Pre-training language models have achieved thriving success in numerous natural language understanding and autoregressive generation tasks, but non-autoregressive generation in applications such as machine translation has not suf\ufb01ciently bene\ufb01ted from the pre-training paradigm. In this work, we establish the connection between a pre-trained masked lan- guage model (MLM) and non-autoregressive generation on machine translation. From this perspective, we present XLM-D, which seam- lessly transforms an off-the-shelf cross-lingual pre-training model into a non-autoregressive translation (NAT) model with a lightweight yet effective decorator. Speci\ufb01cally, the dec- orator ensures the representation consistency of the pre-trained model and brings only one additional trainable parameter. Extensive ex- periments on typical translation datasets show that our models obtain state-of-the-art perfor- mance while realizing the inference speed- up by 19 . 9 \u00d7 . One striking result is that on WMT14 En \u21d2 De, our XLM-D obtains 29 . 80 BLEU points with multiple iterations, which outperforms the previous mask-predict model by 2 . 77 points. 1"}
{"introduction": "\nDespite the success of neural machine translation\n(NMT) (Bahdanau et al., 2015; Vaswani et al.,\n2017; Barrault et al., 2020), real applications usu-\nally require the precise (if not exact) translation of\nspecific terms. One popular solution is to incor-\nporate dictionaries of pre-defined terminologies as\nlexical constraints to ensure the correct translation\nof terms, which has been demonstrated to be ef-\nfective in many areas such as domain adaptation,\ninteractive translation, etc.\n\nPrevious methods on lexically constrained\ntranslation are mainly built upon Autoregressive\nTranslation (AT) models, imposing constraints at\ninference-time (Ture et al., 2012; Hokamp and Liu,\n2017; Post and Vilar, 2018) or training-time (Lu-\nong et al., 2015; Ailem et al., 2021). However, such\nmethods either are time-consuming in real-time ap-\nplications or do not ensure the appearance of con-\nstraints in the output. To develop faster MT mod-\nels for industrial applications, Non-Autoregressive\n\n1Code will be released upon publication.\n\nTranslation (NAT) has been put forth (Gu et al.,\n2018; Ghazvininejad et al., 2019; Gu et al., 2019;\nQian et al., 2021), which aims to generate tokens\nin parallel, boosting inference efficiency compared\nwith left-to-right autoregressive decoding.\n\nResearches on lexically constrained NAT are rel-\natively under-explored. Recent studies (Susanto\net al., 2020; Xu and Carpuat, 2021) impose lexical\nconstraints at inference time upon editing-based\niterative NAT models, where constraint tokens are\nset as the initial sequence for further editing. How-\never, such methods are vulnerable when encoun-\ntered with low-frequency words as constraints. As\nillustrated in Table 1, when translated with a rare\nconstraint, the model is unable to generate the cor-\nrect context of the term \u201cgeschrien\u201d as if it does\nnot understand the constraint at all. It is dangerous\nsince terms in specific domains are usually low-\nfrequency words. We argue that the main reasons\nbehind this problem are 1) the inconsistency be-\ntween training and constrained inference and 2) the\nunawareness of the source-side context of the con-\nstraints.\n\nTo solve this problem, we build our algorithm\nbased on the idea that the context of a rare con-\nstraint tends not to be rare as well, i.e., \u201ca stranger\u2019s\nneighbors are not necessarily strangers\u201d, as demon-\nstrated in Table 1. We believe that, when the con-\nstraint is aligned to the source text, the context of\nits source-side counterpart can be utilized to be\ntranslated into the context of the target-side con-\nstraint, even if the constraint itself is rare. Also,\nwhen enforced to learn to preserve designated con-\nstraints at training-time, a model should be better\nat coping with constraints during inference-time.\n\nDriven by these motivations, we propose a plug-\nin algorithm to improve constrained NAT, namely\nAligned Constrained Training (ACT). ACT ex-\ntends the family of editing-based iterative NAT (Gu\net al., 2019; Susanto et al., 2020; Xu and Carpuat,\n2021), the current paradigm of constrained NAT.\nSpecifically, ACT is composed of two major com-\nponents: Constrained Training and Alignment\nPrompting. The former extends regular training of\niterative NAT with pseudo training-time constraints\ninto the state transition of imitation learning. The\nlatter incorporates source alignment information of\nconstraints into training and inference, indicating\nthe context of the potentially rare terms.\n\nIn summary, this work makes the following con-\ntributions: 1) We identify and analyse the problems\nw.r.t. rare lexical constraints in current constrained\nNAT methods; 2) We propose a plug-in algorithm\nfor current constrained NAT models, i.e., aligned\nconstrained training, to improve the translation un-\nder rare constraints; 3) Experiments show that our\napproach improves the backbone model w.r.t. con-\nstraint preservation and translation quality, espe-\ncially for rare constraints.\n\n2018; Song et al., 2020; Chen et al., 2021). For\nthe purpose of efficiency, recent studies also fo-\ncus on non-autoregressive constrained translation.\nSusanto et al. (2020) proposes to modify the infer-\nence procedure of Levenshtein Transformer (Gu\net al., 2019) where they disallow the deletion of\nconstraint words during iterative editing. Xu and\nCarpuat (2021) further develops this idea and in-\ntroduces a reposition operation that can reorder the\nconstraint tokens. Our work absorbs the idea of\nboth lines of work. Based on NAT methods, we\nbrings alignment information by terminologies to\nhelp learn the contextual information for lexical\nconstraints, especially the rare ones.\n\nNon-Autoregressive Translation Although en-\njoy the speed advantage, NAT models suffer from\nperformance degradation due to the multi-modality\nproblem, i.e., generating text when multiple trans-\nlations are plausible. Gu et al. (2018) applies\nsequence-level knowledge distillation (KD) (Kim\nand Rush, 2016) that uses an AT\u2019s output as an\nNAT\u2019s new target, which reduces word diversity\nand reordering complexity in reference, resulting\nin fewer modes (Zhou et al., 2020; Xu et al., 2021).\nVarious algorithms have also been proposed to alle-\nviate this problem, including incorporating latent\nvariables (Kaiser et al., 2018; Shu et al., 2020),\niterative refinement (Ghazvininejad et al., 2019;\nStern et al., 2019; Gu et al., 2019; Guo et al.,\n2020), advanced training objective (Wang et al.,\n2019; Du et al., 2021) and gradually learning target-\nside word inter-dependency by curriculum learning\n(Qian et al., 2021). Our work extends the family\nof editing-based iterative NAT models for its flexi-\nbility to impose lexical constraints (Susanto et al.,\n2020; Xu and Carpuat, 2021).", "contribution": "The main contributions of this academic paper are:\n\n1. Identification and analysis of problems related to rare lexical constraints in current constrained NAT methods.\n2. Proposal of a plug-in algorithm for current constrained NAT models, i.e., aligned constrained training, to improve the translation under rare constraints.\n3. Incorporation of source alignment information of constraints into training and inference, indicating the context of the potentially rare terms.\n4. Improvement of the backbone model with respect to constraint preservation and translation quality, especially for rare constraints.", "title": "Neighbors Are Not Strangers Anonymous ACL submission Abstract References", "words": 1348, "abstract": "001 tion (NMT) draws much industrial attention for 002 its practical usage in specific domains. How- 003 ever, current autoregressive approaches suffer 004 from high latency. 005 on non-autoregressive translation (NAT) for 006 this problem for its efficiency advantage. We 007 identify that current constrained NAT models, 008 which are based on iterative editing, do not 009 handle low-frequency constraints well. To this 010 end, we propose a plug-in algorithm for this 011 line of work, i.e. , Aligned Constrained Training 012 (ACT), which alleviates this problem by famil- 013 iarizing the model with the source-side context 014 of the constraints. Experiments on the gen- 015 eral and domain datasets show that our model 016 improves over the backbone constrained NAT 017 model in constraint preservation and translation 018 quality, especially for rare constraints. 1 019 1"}
{"introduction": "state-of-the-art performance on a wide range of text generation tasks, such as machine transla- tion ( Vaswani et al. , 2017 ) and text summarization ( , 2015 ). Such models generate a token sequence in a left-to-right, token-by-token fashion. The prediction for the next token is conditioned on all previously generated tokens. This characteris- tic makes it impossible to parallelize the computa- tional overhead for token predictions in different 1 All related code, data, and models can be found in https://github.com/yxuansu/NAG-BERT. positions, which leads to a relatively high latency in inference. On the other hand, non-autoregressive generation (NAG) models ( , 2018 ) have emerged as a promising alternative due to their fast inference speed. NAG models omit the sequential dependencies within the output-side sequence and predict tokens in all positions simultaneously once the output length has been determined beforehand. While NAG models enjoy full parallelism and faster inference, the generation quality of NAG models often lags behind their autoregressive counterparts. scale pre-trained language models for improving the performance of non-autoregressive generation. Speci\ufb01cally, we utilize BERT ( Devlin et al. , 2019 ) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer ( , 2001 ; , 2019 ) for better capturing the output-side dependencies. tions that NAG models currently suffer from: (1) the in\ufb02exibility of pre\ufb01xed output length, and (2) the conditional independence of individual token predictions. Accordingly, we devise two solutions to these two problems. length to be determined before token generation, thus an extra module for output length prediction is always required. Nevertheless, the most likely length from the prediction module is not neces- sarily the best-suited one for the token generation model. To this end, previous works ( Gu et al. , 2018 ; , 2019 ) usually rely on length-parallel de- coding (LPD) ( , 2019 ) for performance enhancement; that is, generating and re-ranking the results from different output length candidates. In this work, we propose a simple and elegant decod- ing mechanism that lets the model determine the output length on-the-\ufb02y. Speci\ufb01cally, our model dynamically adjusts the output sequence length via 235 emitting an [eos] token at any output position to indicate the ending of the generated sequence. output length prediction and results re-ranking. token predictions in different positions are condi- tionally independent. As a consequence, they often tend to generate results that are ungrammatical with repetitions ( , 2019b ). To alleviate this problem, we propose a context-aware learning ob- jective which impels the model to output different tokens at adjacent positions, thereby reducing the possibility of repetitive generation. the output sequence (summary) is known to be shorter than the source sequence (article). In such cases, to further improve the model\u2019s inference ef- \ufb01ciency, we introduce a new ratio-\ufb01rst decoding strategy. Speci\ufb01cally, instead of performing infer- ence on all source-side hidden states, ratio-\ufb01rst gen- erates the result only based on a subset of source hidden states. The subset size is jointly determined by the source length T and a prede\ufb01ned ratio \u03b1 that is set based on our prior knowledge from the data statistics. In the experiments, we show that ratio-\ufb01rst can signi\ufb01cantly improve the inference speed while maintaining the generation quality. We evaluate the proposed model on three typical text generation tasks, including text summarization, sentence compression and machine translation. Ex- perimental results show that our model signi\ufb01cantly outperforms many strong non-autoregressive base- lines, and even performs competitively with several strong autoregressive models. In addition, we con- duct extensive analysis experiments to study the effect of individual proposed components. pose a novel framework that utilizes BERT for text generation under the non-autoregressive generation paradigm; (2) We propose a decoding mechanism that allows the model to dynamically determine the output length, and a new context-aware learning objective that reduces errors stemming from the output-side conditional independence assumption; (3) We introduce a ratio-\ufb01rst decoding strategy that further improve the model\u2019s inference ef\ufb01ciency. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing a novel framework that utilizes BERT for text generation under the non-autoregressive generation paradigm.\n2. Proposing a decoding mechanism that allows the model to dynamically determine the output length, and a new context-aware learning objective that reduces errors stemming from the output-side conditional independence assumption.\n3. Introducing a ratio-first decoding strategy that further improves the model's inference efficiency.", "title": "Non-Autoregressive Text Generation with Pre-trained Language Models Yixuan Su Simon Baker Abstract Acknowledgments References", "words": 933, "abstract": "Non-autoregressive generation (NAG) has re- cently attracted great attention due to its fast inference speed. However, the generation qual- ity of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mech- anisms to alleviate the two common prob- lems of vanilla NAG models: the in\ufb02exibil- ity of pre\ufb01xed output length and the condi- tional independence of individual token predic- tions. Lastly, to further increase the speed ad- vantage of the proposed model, we propose a new decoding strategy, ratio-\ufb01rst , for applica- tions where the output lengths can be approx- imately estimated beforehand. For a compre- hensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and ma- chine translation. Experimental results show that our model signi\ufb01cantly outperforms exist- ing non-autoregressive baselines and achieves competitive performance with many strong au- toregressive models. In addition, we also con- duct extensive analysis experiments to reveal the effect of each proposed component. 1 1"}
{"introduction": "\nTransformer has been the most widely used ar-\nchitecture for machine translation (Vaswani et al.,\n2017). Despite its strong performance, the decod-\ning of Transformer is inef\ufb01cient as it adopts the\nsequential auto-regressive factorization for its prob-\nability model (Figure 1a). Recent work such as\nthe non-autoregressive transformer (NAT), aims to\ndecode target tokens in parallel to speed up the gen-\neration (Gu et al., 2018). However, the vanilla NAT\nstill lags behind the Transformer in translation qual-\nity \u2013 with a gap of about 7.0 BLEU points. NAT\nassumes the conditional independence of the target\ntokens given the source sentence. We suspect that\nNAT\u2019s conditional independence assumption pre-\nvents learning word interdependency in the target\n\n\u2217The work was done when the \ufb01rst author was an intern at\n\nBytedance.\n\n(a) Sequential LM\n\n(b) Cond. Independent LM\n\n(c) Masked LM (MLM)\n\n(d) Glancing LM (GLM)\n\nFigure 1: Probabilistic models for machine translation\n(b) Vanilla NAT uses conditional indepe-\nmethods.\ndent LM. (c) Mask-Predict NAT uses MLM and re-\nquires multiple passes of decoding. (d) Our proposed\nGLM leverages the decoder prediction to decide glanc-\ning sampling policy during training and only requires\none pass of decoding during inference.\n\nsentence. Notice that such word interdependency\nis crucial, as the Transformer explicitly captures\nthat via decoding from left to right (Figure 1a).\n\nSeveral remedies are proposed (Ghazvininejad\net al., 2019; Gu et al., 2019) to capture word inter-\ndependency while keeping parallel decoding. Their\ncommon idea is to decode the target tokens itera-\ntively while each pass of decoding is trained using\nthe masked language model (Figure 1c). Since\nthese methods require multiple passes of decod-\ning, its generation speed is measurably slower than\nthe vanilla NAT. With single-pass generation only,\nthese methods still largely lag behind the autore-\ngressive Transformer.\n\n\nTo address the quest, we propose glancing lan-\nguage model (GLM), a new method to train a prob-\nabilistic sequence model. Based on GLM, we de-\nvelop the glancing Transformer (GLAT) for neural\nmachine translation. It achieves parallel text gener-\nation with only single decoding. Yet, it outperforms\nprevious NAT methods and achieves comparable\nperformance as the strong Transformer baseline in\nmultiple cases. Intuitively, GLM adopts a adaptive\nglancing sampling strategy, which glances at some\nfragments of the reference if the reference is too\ndif\ufb01cult to \ufb01t in the training of GLAT. Correspond-\ningly, when the model is well tuned, it will adap-\ntively reduce the percentage of glancing sampling,\nmaking sure that the resulting model could learn\nto generate the whole sentence in the single-pass\nfashion. The gradual learning process smooths the\nlearning curve of single-pass parallel generation.\n\nSpeci\ufb01cally, our proposed GLM differs from\nMLM in two aspects. Firstly, GLM proposes an\nadaptive glancing sampling strategy, which enables\nGLAT to generate sentences in a one-iteration way,\nworking by gradual training instead of iterative in-\nference (see Figure 1d). Generally, GLM is quite\nsimilar to curriculum learning (Bengio et al., 2009)\nin spirit, namely \ufb01rst learning to generate some\nfragments and gradually moving to learn the whole\nsentences (from easy to hard). To achieve the adap-\ntive glancing sampling, GLM performs decoding\ntwice in training. The \ufb01rst decoding is the same as\nthe vanilla NAT, and the prediction accuracy indi-\ncates whether the current reference is \u201cdif\ufb01cult\u201d for\n\ufb01tting. In the second decoding, GLM gets words\nof the reference via glancing sampling according\nto the \ufb01rst decoding, and learn to predict the re-\nmaining words that are not sampled. Note that\nonly the second decoding will update the model pa-\nrameters. Secondly, instead of using the [MASK]\ntoken, GLM directly uses representations from the\nencoder at corresponding positions, which is more\nnatural and could enhance the interactions between\nsampled words and signals from the encoder.\n\nNote that GLAT does not modify the network ar-\nchitecture, which is a training method to explicityly\nlearn word interdependency. Experimental results\nshow that GLAT obtains signi\ufb01cant improvements\n\n(about 5 BLEU) on standard benchmarks compared\nto the vanilla NAT, without losing inference speed-\nup. GLAT achieves competitive results against iter-\native approaches like Mask-Predict (Ghazvininejad\net al., 2019), even outperforming the Mask-Predict\nmodel on WMT14 DE-EN and WMT16 RO-EN.\nCompared to the strong AT baseline, GLAT can\nstill close the performance gap within 0.9 BLEU\npoint while keeping 7.9\u00d7 speed-up. Empirically,\nwe even \ufb01nd that GLAT outperforms AT when the\nlength of the reference is less than 20 on WMT14\nDE-EN. We speculate this is because GLM could\ncapture bidirectional context for generation while\nits left-to-right counterpart is only unidirectional,\nwhich indicates the potential of parallel generation\napproaches like GLAT.", "contribution": "The main contributions of this paper are:\n\n1. Proposing the glancing language model (GLM) which adopts an adaptive glancing sampling strategy to train a probabilistic sequence model and enables the glancing Transformer (GLAT) to generate sentences in a one-iteration way, working by gradual training instead of iterative inference.\n\n2. Developing GLAT for neural machine translation which achieves parallel text generation with only single decoding and outperforms previous non-autoregressive transformer (NAT) methods and achieves comparable performance as the strong autoregressive transformer (AT) baseline in multiple cases.\n\n3. Showing that GLAT obtains significant improvements (about 5 BLEU) on standard benchmarks compared to the vanilla NAT, without losing inference speed-up, and achieves competitive results against iterative approaches like Mask-Predict.\n\n4. Demonstrating that GLAT can still close the performance gap within 0.9 BLEU point while keeping 7.9\u00d7 speed-up compared to the strong AT baseline, and even outperforms AT when the length of the reference is less than 20 on WMT14 DE-EN.", "title": "Glancing Transformer for Non-Autoregressive Lihua Qian Lin Qiu Abstract Acknowledgments References", "words": 1172, "abstract": "Recent work on non-autoregressive neural ma- chine translation (NAT) aims at improving the ef\ufb01ciency by parallel decoding without sac- ri\ufb01cing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass par- allel generation models. With GLM, we de- velop Glancing Transformer (GLAT) for ma- chine translation. With only single-pass par- allel decoding, GLAT is able to generate high-quality translation with 8 \u00d7 -15 \u00d7 speedup. Note that GLAT does not modify the net- work architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points. 1"}
{"introduction": "\nNon-autoregressive translation (NAT, Gu et al.,\n2018) has been proposed to improve the decoding\nef\ufb01ciency by predicting all tokens independently\nand simultaneously. Different from autoregressive\ntranslation (AT, Vaswani et al., 2017) models that\ngenerate each target word conditioned on previ-\nously generated ones, NAT models suffer from\nthe multimodality problem (i.e. multiple transla-\ntions for a single input), in which the conditional\n\n\u2217 Liang Ding and Longyue Wang contributed equally to\nthis work. Work was done when Liang Ding and Xuebo Liu\nwere interning at Tencent AI Lab.\n\nindependence assumption prevents a model from\nproperly capturing the highly multimodal distribu-\ntion of target translations. To reduce the modes of\ntraining data, sequence-level knowledge distillation\n(KD) (Kim and Rush, 2016) is widely employed\nvia replacing their original target samples with sen-\ntences generated from an AT teacher (Gu et al.,\n2018; Zhou et al., 2020; Ren et al., 2020).\n\nAlthough KD reduces the learning dif\ufb01culty for\nNAT, there are still complicated word orders and\nstructures (Gell-Mann and Ruhlen, 2011) in the\nsynthetic sentences, making the NAT performance\nsub-optimal. To answer this challenge, Saharia\net al. (2020); Ran et al. (2021) propose to lowers\nthe bilingual modeling dif\ufb01culties under the mono-\ntonicity assumption, where bilingual sentences are\nin the same word order. However, they make ex-\ntensive modi\ufb01cations to model structures or objec-\ntives, limiting the applicability of their methods to\na boarder range of tasks and languages.\n\nAccordingly, we turn to break down the sentence-\nlevel high modes into \ufb01ner granularities, i.e. bilin-\ngual words and phrases, where we assume that\n\ufb01ner granularities are easy to be learned by NAT.\nAs shown in Table 1, we analyzed the transla-\ntion accuracy at three linguistic levels (i.e. word,\nphrase and sentence) and found that although KD\nbrings promising improvements at three granular-\nities, there are still some gaps with AT teacher.\nAlso, we showed that \ufb01ner granularities are eas-\nier to be learned, that is, accuracy gap \u201c\u2206\u201d of\nWORD is small than that of PHRASE, and SEN-\nTENCE (0.8<1.8<2.2). Thus, we propose a sim-\nple and effective training strategy to enhance the\nability to handle the sentence-level high modes.\nMore speci\ufb01cally, we generate bilingual lexicons\nfrom parallel data by leveraging word alignment\nand phrase extraction in statistical machine trans-\nlation (SMT, Zens et al., 2002). Then we guide\nthe NAT model to progressively learn the bilingual\nknowledge from low to high granularity. Experi-\nmental results on four commonly-cited translation\nbenchmarks show that our proposed PROGRESSIVE\nMULTI-GRANULARITY (PMG) training strategy\nconsistently improves the translation performance.\nThe main contributions are:\n\n\u2022 Our study reveals that NAT is better at learn-\ning \ufb01ne-grained knowledge. Training with\nsentences merely may be sub-optimal.\n\n\u2022 We propose PMG training to encourage NAT\nmodels to learn from easy to hard. The \ufb01ne-\ngrained knowledge distilled by SMT will be\ndynamically transferred during training.\n\n\u2022 Experiments across language pairs and model\nstructures show the effectiveness and univer-\nsality of PMG training.", "contribution": "- The study reveals that non-autoregressive translation (NAT) is better at learning fine-grained knowledge and training with sentences alone may be sub-optimal.\n- The paper proposes a Progressive Multi-Granularity (PMG) training strategy to encourage NAT models to learn from easy to hard, with fine-grained knowledge distilled by statistical machine translation (SMT) dynamically transferred during training.\n- Experimental results across language pairs and model structures show the effectiveness and universality of PMG training.", "title": "Progressive Multi-Granularity Training for Liang Ding The University of Sydney Longyue Wang Tencent AI Lab Xuebo Liu Derek F. Wong Dacheng Tao Zhaopeng Tu Abstract Acknowledgments References", "words": 786, "abstract": "Non-autoregressive translation (NAT) signif- icantly accelerates the inference process via predicting the entire target sequence. How- ever, recent studies show that NAT is weak at learning high-mode of knowledge such as one-to-many translations. We argue that modes can be divided into various granu- larities which can be learned from easy to hard. In this study, we empirically show that NAT models are prone to learn \ufb01ne-grained lower-mode knowledge, such as words and phrases, compared with sentences. Based on this observation, we propose progressive multi- granularity training for NAT. More speci\ufb01- cally, to make the most of the training data, we break down the sentence-level examples into three types, i.e. words, phrases, sen- tences, and with the training goes, we pro- gressively increase the granularities. Experi- ments on Romanian-English, English-German, Chinese-English and Japanese-English demon- strate that our approach improves the phrase translation accuracy and model reordering abil- ity, therefore resulting in better translation quality against strong NAT baselines. Also, we show that more deterministic \ufb01ne-grained knowledge can further enhance performance. 1"}
{"introduction": "\nTransformer (Vaswani et al., 2017) has recently become the most prevailing neural architecture for\nsequence-to-sequence learning (Bahdanau et al., 2015). Transformer is originally an autoregressive\n(AR) sequence generative models, which adopts a sequential factorization to estimate the conditional\n, y[N ]\nx) =\nprobability of a target sequence y =\n1], x). Albeit simple and effective, such a fixed left-to-right restriction is not\nnecessarily the unique and the best formulation for sequence modeling, limiting the design space\n(cid:81)\nof neural networks and applicable tasks for AR models. Hence researchers are motivated to study\nnon-autoregressive (NAR) sequence generative models (Gu et al., 2018) as an alternative to AR\nmodels, which instead use a per-token factorization p(y\nx). Despite their favorable\n|\ndecoding speed and flexible formulation to introduce constraints, NAR models still lag behind their\nAR counterparts and require data distillation.\n\nconditioned on a source sequence x: p(y\n\n\nNAR models can be viewed as generating sequences by iteratively denoising from an initial guess\n(Figure 1(a)). Several studies based on this idea of iterative refinement show promising and competi-\ntive results compared AR models. For instance, Lee et al. (2018) and Savinov et al. (2021) propose to\nregard NAR models as denoising autoencoders, while Ghazvininejad et al. (2019) task NAR models\nwith conditional masked language modeling. More recently, discrete denoising diffusion models\nhave started to attract the community\u2019s attention. Besides iteratively manipulating sequences of\ndiscrete tokens, research also finds that for fully NAR models Gu et al. (2018), layer recurrence also\ncalibrates intermediate continuous representations towards the target discrete sequence (Huang et al.,\n2021; Elbayad et al., 2020; Li et al., 2022). In other words, fully NAR and iterative-based NAR\nmodels are tasked with approaching their equilibrium states, in terms of either discrete or continuous\nrepresentation, which is also found in our empirical observation in Figure 1(c).\n\nIn this paper, we argue that NAR models, including fully NAR and iterative-based NAR models,\ncan be regarded as a dynamical system in the form of zt+1 = f\u03b8(zt, x), implying a dynamics of\nparallel denoising or iterative refinement process over the whole sequence (Figure 1(b)). More\nconcretely, NAR models apply a Markov chain factorization to a series of intermediate predictions\nfrom the bottom up, where a neural parametric transition kernel f\u03b8 learns denoising sequences in a\ncoarse-to-fine manner, while zt is the t-th running discrete or continuous state.\n\n\nFrom such a unified dynamical system perspective, intuitively, the state of an NAR system is supposed\nto evolve towards the target sequence limt\ny, where we may obtain a solution\nz\u22c6 of this system that can best estimate the target y while no further improvement could be made.\nHowever, the current NAR systems, which naively evaluate the transition function\nup to a manually-\ndefined maximum iteration N , cannot guarantee to reach such a stationary equilibrium state, making\nthe final output zN a sub-optimal representation with regard to the target sequence. This motivates us\nto solve for such an equilibrium state of the NAR dynamical system for better understanding and\nmodeling.\n\nTo this end, in this paper, we reformulate the sequence generation problem as solving the equilibrium\nstate of NAR models. We propose our framework, the DEQNAR, and apply it to the cases where\nthe iterative refinement can be conducted either in continuous feature state space, discrete data state\nspace, or a combination of both. This enables multiple preferable properties for our model over\nprevious studies. (1) Instead of naive iterative layer stacking, DEQNAR models define the output\nF\u03b8 given the input x, i.e., z\u22c6 = f (z\u22c6, x), modeling an equilibrium representation.\nas fixed point of\n(2) Compared with typical NAR systems, the proposed DEQNAR permits better convergence to the\nequilibrium point. We can leverage any advanced black-box solvers, e.g., quasi-Newton methods,\nto directly solve for the equilibrium solution, leading to better results. (3) The DEQNAR is also\northogonal to existing advanced techniques for NAR models, for which we studied its effectiveness\nwhen combined with the current best practices, including better modeling approach (VAE, Gu &\nKong, 2021), training objective (CTC, Graves et al., 2006) and training strategy (GLAT, Qian et al.,\n2021).\n\nWe conduct extensive experiments on WMT14 English-German and WMT16 English-Romanian\nmachine translation benchmarks. Based on the empirical results, our main findings are as follows:\n(1) DEQNAR is a general-purpose framework that can supplement several existing NAR techniques,\nincluding vanilla NAR, VAE, CTC loss, and GLAT training, giving rise to considerable performance\ngains. (2) We verify that convergence to an equilibrium state in DEQNAR is almost indeed via\nquantitative and qualitative evaluation. The closer to the equilibrium state, the more likely DEQNAR\nachieves more accurate performance.\n", "contribution": "The main contributions of this paper are:\n\n1. The proposal of a framework called DEQNAR that reformulates the sequence generation problem as solving the equilibrium state of non-autoregressive (NAR) models.\n2. The application of DEQNAR to cases where iterative refinement can be conducted either in continuous feature state space, discrete data state space, or a combination of both.\n3. The demonstration that DEQNAR is a general-purpose framework that can supplement several existing NAR techniques, including vanilla NAR, VAE, CTC loss, and GLAT training, giving rise to considerable performance gains.\n4. The verification that convergence to an equilibrium state in DEQNAR is almost indeed via quantitative and qualitative evaluation. The closer to the equilibrium state, the more likely DEQNAR achieves more accurate performance.", "title": "DEEP EQUILIBRIUM NON-AUTOREGRESSIVE SEQUENCE LEARNING", "words": 1147, "abstract": "\nIn this work, we argue that non-autoregressive (NAR) sequence generative\nmodels can equivalently be regarded as iterative refinement process towards\nthe target sequence, implying an underlying dynamical system of NAR model:\nz =\ny. In such a way, the optimal prediction of a NAR model should be\nthe equilibrium state of its dynamics if given infinitely many iterations. However,\nthis is infeasible in practice due to limited computational and memory budgets.\nTo this end, we propose DEQNAR to directly solve for the equilibrium state of\nNAR models based on deep equilibrium networks (Bai et al., 2019) with black-box\nroot-finding solvers and back-propagate through the equilibrium point via implicit\ndifferentiation with constant memory. We conduct extensive experiments on four\nWMT machine translation benchmarks. Our main findings show that DEQNAR can\nindeed converge to a more accurate prediction and is a general-purpose framework\nthat consistently yields substantial improvement for several strong NAR backbones.\n\n"}
{"introduction": "Neural Machine Translation (NMT) based on deep neural networks has gained rapid progress over recent years (Cho et al. 2014; Bahdanau, Cho, and Bengio 2014; Wu et al. 2016; Vaswani et al. 2017; Hassan et al. 2018). NMT systems are typically implemented in an encoder-decoder framework, in which the encoder network feeds the representations of source side sentence x into the decoder network to gener- ate the tokens in target sentence y . The decoder typically works in an auto-regressive manner: the generation of the t - th token y t follows a conditional distribution P ( y t | x, y <t ) , where y <t represents all the generated tokens before y t . \u2217 The work was done when the author was an intern at Microsoft Research Asia. Copyright c \u20dd 2019, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. Therefore, during the inference process, we have to sequen- tially translate each target-side word one by one, which sub- stantially hampers the inference ef\ufb01ciency of NMT systems. To alleviate the latency of inference brought by auto- regressive decoding, recently the community has turned to Non-Autoregressive Translation (NAT) systems (Gu et al. 2018; Lee, Mansimov, and Cho 2018; Kaiser et al. 2018). A basic NAT model has the same encoder-decoder archi- tecture as the autoregressive translation (AT) model, except that the sequential dependency within the target side sen- tence is omitted. In this way, all the tokens can be gen- erated in parallel, and the inference speed is thus signi\ufb01- cantly boosted. However, it comes at the cost that the trans- lation quality is largely sacri\ufb01ced since the intrinsic depen- dency within the natural language sentence is abandoned. To mitigate such performance degradation, previous work has tried different ways to insert intermediate discrete variables to the basic NAT model, so as to incorporate some light- weighted sequential information into the non-autoregressive decoder. The discrete variables include the autoregressively generated latent variables (Kaiser et al. 2018), and the fer- tility information brought by a third-party word alignment model (Gu et al. 2018). However, leveraging such discrete variables not only brings additional dif\ufb01culty for optimiza- tion, but also slows down the translation by introducing extra computational cost for producing such discrete variables. In this paper, we propose a new solution to the problem that does not rely on any discrete variables and makes lit- tle revision to the basic NAT model, thus retaining most of the bene\ufb01t of an NAT model. Our approach was motivated by the following result we obtained from carefully analyz- ing the key issues existed in the basic NAT model. We em- pirically observed that the two types of translation errors frequently made by the basic NAT model are: 1) repeated translation , where the same token is generated repeatedly at consecutive time steps; 2) incomplete translation , where the semantics of several tokens/sentence pieces from the source sentence are not adequately translated. Both issues suggest that the decoder hidden states in NAT model, i.e., the hid- den states output in the topmost layer of the decoder, are of low quality: the repeated translation shows that two adjacent hidden states are indistinguishable , leading to the same to- kens decoded out, while the incomplete translation re\ufb02ects that the hidden states in the decoder are incomplete in repre- arXiv:1902.10245v1  [cs.CL]  22 Feb 2019 senting source side information. Such a poor quality of decoder hidden states is in fact a direct consequence of the non-autoregressive nature of the model: at each time step, the hidden states have no access to their prior decoded states, making them in a \u2018chaotic\u2019 state being unaware of what has and has not been translated. Therefore, it is dif\ufb01cult for the neural network to learn good hidden representations all by itself. Thus, in order to im- prove the quality of the decoder representations, we must go beyond the pure non-autoregressive models. The challenge, though, is how to improve the quality of decoder representa- tions to address the two problems identi\ufb01ed above while still keeping the bene\ufb01t of ef\ufb01cient inference of the NAT models. We propose to address this challenge by directly regular- izing the learning of the decoder representations using two auxiliary regularization terms for model training. First, to overcome the problem of repeated translation, we propose to force the similarity of two neighboring hidden state vectors to be well aligned with the similarity of the embedding vec- tors representing the two corresponding target tokens they aim to predict. We call this regularization strategy similar- ity regularization . Second, to overcome the problem of in- complete translation, inspired by the dual nature of machine translation task (He et al. 2016), we propose to impose that the hidden representations should be able to reconstruct the source side sentence, which can be achieved by putting an auto-regressive backward translation model on top of the de- coder; the backward translation model would \u201cdemand\u201d the decoder hidden representations of NAT to contain enough information about the source. We call this regularization strategy reconstruction regularization . Both regularization terms only appear in the training process and have no ef- fect during inference, thus bringing no additional computa- tional/time cost to inference and allowing us to retain the major bene\ufb01t of an NAT model. Meanwhile, the direct reg- ularization on the hidden states effectively improves their representation. In contrast, the existing approaches would need either a third-party word alignment module (Gu et al. 2018) (hindering end-to-end learning), or a special compo- nent handling the discrete variables (e.g., several softmax operators in (Lee, Mansimov, and Cho 2018)) which brings additional latency for decoding. We evaluate the proposed regularization strategies by con- ducting extensive experiments on several benchmark ma- chine translation datasets. The experiment results show that both regularization strategies are effective and they can al- leviate the issues of repeated translations and incomplete translations in NAT models, leading to improved NAT mod- els that can improve accuracy substantially over the state- of-the-art NAT models without sacri\ufb01cing ef\ufb01ciency. We set a new state-of-the-art performance of non-autoregressive translation models on the WMT14 datasets, with 24 . 61 BLEU in En-De and 28 . 90 in De-En.", "contribution": "The paper proposes a new solution to improve the quality of decoder representations in Non-Autoregressive Translation (NAT) models without relying on any discrete variables. The proposed solution includes two auxiliary regularization terms for model training: similarity regularization and reconstruction regularization. The regularization terms effectively improve the representation of decoder hidden states, addressing the issues of repeated translations and incomplete translations in NAT models. The proposed approach sets a new state-of-the-art performance of non-autoregressive translation models on the WMT14 datasets, with 24.61 BLEU in En-De and 28.90 in De-En.", "title": "Non-Autoregressive Machine Translation with Auxiliary Regularization", "words": 1405, "abstract": "Abstract As a new neural machine translation approach, Non- Autoregressive machine Translation (NAT) has attracted at- tention recently due to its high ef\ufb01ciency in inference. How- ever, the high ef\ufb01ciency has come at the cost of not capturing the sequential dependency on the target side of translation, which causes NAT to suffer from two kinds of translation er- rors: 1) repeated translations (due to indistinguishable adja- cent decoder hidden states), and 2) incomplete translations (due to incomplete transfer of source side information via the decoder hidden states). In this paper, we propose to address these two problems by improving the quality of decoder hid- den representations via two auxiliary regularization terms in the training process of an NAT model. First, to make the hid- den states more distinguishable, we regularize the similarity between consecutive hidden states based on the correspond- ing target tokens. Second, to force the hidden states to contain all the information in the source sentence, we leverage the dual nature of translation tasks (e.g., English to German and German to English) and minimize a backward reconstruction error to ensure that the hidden states of the NAT decoder are able to recover the source side sentence. Extensive experi- ments conducted on several benchmark datasets show that both regularization strategies are effective and can alleviate the issues of repeated translations and incomplete translations in NAT models. The accuracy of NAT models is therefore improved signi\ufb01cantly over the state-of-the-art NAT models with even better ef\ufb01ciency for inference."}
{"introduction": "et al. , 2020 ), BART ( , 2020a ), and MASS ( , 2019 ) have established strong baselines for the majority of text-to-text trans- duction tasks. A recent trend to massively scale up model sizes, e.g., all the way up to 540B params ( , 2022 ), as well as the sizes of pretraining corpora, has further pushed the 1 Code and pre-trained models https://edit5.page. link/code A  long  user  query D user K K Encoder query long K D Pointer <s>  pos0 The  pos2  is  very pos0  The pos2  is  very  </s> Output: The user query is very long Decoder Figure 1: EdiT5 transforms the input text A long user query into the output The user query is very long by \ufb01rst generating a sequence of edit tags D K K K (where K stands for keeping and D for deleting the input to- ken), re-ordering the input tokens with the pointer net- work, and in\ufb01lling missing tokens into the source se- quence with an autoregressive decoder which jointly predicts the text spans ( The and is very ) and the position where to insert them ( pos0 and pos2 ). The blue arrow shows how the token pos2 is predicted con- ditioned on the pre\ufb01x <s> pos0 The generated thus far. The dotted arrow lines depict the encoder-decoder cross attention over the re-ordered input tokens and edit tags. state-of-the-art without signs of reaching a plateau. with such models is prohibitively expensive for most applications, which motivates the work on \ufb01nding ef\ufb01cient recipes for model distillation, e.g., ( , 2016 ) and choosing a model ar- chitecture that can provide a better trade-off be- tween performance on a given task and inference speed. A typical choice is to distill a large language model into a smaller seq2seq model, e.g., Trans- former ( , 2017 ). In this paper we propose a novel model architecture E DI T5which blends ideas from a seq2seq T5 ( , 2020 ) and text-editing to provide faster inference without sacri\ufb01cing on task performance. arXiv:2205.12209v2  [cs.CL]  26 Oct 2022 token from scratch, allowing them to model any kind of input-output relationship. However, for many real-world tasks this degree of generality is unnecessary, especially for monolingual tasks where the input and output texts have relatively high degrees of overlap. In such cases a natural approach is to cast conditional text generation as a text-editing task, where the model learns to con- struct target texts by applying a set of edit oper- ations to the inputs ( , 2022 ). Typi- cally the set of edit operations is de\ufb01ned ahead of time ( , 2020 ; , 2019 ; , 2019 ), which on the one hand limits the \ufb02exibility of the model to reconstruct arbitrary output texts from the inputs, but on the other, leads to latency improvements as the limited set of al- lowed operations signi\ufb01cantly reduces the output vocabulary of the decoder. In this paper, we pro- pose an approach which is both fast at inference time and \ufb02exible, able to model arbitrary rewrites. A common method for achiev- ing low latency in serving models is to reduce their size, thus reducing their computational cost. Doing so naively, however, often leads to inferior model quality, and much work has gone into \ufb01nding better methods for model size reduction, such as distilla- tion ( , 2016 ). tributors to the total inference time for seq2seq models is the decoder, which generates the output sequence step-by-step. E DI T5 also relies on an au- toregressive decoder, but generates the majority of the output sequence with its tagging and pointing networks, and as such the decoder makes far fewer steps. proaches, e.g., ( , 2019 ; , 2019 ), are not as powerful as general purpose seq2seq approaches when it comes to modeling arbitrary input-output text transductions. E DI T5 supports open-vocabulary generation by relying on an autoregressive decoder. In the extreme case, where there is no overlap between the source and the target texts, it reduces to a vanilla seq2seq model generating the entire output from scratch. bene\ufb01t from the tagging and pointer networks to reconstruct the bulk of the output text that is further in\ufb01lled (re\ufb01ned) by the autoregressive decoder. generation model typically requires large amounts of high-quality supervised data. Self-supervised techniques based on text in-\ufb01lling ( , 2020a ; , 2020b ; , 2020 ) have been shown to provide a crucial advantage over non-pre-trained models especially in low-resource settings. Hence, we design E DI T5 to be able to bene\ufb01t from already existing pre-trained language models (speci\ufb01cally T5), where the \ufb01nal model is directly \ufb01ne-tuned on the downstream task. E DI T5 decomposes the generation task into three steps: tagging , pointing and insertion (see 1 ). The tagger and pointer networks decide which source tokens to preserve and in which or- der they should appear in the output, thus allowing for arbitrary word dropping and reordering. The tagger is implemented using a non-autoregressive feedforward network, and pointing is implemented using a novel non-autoregressive pointing mecha- nism ( , 2015 ) combined with sinkhorn layers ( , 2018 ). The insertion network inserts/in\ufb01lls words which are present in the tar- get sequence but do not appear in the source se- quence. The network is implemented using an au- toregressive transformer decoder, which attends to the tagged, reordered source sequence. The de- coder predicts both the locations of where the token spans should be in\ufb01lled, as well as the spans them- selves. DI T5 on three distinct text gener- ation tasks: Sentence Fusion, Grammatical Error paring to recent text-editing approaches and T5. quired and the amount of training data available, which helps to better quantify the value of model- ing decisions we have integrated into E DI T5. data size and model size on E DI T5. Finally we quantify the latency of E DI T5, providing a detailed analysis and comparison to T5. 2", "contribution": "The main contributions of this academic paper are:\n\n1. Proposing a novel model architecture called E DI T5 that combines ideas from a seq2seq T5 and text-editing to provide faster inference without sacrificing task performance.\n2. Introducing an approach that is both fast at inference time and flexible, able to model arbitrary rewrites.\n3. Supporting open-vocabulary generation by relying on an autoregressive decoder.\n4. Designing E DI T5 to be able to benefit from already existing pre-trained language models, specifically T5, where the final model is directly fine-tuned on the downstream task.\n5. Decomposing the generation task into three steps: tagging, pointing, and insertion, and implementing them using non-autoregressive feedforward and pointing networks, as well as an autoregressive transformer decoder.\n6. Evaluating E DI T5 on three distinct text generation tasks: Sentence Fusion, Grammatical Error Correction, and Text Summarization, and comparing it to recent text-editing approaches and T5.\n7. Conducting experiments to analyze the impact of data size and model size on E DI T5, as well as its latency compared to T5.", "title": "EdiT5: Semi-Autoregressive Text Editing with T5 Warm-Start", "words": 1388, "abstract": "We present E DI T5 1 \u2013 a novel semi- autoregressive text-editing model designed to combine the strengths of non-autoregressive text-editing and autoregressive decoding. E DI T5 is faster during inference than con- ventional sequence-to-sequence (seq2seq) models, while being capable of modeling \ufb02exible input-output transformations. This is achieved by decomposing the genera- tion process into three sub-tasks: (1) tagging to decide on the subset of input tokens to be preserved in the output, (2) re-ordering to de- \ufb01ne their order in the output text, and (3) in- sertion to in\ufb01ll the missing tokens that are not present in the input. The tagging and re- ordering steps, which are responsible for gen- erating the largest portion of the output, are non-autoregressive, while the insertion step uses an autoregressive decoder. Depending on the task, E DI T5 on average re- quires signi\ufb01cantly fewer autoregressive steps, demonstrating speedups of up to 25x when compared to seq2seq models. Quality-wise, E DI T5 is initialized with a pre-trained T5 checkpoint yielding comparable performance to T5 in high-resource settings when evaluated on three NLG tasks: Sentence Fusion, Gram- matical Error Correction, and Decontextualiza- tion while clearly outperforming T5 in low- resource settings. 1"}
{"introduction": "achieved state-of-the-art performance in recent years ( , 2014 ; , 2015 ; , 2017 ), most NMT models still suf- fer from the slow decoding speed problem due to their autoregressive property: the generation of a target token depends on all the previously gener- ated target tokens, making the decoding process intrinsically nonparallelizable. translation (NAT) models ( Gu et al. , 2018 ; Li et al. , 2019 ; , 2019 ; , 2019a ; et al. , 2019 ) have been investigated to mitigate the \u2217 indicates equal contribution \u2020 indicates corresponding author Src. es gibt heute viele Farmer mit diesem Ansatz Feasible there are lots of farmers doing this today Trans. there are a lot of farmers doing this today Trans. 1 there are lots of of farmers doing this today Trans. 2 there are a lot farmers doing this today Table 1: A multi-modality problem example: NAT models generate each target token independently such that they may correspond to different feasible transla- tions, which usually manifests as repetitive (Trans. 1) or missing (Trans. 2) tokens. slow decoding speed problem by generating all tar- get tokens independently in parallel, speeding up the decoding process signi\ufb01cantly. Unfortunately, these models suffer from the multi-modality prob- lem ( , 2018 ), resulting in inferior transla- tion quality compared with autoregressive NMT. tiple feasible translations, and each target token may be generated with respect to different fea- sible translations since NAT models discard the dependency among target tokens. This generally manifests as repetitive or missing tokens in the translations. Table 1 shows an example. The Ger- man phrase \u201c viele Farmer \u201d can be translated as either \u201c lots of farmers \u201d or \u201c a lot of farmers \u201d. In the \ufb01rst translation (Trans. 1), \u201c lots of \u201d are trans- lated w.r.t. \u201c lots of farmers \u201d while \u201c of farmers \u201d are translated w.r.t. \u201c a lot of farmers \u201d such that two \u201c of \u201d are generated. Similarly, \u201c of \u201d is missing in the second translation (Trans. 2). Intuitively, the multi-modality problem has a signi\ufb01cant negative effect on the translation quality of NAT. the above problem, which can be roughly divided into two lines. The \ufb01rst line of work leverages the iterative decoding framework to break the in- dependence assumption, which \ufb01rst generates an initial translation and then re\ufb01nes the translation 3060 BOS there there are are EOS BOS lots lots of of farmers BOS lots lots of BOS doing doing this this today Decoder of DEL farmers EOS Encoder es gibt Ansatz today EOS \u2026 t =1 t =1 t =1 t =1 t =2 t =2 t =2 t =2 t =3 t =3 t =3 t =3 t =4 t =4 Segment 1 Segment 2 Segment 3 Segment 4 Final translation: there are lots of farmers doing this today Post-process Figure 1: An overview of our RecoverSAT model. RecoverSAT generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is generated token-by-token conditioned on both the source tokens and the translation history of all segments (e.g., the token \u201c are \u201d in the \ufb01rst segment is predicted based on all the tokens colored green). Repetitive segments (e.g., the third segment \u201c lots of \u201d) are detected and deleted automatically. iteratively by taking both the source sentence and the translation of last iteration as input ( , 2018 ; Ghazvininejad et al. , 2019 ). Nevertheless, it requires to re\ufb01ne the translations for multiple times in order to achieve better translation quality, which hurts decoding speed signi\ufb01cantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leverag- ing extra autoregressive layers in the decoder ( Shao et al. , 2019a ; Wang et al. , 2018 ), introducing latent variables and/or more powerful probabilistic frame- works to model more complex distributions ( Kaiser et al. , 2018 ; , 2019 ; , 2019 ; Ma et al. , 2019 ), guiding the training process with an autoregressive model ( Li et al. , 2019 ; Wei et al. , 2019 ), etc. However, these models cannot alter a target token once it has been generated, which means these models are not able to recover from an error caused by the multi-modality problem. maintaining a reasonable decoding speedup, we propose a novel semi-autoregressive model named RecoverSAT in this work. RecoverSAT features in three aspects: (1) To improve decoding speed, we assume that a translation can be divided into several segments which can be generated simultaneously. (2) To better capture target-side dependency, the to- kens inside a segment is autoregressively generated conditioned not only on the previously generated tokens in this segment but also on those in other segments. On one hand, we observe that repeti- tive tokens are more likely to occur within a short context. Therefore, autoregressively generating a segment is bene\ufb01cial for reducing repetitive tokens. generated tokens in other segments, the model is capable of guessing what feasible translation candi- dates have been chosen by each segment and adapts accordingly, e.g., recovering from missing token errors. As a result, our model captures more target- side dependency such that the multi-modality prob- lem can be alleviated naturally. (3) To make the model capable of recovering from repetitive token errors, we introduce a segment deletion mechanism into our model. Informally speaking, our model will mark a segment to be deleted once it \ufb01nds the content has been translated in other segments. datasets for machine translation to evaluate the proposed method. The experimental results show that RecoverSAT is able to decode over 4 \u00d7 faster than the autoregressive counterpart while maintain- ing comparable performance. The source code of this work is released on https://github.com/ ranqiu92/RecoverSAT . 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing a novel semi-autoregressive model named RecoverSAT that divides a translation into several segments which can be generated simultaneously to improve decoding speed.\n2. Autoregressively generating tokens inside a segment conditioned not only on the previously generated tokens in this segment but also on those in other segments to better capture target-side dependency and reduce repetitive tokens.\n3. Introducing a segment deletion mechanism to recover from repetitive token errors.\n4. Evaluating the proposed method on machine translation datasets and showing that RecoverSAT is able to decode over 4x faster than the autoregressive counterpart while maintaining comparable performance.", "title": "Learning to Recover from Multi-Modality Errors for Non-Autoregressive Qiu Ran Abstract Acknowledgments References", "words": 1295, "abstract": "Non-autoregressive neural machine translation (NAT) predicts the entire target sequence si- multaneously and signi\ufb01cantly accelerates in- ference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often caus- ing token repetitions or missing. To allevi- ate this problem, we propose a novel semi- autoregressive model RecoverSAT in this work, which generates a translation as a se- quence of segments. The segments are gen- erated simultaneously while each segment is predicted token-by-token. By dynamically de- termining segment length and deleting repet- itive segments, RecoverSAT is capable of re- covering from repetitive and missing token er- rors. Experimental results on three widely- used benchmark datasets show that our pro- posed model achieves more than 4 \u00d7 speedup while maintaining comparable performance compared with the corresponding autoregres- sive model. 1"}
{"introduction": "non-autoregressive NMT (NART) models ( et al. , 2018 ) have been proposed to alleviate the low translation speeds of autoregressive NMT (ART) models. However, these models suffer from degen- erated translation quality ( Gu et al. , 2018 ; Sun et al. , 2019 ). To improve the translation quality of NART, several studies on NART iteratively re\ufb01ne decoded outputs with minimal iterations ( et al. , 2019 ; , 2020a ; , 2020 ; Guo et al. , 2020 ; Saharia et al. , 2020 ); other recent \u2217 This work was done during an internship at Kakao Enterprise. \u2020 Corresponding author. works target to improve NART without iteration ( Qian et al. , 2021 ; Gu and Kong , 2021 ). One of the signi\ufb01cant limitations of non-iterative NART models is the multi-modality problem . This problem originates from the fact that the models should maximize the probabilities of multiple tar- gets without considering conditional dependencies between target tokens. For example, in English-to- German translation, a source sentence \"Thank you very much.\" can be translated to \"Danke sch\u00f6n.\" or \"Vielen Dank.\". Under the conditional indepen- dence assumption, the non-iterative NART models are likely to generate improper translations such as \"Danke Dank.\" or \"Vielen sch\u00f6n.\" ( Gu et al. , 2018 ). lems such as token repetition or omission occur frequently in non-iterative NART ( , 2021 ). NART to address the multi-modality problem . dependencies between the target tokens ( , 2021 ). For example, ( 2020 ), ( 2020 ), and ( 2021 ) modify the objective function based on dy- namic programming, whereas ( 2021 ) provide target tokens to the decoder during train- ing. plicit reduction of the modality of the target dis- tribution by utilizing external source or target sen- tence information rather than modifying the objec- tive function. For example, ( 2019 ) and ( 2021 ) use syntactic or semantic in- formation; ( 2018 ), ( 2020b ), and ( 2021 ) use the alignment informa- tion between source and target tokens. However, previous explicit modality reduction methods show suboptimal performance. ( 2020b ) and ( 2021 ) ex- tract fertility ( , 1993 ) and ordering 2 information in word alignments, which enables the modeling of several types of mappings except for many-to-one and many-to-many cases. We hypoth- esize that leveraging entire mappings signi\ufb01cantly reduces the modality and is the key to performance improvement. iterative NART model that mitigates the multi- modality problem by utilizing complete informa- tion in word alignments. AligNART divides the ma- chine translation task into ( i ) alignment estimation and ( ii ) non-autoregressive translation under the given alignments. Modeling all the type of mapping guides ( ii ) more close to one-to-one translation. In AligNART, a module called Aligner is simply aug- mented to NAT ( , 2018 ) which estimates alignments to generate aligned decoder inputs. plex alignment information using only source sentence during inference. Speci\ufb01cally, Aligner should simultaneously predict the number of tar- get tokens corresponding to each source token and their mapping. To overcome this problem, we further propose alignment decomposition which factorizes the alignment process into three sub- processes: duplication , permutation , and group- ing . Each sub-process corresponds to much feasi- ble sub-problems: one-to-many mapping, ordering, and many-to-one mapping, respectively. outperforms previous non-iterative NART models of explicit modality reduction on WMT14 En \u2194 De and WMT16 Ro \u2192 formance comparable to that of the recent state- of-the-art non-iterative NART model on WMT14 \u2194 in AligNART addresses the token repetition issue even without sequence-level knowledge distillation ( , 2016 ). We also conduct quantita- tive and qualitative analyses on the effectiveness of alignment decomposition . 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing AligNART, an iterative non-autoregressive neural machine translation (NMT) model that addresses the multi-modality problem by utilizing complete information in word alignments.\n2. Introducing alignment decomposition, which factorizes the alignment process into three sub-processes: duplication, permutation, and grouping, to overcome the problem of predicting complex alignment information using only the source sentence during inference.\n3. Demonstrating that AligNART outperforms previous non-iterative NART models of explicit modality reduction on WMT14 En \u2194 De and WMT16 Ro \u2192 En, and achieves performance comparable to that of the recent state-of-the-art non-iterative NART model on WMT14 En \u2194 De.\n4. Addressing the token repetition issue even without sequence-level knowledge distillation.\n5. Conducting quantitative and qualitative analyses on the effectiveness of alignment decomposition.", "title": "AligNART: Non-autoregressive Neural Machine Translation by Jointly Jongyoon Song Sungroh Yoon Abstract Acknowledgement References Appendix", "words": 894, "abstract": "Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsis- tency such as token repetition. Most recent ap- proaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into ( i ) alignment estimation and ( ii ) translation with aligned decoder in- puts, guiding the decoder to focus on sim- pli\ufb01ed one-to-one translation. To alleviate the alignment estimation problem, we further pro- pose a novel alignment decomposition method. Our experiments show that AligNART out- performs previous non-iterative NART models that focus on explicit modality reduction on WMT14 En \u2194 De and WMT16 Ro \u2192 En. Fur- thermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art con- nectionist temporal classi\ufb01cation based mod- els on WMT14 En \u2194 De. We also observe that AligNART effectively addresses the token rep- etition problem even without sequence-level knowledge distillation. 1"}
{"introduction": "state-of-the-art performance on a wide range of text generation tasks, such as machine transla- tion ( Vaswani et al. , 2017 ) and text summarization ( , 2015 ). Such models generate a token sequence in a left-to-right, token-by-token fashion. The prediction for the next token is conditioned on all previously generated tokens. This characteris- tic makes it impossible to parallelize the computa- tional overhead for token predictions in different 1 All related code, data, and models can be found in https://github.com/yxuansu/NAG-BERT. positions, which leads to a relatively high latency in inference. On the other hand, non-autoregressive generation (NAG) models ( , 2018 ) have emerged as a promising alternative due to their fast inference speed. NAG models omit the sequential dependencies within the output-side sequence and predict tokens in all positions simultaneously once the output length has been determined beforehand. While NAG models enjoy full parallelism and faster inference, the generation quality of NAG models often lags behind their autoregressive counterparts. scale pre-trained language models for improving the performance of non-autoregressive generation. Speci\ufb01cally, we utilize BERT ( Devlin et al. , 2019 ) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer ( , 2001 ; , 2019 ) for better capturing the output-side dependencies. tions that NAG models currently suffer from: (1) the in\ufb02exibility of pre\ufb01xed output length, and (2) the conditional independence of individual token predictions. Accordingly, we devise two solutions to these two problems. length to be determined before token generation, thus an extra module for output length prediction is always required. Nevertheless, the most likely length from the prediction module is not neces- sarily the best-suited one for the token generation model. To this end, previous works ( Gu et al. , 2018 ; , 2019 ) usually rely on length-parallel de- coding (LPD) ( , 2019 ) for performance enhancement; that is, generating and re-ranking the results from different output length candidates. In this work, we propose a simple and elegant decod- ing mechanism that lets the model determine the output length on-the-\ufb02y. Speci\ufb01cally, our model dynamically adjusts the output sequence length via arXiv:2102.08220v1  [cs.CL]  16 Feb 2021 emitting an [eos] token at any output position to indicate the ending of the generated sequence. output length prediction and results re-ranking. token predictions in different positions are condi- tionally independent. As a consequence, they often tend to generate results that are ungrammatical with repetitions ( , 2019b ). To alleviate this problem, we propose a context-aware learning ob- jective which impels the model to output different tokens at adjacent positions, thereby reducing the possibility of repetitive generation. the output sequence (summary) is known to be shorter than the source sequence (article). In such cases, to further improve the model\u2019s inference ef- \ufb01ciency, we introduce a new ratio-\ufb01rst decoding strategy. Speci\ufb01cally, instead of performing infer- ence on all source-side hidden states, ratio-\ufb01rst gen- erates the result only based on a subset of source hidden states. The subset size is jointly determined by the source length T and a prede\ufb01ned ratio \u03b1 that is set based on our prior knowledge from the data statistics. In the experiments, we show that ratio-\ufb01rst can signi\ufb01cantly improve the inference speed while maintaining the generation quality. We evaluate the proposed model on three typical text generation tasks, including text summarization, sentence compression and machine translation. Ex- perimental results show that our model signi\ufb01cantly outperforms many strong non-autoregressive base- lines, and even performs competitively with several strong autoregressive models. In addition, we con- duct extensive analysis experiments to study the effect of individual proposed components. pose a novel framework that utilizes BERT for text generation under the non-autoregressive generation paradigm; (2) We propose a decoding mechanism that allows the model to dynamically determine the output length, and a new context-aware learning objective that reduces errors stemming from the output-side conditional independence assumption; (3) We introduce a ratio-\ufb01rst decoding strategy that further improve the model\u2019s inference ef\ufb01ciency. 2", "contribution": "The main contributions of this paper are:\n\n1. A novel framework that utilizes BERT for text generation under the non-autoregressive generation paradigm.\n2. A decoding mechanism that allows the model to dynamically determine the output length, and a new context-aware learning objective that reduces errors stemming from the output-side conditional independence assumption.\n3. Introduction of a ratio-first decoding strategy that further improves the model's inference efficiency.", "title": "Non-Autoregressive Text Generation with Pre-trained Language Models", "words": 954, "abstract": "Non-autoregressive generation (NAG) has re- cently attracted great attention due to its fast inference speed. However, the generation qual- ity of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mech- anisms to alleviate the two common prob- lems of vanilla NAG models: the in\ufb02exibil- ity of pre\ufb01xed output length and the condi- tional independence of individual token predic- tions. Lastly, to further increase the speed ad- vantage of the proposed model, we propose a new decoding strategy, ratio-\ufb01rst , for applica- tions where the output lengths can be approx- imately estimated beforehand. For a compre- hensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and ma- chine translation. Experimental results show that our model signi\ufb01cantly outperforms exist- ing non-autoregressive baselines and achieves competitive performance with many strong au- toregressive models. In addition, we also con- duct extensive analysis experiments to reveal the effect of each proposed component. 1 1"}
{"introduction": "024 Neural Machine Translation (NMT) achieves great 025 success in recent years, and typical sequence-to- 026 sequence frameworks like Transformer ( 027 et al. , 2017 ) achieved state-of-the-art performance 028 on the task of NMT. In this framework, source 029 sentences are translated in an autoregressive (AT) 030 manner where each token is generated depending 031 on previously generated tokens, inevitably, such se- 032 quential decoding strategy result in a high inference 033 latency. To alleviate this issue, Non-autoregressive 034 translation (NAT; Gu et al. , 2018 ) was proposed to 035 speed-up decoding procedure by generating target 036 tokens in parallel. However, the translation quality 037 of vanilla NAT is compromised, one of the most 038 significant problem is multi-modality and it usually 039 results in multiple translation results, duplicate or 040 missing words in target sentences of NAT models 041 ( , 2018 ). This situation results from the 042 conditional independence proposed by NAT, since 043 models are trained to maximize the log-probability 044 of target tokens at each position while the interde- 045 pendency is omitted. 046 047 performing dependency reduction ( , 048 2021 ) by modeling the target dependency informa- 049 tion implicitly or explicitly so decoder can ease 050 the difficulty of learning and capturing the infor- 051 mation between target tokens and generate more 052 accurate translations. For example, Ghazvininejad 053 et al. ( 2019 ) ( 2020b ) and ( 2020 ) model 054 the target dependency by providing observed tar- 055 get tokens in training and performing iterative in- 056 ference. ( 2021 ) generates intermediate 057 representations by permuting the source sentences 058 in the target order. ` y and Helcl ( 2018 ) 059 aligns model outputs with target tokens implicitly 060 by applying Connectionist Temporal Classification 061 (CTC; Graves et al. , 2006 ). 062 Previous works have validated the effectiveness 063 of applying Variational Autoencoder (VAE) on AT 064 ( ; ; 065 2018 ) and NAT ( Kaiser et al. 2018 ; Shu et al. 2020 ) 066 frameworks to alleviate multi-modality issue. A 067 representative NAT model is LaNMT 1 ( , 068 2020 ) which encodes the source and target tokens 069 into intermediate Gaussian distribution latent vari- 070 ables and outperforms vanilla NAT with about 5.0 071 BLEU points on WMT14 En-De task with 12 . 5 \u00d7 072 speedup to base Transformer. However, there ex- 073 ists a slight lag behind the state-of-the-art fully 074 NAT models. It may be attributed to two reasons: 075 (1) The inadequate representations of latent vari- 076 ables which are low in dimensions (4 to 32 is 077 recommended). This is significantly lower than 078 the model\u2019s hidden size (512) while high-capacity 079 latent variables conversely deteriorate the perfor- 080 mance because the minimization between prior 081 1 https://github.com/zomux/lanmt 1 and posterior becomes difficult ( , 2020 ). 082 (2) The mismatch between training and inference 083 circumstances that the posterior module receives 084 the gold sentence as inputs during training but im- 085 perfect initial translation instead during inference. 086 087 ness of the latent representation and move the train- 088 ing circumstance close to inference circumstance. 089 To this end, we apply consistency regularization 090 over the posterior network to improve its robustness 091 for better latent representations since the posterior 092 is the key module that both encoder and decoder 093 are relying on its latent representations during train- 094 ing. To cooperate with consistency regularization, 095 and simultaneously, close the gap between training 096 and inference circumstances for better refinement 097 from imperfect initial translations during inference, 098 four data augmentation methods are adopted to 099 work together. Specifically, we first apply stochas- 100 tic data augmentation methods e.g. 101 et al. , 2020 ) to inject stochastic noises in posterior 102 inputs x and y to get two different views. Both 103 views are then forwarded to the posterior network 104 for two latent variables z 1 , z 2 . As these two latent 105 variables are derived from the same pair of input x 106 and y , the gap between them is trained to be min- 107 imised by consistency regularization. Meanwhile, 108 posterior module receives noisy views instead of 109 gold samples during training, it is more adaptive 110 to the inputs with imperfect initial translations in 111 inference. 112 113 of our methods on WMT14 En-De, De-En and 114 WMT16 En-Ro benchmarks. Our methods out- 115 perform the latent variable baseline with about 116 1.3/0.7/0.8 BLEU points improvement on three 117 benchmarks. With these improvements, we achieve 118 the comparable performance to the state-of-the-art 119 fully NAT approaches: 25.47/30.23/31.56 BLEU 120 scores on WMT14 En-De/De-En/WMT16 En-Ro 121 with similar decoding speed, and it can be improved 122 further with latent search. The contributions of our 123 work can be summarized as follows: 124 \u2022 125 propose posterior consistency regularization 126 on the posterior latent variables, which im- 127 proves the translation quality by training a 128 more robust posterior network. 129 \u2022 130 and inference circumstances and cooperate 131 with posterior consistency regularization, we 132 apply four data augmentation methods where 133 all of them benefit to the translation quality. 134 \u2022 135 ing the translation quality of the base latent- 136 variable NAT model to be comparable with 137 the state-of-the-art fully NAT frameworks. 138 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing posterior consistency regularization on the posterior latent variables, which improves the translation quality by training a more robust posterior network.\n\n2. Applying four data augmentation methods to bridge the gap between training and inference circumstances and cooperate with posterior consistency regularization, where all of them benefit the translation quality.\n\n3. Improving the translation quality of the base latent-variable NAT model to be comparable with the state-of-the-art fully NAT frameworks.", "title": "Non-Autoregressive Neural Machine Translation with Consistency Anonymous ACL submission Abstract Conclusion References", "words": 1289, "abstract": "Variational Autoencoder (VAE) is an effec- 001 tive way to model the interdependency for 002 Non-autoregressive neural machine translation 003 (NAT). LaNMT, a representative VAE-based 004 latent-variable NAT framework achieves great 005 improvements to vanilla models, but still suf- 006 fers from two main issues which lower down 007 the translation quality: (1) mismatch between 008 training and inference circumstances and (2) 009 inadequacy of latent representations. In this 010 work, we target on addressing these issues by 011 proposing posterior consistency regularization. 012 Specifically, we first apply stochastic data aug- 013 mentation on the input samples to better adapt 014 the model for inference circumstance, and then 015 perform consistency training on posterior la- 016 tent variables to train a more robust posterior 017 network with better latent representations. Ex- 018 periments on En-De/De-En/En-Ro benchmarks 019 confirm the effectiveness of our methods with 020 about 1.3/0.7/0.8 BLEU points improvement 021 to the baseline model with about 12 . 6 \u00d7 faster 022 than autoregressive Transformer. 023 1"}
{"introduction": ", 2018 ; , 2019 ; , 2018 ; Ghazvininejad et al. , 2019 ) is a promising text gen- eration model for machine translation. It introduces the conditional independent assumption among the target language outputs and simultaneously gener- ates the whole sentence, bringing in a remarkable ef\ufb01ciency improvement (more than 10 \u00d7 speed-up) versus the autoregressive model. However, the NAT models still lay behind the autoregressive models in terms of BLEU ( Papineni et al. , 2002 ) for machine translation. We attribute the low-quality of NAT models to the lack of dependencies modeling for the target outputs, making it harder to model the generation of the target side translation. A promising way is to model the dependencies of the target language by the latent variables. A line of research works ( , 2018 ; , 2018 ; , 2019 ; , 2019 ) introduce latent variable modeling to the non-autoregressive Transformer and improves translation quality. The latent variables could be regarded as the spring- board to bridge the modeling gap, introducing more informative decoder inputs than the previ- ously copied inputs. More speci\ufb01cally, the latent- variable based model \ufb01rst predicts a latent vari- able sequence conditioned on the source represen- tation, where each variable represents a chunk of words. The model then simultaneously could gen- erate all the target tokens conditioning on the latent sequence and the source representation since the target dependencies have been modeled into the latent sequence. the chunks, the above approaches always rely on a large number (more than 2 15 , , 2018 ; Roy et al. , 2018 ) of latent codes for discrete latent spaces, which may hurt the translation ef\ufb01ciency\u2014 the essential goal of non-autoregressive decoding. ( 2019 ) introduce syntactic labels as a proxy to the learned discrete latent space and improve the NATs\u2019 performance. The syntactic label greatly reduces the search space of latent codes, leading to a better performance in both qual- ity and speed. However, it needs an external syn- tactic parser to produce the reference syntactic tree, which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency between latent variables for non-autoregressive de- coding ef\ufb01ciently. codes that can act like the syntactic label, which is learned without using the explicit syntactic trees. use each latent code to represent a fuzzy target category instead of a chunk as the previous re- search ( , 2019 ). More speci\ufb01cally, we \ufb01rst employ vector quantization ( , 2018 ) to discretize the target language to the la- tent space with a smaller number (less than 128) of latent variables, which can serve as the fuzzy word-class information each target language word. We then model the latent variables with conditional random \ufb01elds (CRF, Lafferty et al. , 2001 ; Sun et al. , 2019 ). To avoid the mismatch of the training and 5750 inference for latent variable modeling, we propose using a gated neural network to form the decoder inputs. Equipping it with scheduled sampling ( Ben- gio et al. , 2015 ), the model works more robustly. show that CNAT achieves the new state-of-the- art performance without knowledge distillation. With the sequence-level knowledge distillation and reranking techniques, the CNAT is comparable to the current state-of-the-art iterative-based model while keeping a competitive decoding speedup. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing a new non-autoregressive translation model called CNAT that uses latent variables to model the dependencies of the target language and fuzzy word-class information for each target language word.\n2. Using vector quantization and conditional random fields to model the latent variables efficiently.\n3. Proposing a gated neural network to form the decoder inputs and using scheduled sampling to make the model more robust.\n4. Achieving state-of-the-art performance without knowledge distillation and being comparable to the current state-of-the-art iterative-based model while keeping a competitive decoding speedup with sequence-level knowledge distillation and reranking techniques.", "title": "Non-Autoregressive Translation by Learning Target Categorical Codes Yu Bao Xinyu Dai Abstract Acknowledgments References", "words": 749, "abstract": "Non-autoregressive Transformer is a promis- ing text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation qual- ity. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The in- teraction among these categorical codes reme- dies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks than several strong baselines. 1"}
{"introduction": "\nWhen training NAR models for neural machine\ntranslation (NMT), sequence-level knowledge dis-\ntillation (Kim and Rush, 2016) is key to match\nthe translation quality of autoregressive (AR) mod-\nels (Gu et al., 2018; Lee et al., 2018; Ghazvinine-\njad et al., 2019; Gu et al., 2019). Knowl-\nedge distillation was \ufb01rst proposed to obtain\nsmall student models that match the quality of\na higher-capacity teacher models (Liang et al.,\nSequence-level\n2008; Hinton et al., 2015).\nknowledge distillation (SLKD) trains the student\nmodel p(y | x) to approximate the teacher distribu-\ntion q(y | x) by maximizing the following objec-\ntive: LSEQ-KD = \u2212 (cid:80)\ny\u2208Y q(y | x) log p(y | x) \u2248\n\u2212 (cid:80)\n1 [y = \u02c6y] log p(y | x), where Y repre-\nsents the space of all possible target sequences,\nand \u02c6y is the output from running beam search with\nthe teacher model q.\n\ny\u2208Y\n\n\u2217Work done during internship at Microsoft Research Asia.\n\nHowever, we do not yet have a clear picture for\nhow SLKD impacts NAR training. Ren et al. (2020)\nshow that SLKD reduces the degree of dependency\nbetween target tokens. Gu et al. (2018) hypothe-\nsize that SLKD reduces the number of modes in\nthe output distribution (alternative translations for\na source). This hypothesis was supported by exper-\niments that use multiway parallel data to simulate\nthe modes (Zhou et al., 2019). Zhou et al. (2019)\nalso investigate the impact of data complexity on\nNAR translation quality \u2013 they generate distilled\ndata of varying complexity with AR models of\ndifferent capacity and show that higher-capacity\nNAR models require more complex distilled data\nto achieve better translation quality. They further\nshow that generating distilled references with mix-\nture of experts (Shen et al., 2019) improves NAR\ntranslation quality. However, training samples can\nbe complex in different ways, and it remains un-\nclear how different types of data complexity alter\nthe internal working of NAR models and their trans-\nlation quality. We also anticipate that data com-\nplexity may impact the uncertainty and calibration\nof NAR models \u2013 an understudied question, unlike\nfor AR models (Ott et al., 2018; Wang et al., 2020).\nThis paper focuses on two types of data com-\nplexity \u2013 lexical diversity and degree of word re-\nordering. We expose two state-of-the-art NAR\nmodels (Mask-Predict (Ghazvininejad et al., 2019)\nand Levenshtein Transformer (Gu et al., 2019)) to\ndistilled references of varying complexity on the\nWMT14 German-English task. Experiments show\nthat decreasing reordering complexity and reducing\nlexical diversity via distillation both help NAR mod-\nels learn better alignment between source and target\nand thus improve translation quality. Further analy-\nsis shows that knowledge distillation lowers model\nuncertainty by reducing lexical diversity, which\naffects the calibration of Mask-Predict and Leven-\nshtein Transformer models in opposite directions.", "contribution": "The main contributions of this paper are: \n1. Investigating the impact of sequence-level knowledge distillation (SLKD) on non-autoregressive (NAR) models for neural machine translation (NMT).\n2. Examining the effect of data complexity, specifically lexical diversity and degree of word reordering, on NAR models and their translation quality.\n3. Showing that decreasing reordering complexity and reducing lexical diversity via distillation can improve NAR models' alignment between source and target and thus improve translation quality.\n4. Demonstrating that SLKD lowers model uncertainty by reducing lexical diversity, which affects the calibration of Mask-Predict and Levenshtein Transformer models in opposite directions.", "title": "How Does Distilled Data Complexity Impact the Quality Weijia Xu Abstract Acknowledgments References", "words": 727, "abstract": "While non-autoregressive ( NAR ) models are showing great promise for machine translation ( MT ), their use is limited by their dependence on knowledge distillation from au- toregressive models. To address this issue, we seek to understand why distillation is so effec- tive. Prior work suggests that distilled training data is less complex than manual translations. Based on experiments with the Levenshtein Transformer and the Mask-Predict NAR mod- els on the WMT14 German-English task, this paper shows that different types of complexity have different impacts: while reducing lexical diversity and decreasing reordering complex- ity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model con\ufb01dence, which affects the calibration of different NAR models differently. 1"}
{"introduction": "ceived increasing attention for its ef\ufb01cient decod- ing by predicting every target token in parallel ( Gu et al. , 2018 ; Ghazvininejad et al. , 2019 ). However, such advantage comes at the cost of sacri\ufb01cing translation quality due to the multimodality prob- lem: there exist many possible translations of the same sentence, while vanilla NAT models may con- sider them at the same time due to the independent predictions, which leads to multi-modal outputs in the form of token repetitions ( Gu et al. , 2018 ). improving the standard cross-entropy (XE) loss to ameliorate the effect of multimodality. The motiva- tion for these works is that modeling word order is dif\ufb01cult for NAT, since the model cannot condition on its previous predictions like its autoregressive counterpart. Starting from this intuition, a thread of \u2217 Zhaopeng Tu is the corresponding author. 1 The codes and models are in https://github. com/tencent-ailab/machine-translation/ COLING22_ngram-OAXE/ . research relaxes the word order restriction based on the monotonic alignment assumption ( and Helcl , 2018 ; , 2020 ; haria et al. , 2020 ). ( 2021 ) take a further step by removing the penalty of word order errors with a novel order-agnostic cross entropy (O A XE) loss, which enables NAT models to handle word reordering \u2013 a common source of multimodality problem. Accordingly, O A XE achieves the best performance among these model variants. However, O A XE allows reordering between ev- ery two words, which is not always valid in prac- tice. For example, the reordering of the two words \u201cthis afternoon\u201d is not correct in grammar. The re- ordering generally occurs between ngram phrases, such as \u201cI ate pizza\u201d and \u201cthis afternoon\u201d. Starting from this intuition, we extend O A XE by constrain- ing the reordering between ngrams and requiring a strict match of word order within each ngram (i.e., ngram -O A XE). To this end, we \ufb01rst build the probability distributions of ngrams in the target sentence using the word probabilities produced by NAT models. Then we \ufb01nd the best ordering of target ngrams to minimize the cross entropy loss. ngram -O A XE loss in an ef\ufb01- cient way, which only adds one more line of code on top of the source code of O A XE. Accordingly, ngram -O A XE only marginally increases training time (e.g., 3% more time) over O A XE. Experimental results on widely-used NAT bench- marks show that ngram -O A XE improves transla- tion performance over O A XE in all cases. Encour- agingly, ngram -O A XE outperforms O A XE by up to +3.8 BLEU points on raw data (without knowl- edge distillation) for WMT14 En-De translation (Table 1 ), and narrows the performance gap be- tween training on raw data and on distilled data. Further analyses show that ngram -O A XE improves over O A XE on the generation accuracy of ngram phrases and modeling reordering between ngram phrases, which makes ngram -O A XE handle long arXiv:2210.03999v1  [cs.CL]  8 Oct 2022 ate I pizza this afternoon 0.2 0.1 0.1 0.5 0.1 0.1 0.1 0.1 0.1 0.6 Vocabulary 0.4 0.1 0.3 0.1 0.1 0.1 0.4 0.1 0.3 0.1 0.1 0.1 0.5 0.1 0.3 Output Probability Distribution Pos:1 Pos:2 Pos:3 Pos:4 Pos:5 I ate ate pizza pizza this this afternoon 0.02 0.01 0.01 0.30 0.01 0.03 0.01 0.01 Bigram List 0.16 0.01 0.09 0.01 0.01 0.20 0.01 0.09 Output Probability Distribution Pos:1,2 Pos:2,3 Pos:3,4 Pos:4,5 Word Distribution Bigram Distribution Figure 1: Illustration of the proposed ngram -O A XE loss with N = 2 (i.e., bigram-O A XE). We only show the probabilities of the target words and bigrams for better illustration. Firstly, ngram -O A XE transforms the word probability distributions to the bigram distributions by multiplying the word probabilities at the corresponding positions. For example, P(\u201cI ate\" | Pos:1,2) = P(\u201cI\u201d | Pos:1) * P(\u201cate\u201d | Pos:2) = 0.2*0.1=0.02. Then, we select the ngrams (highlighted in bold) for each neighbouring positions using the ef\ufb01cient Hungarian algorithm. sentences better, especially on raw data. strength of ngram -O A XE on directly learning from the complex raw data indicates the potential to train NAT models without knowledge distillation. 2", "contribution": "The main contribution of this academic paper is the proposal of a new loss function called ngram-OAXE, which improves the performance of Neural Architecture Translation (NAT) models by constraining the reordering between ngrams and requiring a strict match of word order within each ngram. The ngram-OAXE loss function is an extension of the order-agnostic cross-entropy (OAXE) loss function, which removes the penalty of word order errors and enables NAT models to handle word reordering. The paper shows that ngram-OAXE outperforms OAXE by up to +3.8 BLEU points on raw data for WMT14 En-De translation and narrows the performance gap between training on raw data and on distilled data. The paper also demonstrates that ngram-OAXE improves over OAXE on the generation accuracy of ngram phrases and modeling reordering between ngram phrases, which makes ngram-OAXE handle long sentences better, especially on raw data. The strength of ngram-OAXE on directly learning from the complex raw data indicates the potential to train NAT models without knowledge distillation.", "title": "ngram-OAXE : Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation ", "words": 1163, "abstract": "Recently, a new training O A XE loss ( Du et al. , 2021 ) has proven effective to ameliorate the effect of multimodality for non-autoregressive translation (NAT), which removes the penalty of word order errors in the standard cross- entropy loss. Starting from the intuition that reordering generally occurs between phrases, we extend O A XE by only allowing reordering between ngram phrases and still requiring a strict match of word order within the phrases. Extensive experiments on NAT benchmarks across language pairs and data scales demon- strate the effectiveness and universality of our approach. Further analyses show that ngram - O A XE indeed improves the translation of ngram phrases, and produces more \ufb02uent trans- lation with a better modeling of sentence struc- ture. 1 1"}
{"introduction": "Neural networks have been successfully applied to a variety of tasks, including machine transla- tion. The encoder-decoder architecture is the cen- tral idea of neural machine translation (NMT). The encoder \ufb01rst encodes a source-side sentence x = x 1 . . . x m into hidden states and then the decoder generates the target-side sentence y = y 1 . . . y n from the hidden states according to an autoregres- sive model p ( y t | y 1 . . . y t \u2212 1 , x ) Recurrent neural networks (RNNs) are inherently good at processing sequential data. Sutskever \u2217 Part of this work was done when the author was at In- stitute of Automation, Chinese Academy of Sciences. y 1 y 2 y 3 y 4 y 5 Autoregressive y 6 y 1 y 2 y 3 y 4 y 5 Semi-Autoregressive y 6 y 1 y 2 y 3 y 4 y 5 Non-Autoregressive y 6 Figure 1: The different levels of autoregressive proper- ties. Lines with arrow indicate dependencies. We mark the longest dependency path with bold red lines. The length of the longest dependency path decreases as we relieve the autoregressive property. An extreme case is non-autoregressive , where there is no dependency at all. et al. ( 2014 ); Cho et al. ( 2014 ) successfully ap- plied RNNs to machine translation. Bahdanau et al. ( 2014 ) introduced attention mechanism into the encoder-decoder architecture and greatly im- proved NMT. GNMT ( Wu et al. , 2016 ) further im- proved NMT by a bunch of tricks including resid- ual connection and reinforcement learning. The sequential property of RNNs leads to its wide application in language processing. How- ever, the property also hinders its parallelizability thus RNNs are slow to execute on modern hard- ware optimized for parallel execution. As a result, a number of more parallelizable sequence models were proposed such as ConvS2S ( Gehring et al. , 2017 ) and the Transformer ( Vaswani et al. , 2017 ). These models avoid the dependencies between dif- 480 ferent positions in each layer thus can be trained much faster than RNN based models. When infer- ence, however, these models are still slow because of the autoregressive property. A recent work ( Gu et al. , 2017 ) proposed a non-autoregressive NMT model that generates all target-side words in parallel. While the paral- lelizability is greatly improved, the translation quality encounter much decrease. In this paper, we propose the semi-autoregressive Transformer (SAT) for faster sequence generation. Unlike Gu et al. ( 2017 ), the SAT is semi-autoregressive, which means it keeps the autoregressive property in global but relieves in local. As the result, the SAT can produce multiple successive words in parallel at each time step. Figure 1 gives an il- lustration of the different levels of autoregressive properties. Experiments conducted on English-German and Chinese-English translation show that com- pared with non-autoregressive methods, the SAT achieves a better balance between translation qual- ity and decoding speed. On WMT\u201914 English- German translation, the proposed SAT is 5.58 \u00d7 faster than the Transformer while maintaining 88% of translation quality. Besides, when produc- ing two words at each time step, the SAT is almost lossless. It is worth noting that although we apply the SAT to machine translation, it is not designed speci\ufb01cally for translation as Gu et al. ( 2017 ); Lee et al. ( 2018 ). The SAT can also be applied to any other sequence generation task, such as summary generation and image caption generation. 2", "contribution": "The main contribution of this paper is the proposal of a semi-autoregressive Transformer (SAT) for faster sequence generation. Unlike previous non-autoregressive models, the SAT maintains the autoregressive property in global but relieves it in local, allowing for the production of multiple successive words in parallel at each time step. Experiments show that the SAT achieves a better balance between translation quality and decoding speed compared to non-autoregressive methods, and can be applied to any other sequence generation task.", "title": "Semi-Autoregressive Neural Machine Translation Chunqi Wang Abstract Acknowledgments References", "words": 834, "abstract": "Existing approaches to neural machine trans- lation are typically autoregressive models. While these models attain state-of-the-art translation quality, they are suffering from low parallelizability and thus slow at decod- ing long sequences. In this paper, we propose a novel model for fast sequence generation \u2014 the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive property in global but relieves in local and thus are able to produce multiple successive words in parallel at each time step. Experiments conducted on English-German and Chinese- English translation tasks show that the SAT achieves a good balance between translation quality and decoding speed. On WMT\u201914 English-German translation, the SAT achieves 5.58 \u00d7 speedup while maintaining 88% trans- lation quality, signi\ufb01cantly better than the pre- vious non-autoregressive methods. When pro- duces two words at each time step, the SAT is almost lossless (only 1% degeneration in BLEU score). 1"}
{"introduction": "marily developed and tested for machine translation (MT) ( 2015 ); ( 2017 ); ( 2018 ) are increasingly used for other se- quence transduction tasks. This paper focuses on editing tasks , such as post-editing of MT output ( , 2007 ), style transfer ( , 2020 ), or text simpli\ufb01cation ( , 1997 ; , 2015 ), where systems directly edit the input sequence, instead of generating the output from scratch as in MT. As illustrated in Table 1 , in these tasks, there might be substantial overlap in content between inputs and outputs, and also diverse rewrites, ranging from local substitutions to more complex restructuring. signed for these editing tasks, based on e.g., a staging an exhibition focusing on the 17th century self- portraits , highlighting the similarities and the differences between modern-day snapshots and historic works of art. now set to open an exhibit on the 17th century self- portraits . It shows the similarities and differ- ences between modern photos and artworks. Table 1: Text simpli\ufb01cation is an editing task, where the output sequence overlaps with the input, while in- corporating multiple rewrite types to restructure and simplify content. multistep, tag-then-edit approach ( et al. , 2017 ; , 2019 ; , 2019 ; , 2020 ), they can also be addressed with non-autoregressive (NAR) seq2seq models which generate their output by iteratively editing intermediate sequences ( , 2018 ; , 2019 ; , 2019 ; , 2019 ; et al. , 2020 ). NAR models hold the promise of pro- viding a more generic solution, where the model does not need to be tailored to a given editing task. training NAR models for editing tasks using the same strategy as for MT leads to a mismatch be- tween train and test settings that limits their gener- alization ability and output quality. Speci\ufb01cally, the learning algorithms designed for MT are aligned with inference strategies that generate output from an empty initial sequence. By contrast, in sequence editing tasks, the inference step is initialized in- stead with the original input sequence. In addition, since editing samples might range from limited lex- ical substitutions to more thorough rewrites, train- ing samples cover a wide range of edit distances. the more distant samples leading to undertrained models and poor generalization. By contrast, the arXiv:2203.09486v1  [cs.CL]  17 Mar 2022 distance between input and output samples in MT is more uniform, since it always involves at least lexical translation of the input tokens. ing framework, E DITING C URRICULUM , which dynamically exposes the model to more relevant edit actions during training and exploits the full spectrum of available training samples more ef- fectively. First, we design a new roll-in strategy, E DITING roll-in , that exposes the model to interme- diate sequences that it is more likely to encounter during inference. Second, we introduce a train- ing CURRICULUM to expose the model to training samples in order of increasing edit distance, thus gradually increasing the complexity of oracle edit operations that the model learns to imitate. of outputs on two challenging English text edit- ing tasks: controllable text simpli\ufb01cation (TS) and abstractive summarization. It also improves the degree of TS control by generating simpli\ufb01ed out- puts that match the target reading grade level better than the baselines. We conduct an extensive analy- sis which supports our hypothesis, and show that the sequences generated by our training policy im- prove exploration during training and are easier to learn from, leading to better generalization across samples with varying edit distances. Training with curriculum further improves output quality. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing a new editing framework called EDITING CURRICULUM, which dynamically exposes the model to more relevant edit actions during training and exploits the full spectrum of available training samples more effectively.\n2. Designing a new roll-in strategy, EDITING roll-in, that exposes the model to intermediate sequences that it is more likely to encounter during inference.\n3. Introducing a training CURRICULUM to expose the model to training samples in order of increasing edit distance, thus gradually increasing the complexity of oracle edit operations that the model learns to imitate.\n4. Improving the quality of outputs on two challenging English text editing tasks: controllable text simplification (TS) and abstractive summarization.\n5. Improving the degree of TS control by generating simplified outputs that match the target reading grade level better than the baselines.\n6. Conducting an extensive analysis which supports the hypothesis that the sequences generated by the training policy improve exploration during training and are easier to learn from, leading to better generalization across samples with varying edit distances.", "title": "An Imitation Learning Curriculum for Text Editing", "words": 802, "abstract": "We propose a framework for training non- autoregressive sequence-to-sequence models for editing tasks, where the original input se- quence is iteratively edited to produce the out- put. We show that the imitation learning al- gorithms designed to train such models for machine translation introduces mismatches be- tween training and inference that lead to un- dertraining and poor generalization in editing scenarios. We address this issue with two com- plementary strategies: 1) a roll-in policy that exposes the model to intermediate training se- quences that it is more likely to encounter dur- ing inference, 2) a curriculum that presents easy-to-learn edit operations \ufb01rst, gradually in- creasing the dif\ufb01culty of training samples as the model becomes competent. We show the ef\ufb01cacy of these strategies on two challenging English editing tasks: controllable text simpli- \ufb01cation and abstractive summarization. Our approach signi\ufb01cantly improves output quality on both tasks and controls output complexity better on the simpli\ufb01cation task. 1"}
{"introduction": "Transformer has been the de facto architecture for Neural Ma- chine Translation (Vaswani et al. 2017). In this framework, the decoder generates words one by one in a left-to-right man- ner. Despite its strong performance, the autoregressive decod- ing method causes a large latency in the inference phase (Gu et al. 2018). To break the bottleneck of the inference speed caused by the sequential conditional dependence, several non- autoregressive neural machine translation (NAT) models are proposed to generate all tokens in parallel (Figure 1(a)) (Gu et al. 2018; \u0141 ukasz Kaiser et al. 2018; Li et al. 2019; Ma et al. 2019). However, vanilla NAT models suffer from the cost of translation accuracy due to they remove the conditional dependence between target tokens. To close the gap from autoregressive models, iterative NAT models are proposed to re\ufb01ne the translation results. They bring conditional dependency between target tokens within several iterations (Ghazvininejad et al. 2019; Ghazvininejad, 2020; Guo, Xu, and Chen 2020). Among them, Ghazvinine- jad et al. (2019) \ufb01rst explore to apply conditional masked language model (CMLM) on NAT model (Figure 1(b)). Fol- lowing this framework, several CMLM-based NAT models Encoder Decoder \u210e \" \u210e # \u210e $ % & % ' \u210e & \u210e ' % \" % # % $ ( (a) Vanilla NAT Encoder Decoder [MASK] [MASK] ! \" ! # ! $ % ! & ! ' (b) CMLM-NAT Encoder Decoder [MASK] [MASK] ! \" ! # ! $ % ! & ! ' Encoder Decoder [MASK] [MASK] ! & ! # ! $ % ! \" ! ' ()*+,+-.*- (c) CMLM + shared mask cons. Encoder Decoder [MASK] [MASK] ! \" ! # ! $ % ! \" ! & Encoder Decoder [MASK] [MASK] ! ' ! # ! $ % ! \" ! & ()*+,+-.*- / / 012 (d) CMLM + model cons. model. (c) CMLM architecture with shared mask consistency, where the blue [MASK] means shared mask position in the two masked target sentences. (d) CMLM architecture with model consistency, where EMA means the exponential mov- ing average method. are proposed and obtain state-of-the-art performance com- pared with other NATs (Xie et al. 2020a; Guo, Xu, and Chen 2020). based NAT model has been fully exploited, since the masked language model has achieved signi\ufb01cant breakthroughs in natural language processing. improve the performance of the CMLM-based NAT model. Speci\ufb01cally, our approach includes two regularization meth- ods: shared mask consistency and model consistency . For shared mask consistency , as shown in Figure 1(c), we ran- domly mask different subset of the same target sentence twice. masked positions to be consistent with each other. As one arXiv:2108.08447v1  [cs.CL]  19 Aug 2021 original the cat went through an open window in the house . masked the cat [MASK] [MASK] an open [MASK] in the house . masked the cat went through an [MASK] [MASK] in the [MASK] . twice. The blue [MASK] indicates that the token is masked in both two masked sentences. example, consider the original sentence and two masked sen- tences in Table 1. The original token \u201dwindow\u201d is replaced with [MASK] in both two masked sentences. Although the contexts of \u201dwindow\u201d are different due to the random mask strategies, their semantics and generated distributions are ex- pected to be consistent across these two views. To make a summary, we introduce a new paradigm of regularization, dif- ferent mask strategies for the same target sentence, and the to- kens on the shared masked positions are semantic-preserving with different views. This approach is reminiscent of multi- view contrast learning (Tian, Krishnan, and Isola 2020), our method is not \u201dcontrast\u201d but only considers the consistency of \u201dpositive pairs\u201d. model consistency (Figure 1(d)), it is inspired by that checkpoint averaging is an essential method for im- proving the performance of machine translation (Vaswani et al. 2017). Similarly, Mean Teacher (Tarvainen and Valpola 2017) shows that using an average model as a teacher im- proves the results. Correspondingly, we construct an average model by updating the weights with an exponential moving average (EMA) method. Then we penalize the generated dis- tributions that are inconsistent between this average model and the online model. Note that we adopt the bidirectional error (MSE) as the consistency cost. This is related to mutual learning (Zhang et al. 2018) but without extra parameters. several public benchmark datasets. It outperforms previous NAT models and achieves comparable results with autore- gressive Transformer. Intuitively, our two proposed regular- ization methods have two advantages: 1) they can be seen as stabilizers to promote the robustness of the model to random- ness; 2) they reduce the discrepancy between the training and inference phase. shared mask consistency \ufb01rst enhances the robustness of the model to the random mask. Secondly, we adopt the mask-predict decoding method (Ghazvininejad et al. 2019), where the predicted target tokens are replaced by [MASK] symbols in the inference process. Especially in the \ufb01rst decoder iteration, all the target tokens are [MASK] symbols. This decoding strategy causes the discrepancy from training for random mask. Therefore, As a result of a more robust model to random mask, our proposed method can reduce the discrepancy between training and inference caused by [MASK] symbols, thus improving the translation quality. model consistency , it \ufb01rst penalizes the model sen- sitivity to the model weights, thus improving the robustness. same architecture but with different dropout units during training. Therefore, this regularization item also makes our model more robust to random dropout. Moreover, the dropout is closed during inference thus causing the discrepancy be- tween training and inference. By reason of more robust to dropout, the proposed model consistency method implicitly strengthens the generalization ability of the model and im- proves the performance with dropout closing during infer- ence. forms several state-of-the-art NAT models by over 0.36-1.14 BLEU on WMT14 EN \u2194 DE and WMT16 EN \u2194 RO datasets. baseline, our proposed NAT model achieves competitive per- formance, while signi\ufb01cantly reducing the cost of time during inference.", "contribution": "The main contributions of this paper are:\n\n1. Introducing two regularization methods, shared mask consistency and model consistency, to improve the performance of CMLM-based NAT models.\n2. Shared mask consistency enhances the robustness of the model to random mask and reduces the discrepancy between training and inference caused by [MASK] symbols, thus improving the translation quality.\n3. Model consistency penalizes the model sensitivity to the model weights, making the model more robust to random dropout and implicitly strengthening the generalization ability of the model.\n4. The proposed NAT model outperforms several state-of-the-art NAT models by over 0.36-1.14 BLEU on WMT14 EN \u2194 DE and WMT16 EN \u2194 RO datasets while significantly reducing the cost of time during inference.", "title": "MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive", "words": 1445, "abstract": "Abstract Conditional masked language models (CMLM) have shown impressive progress in non-autoregressive machine translation (NAT). They learn the conditional translation model by pre- dicting the random masked subset in the target sentence. Based on the CMLM framework, we introduce Multi-view Subset Regularization (MvSR), a novel regularization method to im- prove the performance of the NAT model. Speci\ufb01cally, MvSR consists of two parts: (1) shared mask consistency : we forward the same target with different mask strategies, and encourage the predictions of shared mask positions to be consistent with each other. (2) model consistency , we maintain an exponential moving average of the model weights, and enforce the pre- dictions to be consistent between the average model and the online model. Without changing the CMLM-based architec- ture, our approach achieves remarkable performance on three public benchmarks with 0.36-1.14 BLEU gains over previous NAT models. Moreover, compared with the stronger Trans- former baseline, we reduce the gap to 0.01-0.44 BLEU scores on small datasets (WMT16 RO \u2194 EN and IWSLT DE \u2192 EN)."}
{"introduction": "Creating a \"human-like\" dialogue system is one of the important goals of arti\ufb01cial intelligence. Recently, due to the rapid advancements in nat- ural language generation (NLG) techniques, data- driven approaches have attracted lots of research interest and have achieved impressive progress in producing \ufb02uent dialogue responses ( Shang et al. , 2015 ; Vinyals and Le , 2015 ; Serban et al. , 2016 ; Li et al. , 2016 ). However, such seq2seq models tend to degenerate generic or off-topic responses ( Tang et al. , 2019 ; Welleck et al. , 2020 ). An effective way to address this issue is to leverage external knowledge ( Zhou et al. , 2018a , b ) or topic information ( Xing et al. , 2017 ), which are integrated as additional semantic representations to improve dialogue informativeness. Although promising results have been obtained by equipping dialogue models with external knowl- I like  going shopping  and  watching tv . What are your  hobbies ? Same. I like to sit on my  couch  and watch  anime . That\u2019s cool. I'm a big fan of   japanese anime. I really like hearing its  music . Figure 1: An exemplar dialogue with concept transi- tions, where each utterance is composed of multiple associated concepts to convey diverse information. edge, the development of dialogue discourse still has its own challenge: human dialogue generally evolves around a number of concepts that might frequently shift in a dialogue \ufb02ow ( Zhang et al. , 2020 ). The lack of concept management strategies might lead to incoherent dialogue due to the loosely connected concepts. To address this problem, recent studies have combined concept planning with response generation to form a more coherent and controllable dialogue ( Wu et al. , 2019 ; Xu et al. , 2020a , b ; Wu et al. , 2020 ; Zhang et al. , 2020 ). Most of these approaches incorporate concepts into responses in an implicit manner, which cannot guarantee the appearance of a concept in a response. Compared with dialogue concepts, a large propor- tion of chit-chat words are common and usually have a high word frequency and are relatively over- optimized in language models ( Gong et al. , 2018 ; Khassanov et al. , 2019 ). Consequently, conven- tional seq2seq generators are more \"familiar\" with these generic words than those requiring concept management, which prevents introducing certain concepts to the response with sequential decoding (either greedily or with beam search) ( Mou et al. , 2016 ). Moreover, speakers naturally associate multiple concepts to proactively convey diverse information, e.g., action, entity, and emotion (see Figure 1 ). Unfortunately, most existing methods arXiv:2109.04084v1  [cs.CL]  9 Sep 2021 can only retrieve one concept for each utterance ( Tang et al. , 2019 ; Qin et al. , 2020 ). Another line of approaches attempt to explicitly integrate concepts into responses and generate the remaining words in both directions ( Mou et al. , 2016 ; Xu et al. , 2020a ), but they also fail to deal with multiple concepts. In this paper, we devise a concept-guided non- autoregressive model (CG-nAR) to facilitate dia- logue coherence by explicitly introducing multiple concepts into dialogue responses. Speci\ufb01cally, following Xu et al. ( 2020a ), a concept graph is constructed based on the dialogue data, where the vertices represent concepts, and edges represent concept transitions between utterances. Based on the concept graph, we introduce a novel multi- concept planning module that learns to manage concept transitions in a dialogue \ufb02ow. It recurrently reads historical concepts and dialogue context to attentively select multiple concepts in the proper order, which re\ufb02ects the transition and arrangement of target concepts. Then, we customize an Insertion Transformer ( Stern et al. , 2019 ) by initializing the selected concepts as a partial response for subsequent non-autoregressive generation. The remaining words of a response are generated in parallel, aiming to foster a fast and controllable decoding process. We conducted experiments on Persona-Chat ( Zhang et al. , 2018 ) and Weibo ( Shang et al. , 2015 ). The results of automatic and human evaluations show that CG-nAR achieves better performance in terms of response diversity and dialogue coherence. We also show that the inference time of our model is much faster than conventional seq2seq models. All our codes and datasets are publicly available. 1 Our contributions to the \ufb01eld are three-fold: 1) We design a concept-guided non-autoregressive strategy that can successfully integrate multiple concepts into responses for a controllable decoding process. 2) The proposed multi-concept plan- ning module effectively manages multi-concept transitions and remedies the problem of dialogue incoherence. 3) Comprehensive studies on two datasets show the effectiveness of our method in terms of response quality and decoding ef\ufb01ciency. 2", "contribution": "The main contributions of this paper are: \n1) Designing a concept-guided non-autoregressive strategy that can integrate multiple concepts into responses for a controllable decoding process. \n2) Developing a multi-concept planning module that manages multi-concept transitions and remedies the problem of dialogue incoherence. \n3) Conducting comprehensive studies on two datasets to show the effectiveness of the proposed method in terms of response quality and decoding efficiency.", "title": "Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive", "words": 1077, "abstract": "Human dialogue contains evolving concepts, and speakers naturally associate multiple con- cepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coher- ent dialogue, in this work, we devise a concept- guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The pro- posed model comprises a multi-concept plan- ning module that learns to identify multiple associated concepts from a concept graph and a customized Insertion Transformer that per- forms concept-guided non-autoregressive gen- eration to complete a response. The experi- mental results on two public datasets show that CG-nAR can produce diverse and coherent responses, outperforming state-of-the-art base- lines in both automatic and human evaluations with substantially faster inference speed. 1"}
{"introduction": "autoregressive translation (NAT, , 2018 ), which can improve the decoding ef\ufb01ciency by pre- dicting all tokens independently and simultane- ously. The non-autoregressive factorization breaks conditional dependencies among output tokens, \u2217 Liang Ding and Longyue Wang contributed equally to this work. Work was done when Liang Ding and Xuebo Liu were interning at Tencent AI Lab. which prevents a model from properly capturing the highly multimodal distribution of target trans- lations. As a result, the translation quality of NAT models often lags behind that of autoregressive translation (AT, , 2017 ) models. To balance the trade-off between decoding speed and translation quality, knowledge distillation (KD) is widely used to construct a new training data for NAT models ( , 2018 ). Speci\ufb01cally, target sentences in the distilled training data are gener- ated by an AT teacher, which makes NAT easily acquire more deterministic knowledge and achieve signi\ufb01cant improvement ( Zhou et al. , 2020 ). Previous studies have shown that distillation may lose some important information in the original training data, leading to more errors on predict- ing low-frequency words. To alleviate this prob- lem, Ding et al. ( 2021b ) proposed to augment NAT models the ability to learn lost knowledge from the original data. However, their approach relies on external resources (e.g. word alignment) and human-crafted priors, which limits the applicabil- ity of the method to a broader range of tasks and languages. Accordingly, we turn to directly expose the raw data into NAT by leveraging pretraining without intensive modi\ufb01cation to model architec- tures ( \u00a7 2.2 ). Furthermore, we analyze bilingual links in the distilled data from two alignment di- rections (i.e. source-to-target and target-to-source). words aligned with targets more deterministically but fails to align low-frequency words from tar- get to source due to information loss. Inspired by this \ufb01nding, we propose reverse KD to recall more alignments for low-frequency target words ( \u00a7 2.3 ). maintain advantages of deterministic knowledge and low-frequency information. To make the most of authentic and synthetic data, we combine three complementary approaches (i.e. raw pretraining, arXiv:2106.00903v2  [cs.CL]  26 Apr 2022 bidirectional distillation training and KD \ufb01netun- ing) as a new training strategy for further boosting NAT performance (\u00a7 2.4 ). benchmarks (WMT14 En-De, WMT16 Ro-En, WMT17 Zh-En, WAT17 Ja-En and WMT19 En- dict, Ghazvininejad et al. , 2019 ; Levenshtein Trans- former, , 2019 ). show that the proposed method consistently im- prove translation performance over the standard NAT models across languages and advanced NAT architectures. Extensive analyses con\ufb01rm that the performance improvement indeed comes from the better lexical translation accuracy especially on low-frequency tokens. Contributions Our main contributions are: \u2022 frequency information by pretraining NAT mod- els from raw data. \u2022 links to demonstrate the necessity to improve low-frequency alignment by leveraging both KD and reverse KD. \u2022 recipe to accomplish this goal, which is robustly applicable to several model structures and lan- guage pairs. 2", "contribution": "Main contributions of the paper are:\n\n- Introducing a new training strategy for non-autoregressive translation (NAT) models that combines raw pretraining, bidirectional distillation training, and knowledge distillation (KD) fine-tuning to improve translation performance.\n- Proposing reverse KD to recall more alignments for low-frequency target words and maintain advantages of deterministic knowledge and low-frequency information.\n- Demonstrating the necessity to improve low-frequency alignment by leveraging both KD and reverse KD.\n- Providing a recipe to accomplish these goals, which is robustly applicable to several model structures and language pairs.", "title": "Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation", "words": 710, "abstract": "Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low- frequency words between the distilled and the original data, leading to more errors on pre- dicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyz- ing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align suf\ufb01cient low-frequency words from tar- get to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low- frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new train- ing strategy for further boosting NAT perfor- mance. We conduct experiments on \ufb01ve trans- lation benchmarks over two advanced architec- tures. Results demonstrate that the proposed approach can signi\ufb01cantly and universally im- prove translation quality by reducing transla- tion errors on low-frequency words. Encour- agingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, re- spectively. Our code, data, and trained mod- els are available at https://github.com/ alphadl/RLFW-NAT . 1"}
{"introduction": "et al. , 2013 ) is a critical component in spoken dialog systems, which aims to understand user\u2019s queries. It typically includes two sub-tasks: intent detection and slot \ufb01lling ( Tur and De Mori , 2011 ). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature ( et al. , 2018 ; , 2018 ; , 2019b ; E et al. , 2019 ; , 2019 ; , 2021 ; Qin et al. , 2021b , c ) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can han- dle an utterance containing multiple intents, which is shown to be more practical in the real-world sce- nario, attracting increasing attention. To this end, \u2217 Corresponding author. (a) How far is Denver airport (b) How far is Denver airport O O O B-AN I-AN O O O B-AN I-AN Figure 1: (a) Autoregressive model generates outputs word by word from left-to-right direction. The gray color denotes the unseen information when model de- codes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name . ( 2013 ) and ( 2017 ) be- gin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot \ufb01lling task. Recently, haraiah and Narayanaswamy ( 2019 ) make the \ufb01rst attempt to propose a multi-task framework to joint model the multiple intent detection and slot \ufb01lling. Qin et al. ( 2020b ) further propose an adaptive inter- action framework (AGIF) to achieve \ufb01ne-grained multi-intent information integration for slot \ufb01lling, obtaining state-of-the-art performance. the existing multi-intent SLU joint models heav- ily rely on an autoregressive fashion, as shown in Figure 1 (a), leading to two issues: \u2022 . models make the generation of slot outputs must be done through the left-to-right pass, which cannot achieve parallelizable, leading to slow inference speed. arXiv:2106.01925v1  [cs.CL]  3 Jun 2021 \u2022 . Autoregressive models predict each word slot conditioned on the pre- viously generated slot information (from left- to-right), resulting in leaking the bidirectional context information. framework for joint multiple intent detection and slot \ufb01lling, with the goal of accelerating inference speed while achieving high accuracy, which is shown in Figure 1 (b). To this end, we propose a G lobal- L ocally G raph- I nteraction N etwork (GL- GIN) where the core module is a proposed lo- cal slot-aware graph layer and global intent-slot interaction layer, which achieves to generate in- tents and slots sequence simultaneously and non- autoregressively. In GL-GIN, a local slot-aware graph interaction layer where each slot hidden states connect with each other is proposed to ex- plicitly model slot dependency, in order to alle- viate uncoordinated slot problem (e.g., B-singer followed by I-song ) ( , 2020 ) due to the non-autoregressive fashion. A global intent-slot graph interaction layer is further introduced to per- form sentence-level intent-slot interaction. Un- like the prior works that only consider the token- level intent-slot interaction, the global graph is con- structed of all tokens with multiple intents, achiev- ing to generate slots sequence in parallel and speed up the decoding process. , 2018 ) and Mix- ATIS ( , 1990 ) show that our frame- work not only obtains state-of-the-art performance but also enables decoding in parallel. In addition, we explore the pre-trained model (i.e., Roberta ( Liu et al. , 2019c )) in our framework. be concluded as follows: (1) To the best of our knowledge, we make the \ufb01rst attempt to explore a non-autoregressive approach for joint multiple intent detection and slot \ufb01lling; (2) We propose a global-locally graph-interaction network, where the local graph is used to handle uncoordinated slots problem while a global graph is introduced to model sequence-level intent-slot interaction; (3) our framework not only achieves the state-of-the- art performance but also considerably speeds up the slot decoding (up to \u00d7 11 . 5 ); (4) Finally, we explore the pre-trained model in our framework. new state-of-the-art level. per is publicly available at https://github.com/ yizhen20133868/GL-GIN . 2", "contribution": "The main contributions of this paper are:\n\n1. The proposal of a non-autoregressive approach for joint multiple intent detection and slot filling in spoken dialog systems.\n2. The development of a Global-Locally Graph-Interaction Network (GL-GIN) that includes a local slot-aware graph layer and a global intent-slot interaction layer to generate intents and slots sequence simultaneously and non-autoregressively.\n3. The introduction of a local slot-aware graph interaction layer to explicitly model slot dependency and alleviate uncoordinated slot problems.\n4. The exploration of a pre-trained model (Roberta) in the proposed framework.\n5. The achievement of state-of-the-art performance and considerably faster slot decoding (up to \u00d7 11.5) compared to prior works.\n6. The availability of the paper's code on GitHub.", "title": "GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple", "words": 1013, "abstract": "Multi-intent SLU can handle multiple intents in an utterance, which has attracted increas- ing attention. However, the state-of-the-art joint models heavily rely on autoregressive ap- proaches, resulting in two issues: slow infer- ence speed and information leakage . In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot \ufb01ll- ing, achieving more fast and accurate. Specif- ically, we propose a G lobal- L ocally G raph I nteraction N etwork (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating unco- ordinated slots problem while a global intent- slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimen- tal results on two public datasets show that our framework achieves state-of-the-art perfor- mance while being 11.5 times faster. 1"}
{"introduction": "portant roles in spoken language understanding, especially for task-oriented dialogue system. For example, for an utterance like \u201c from Beijing to Seattle \u201d, intent detection works on sentence-level to indicate the task is about purchas- ing an air ticket, while the slot \ufb01lling focus on words-level to \ufb01gure out the departure and destina- tion of that ticket are \u201c Beijing \u201d and \u201c Seattle \u201d. separately, where ID was modeled as a classi\ufb01ca- tion task, while SF was regarded as a sequence labeling task. Due to the correlation between these two tasks, training them jointly could enhance each other. ( 2016 ) propose a joint model using bidirectional gated recurrent unit to learn the representation at each time step. Mean- while, a max-pooling layer is employed to capture the global features of a sentence for intent classi- \ufb01cation. ( 2016 ) cast the slot \ufb01lling task as a tag generation problem and introduce a recurrent neural network based encoder-decoder framework with attention mechanism to model it, meanwhile using the encoded vector to predict in- tent. ( 2018 ) and ( 2019 ) dig into the correlation between ID and SF deeper and modeled the relationship between them explic- itly. ( 2019 ) propagate the token-level intent results to the SF task, achieving signi\ufb01cant performance improvement. Brie\ufb02y summarized, most of the previous works heavily rely on autoregressive approaches, e.g., RNN based model or seq2seq architecture, to cap- ture the grammar structure in an utterance. And conditional random \ufb01eld (CRF) is a popular auxil- iary module for SF task as it considers the correla- tions between tags. Thus, several state-of-the-art works combine the autoregressive model and CRF to achieve the competitive performance, which therefore are set as our baseline methods. token dependencies among slot chunk is enough, and it is unnecessary to model the entire sequence dependency in autoregressive fashion, which leads to redundant computation and inevitable high la- tency. In this study, we cast these two tasks jointly as a non-autoregressive tag generation problem to get rid of unnecessary temporal dependencies. Partic- ularly, a Transformer ( , 2017 ) based architecture is adopted here to learn the represen- tations of an utterance in both sentence and word level simultaneously (Sec. \u00a7 2.1 ). The slots and in- tent labels are predicted independently and simulta- neously, achieving better decoding ef\ufb01ciency. We further introduce a two-pass re\ufb01ne mechanism (in \u00a7 2.2 ) to model boundary prediction of each slots explicitly, which also handle the uncoordi- nated slots problem ( e.g., I-song follows B-singer ) caused by conditional independence attribute. arXiv:2010.02693v2  [cs.CL]  31 Oct 2020 Figure 1: Illustration of SlotRe\ufb01ne, where the left and right part indicate the \ufb01rst and second iteration process respectively. In the \ufb01rst pass, wrong slot tagging results are predicted, as shown in the pink dotted box in the \ufb01gure, and the \u201c B-tags \u201d (beginning tag of a slot) are feeded as additional information with utterance for second iteration. The slot results in the green dotted box are re\ufb01ned results by second pass. Note that the initial tag embedding \u201cO\u201d added to each inputting position is designed for the two-pass mechanism(Sec.\u00a7 2.2 ). Figure 2: A example of uncoordinated slot tagging. show that our approach is signi\ufb01cantly and consis- tently superior to the existing models both in SF performance and ef\ufb01ciency (Sec. \u00a7 3 ). Our contribu- tions are as follows: \u2022 proach to model ID and SF tasks jointly, named SlotRe\ufb01ne 1 , achieving the state-of-the- art on ATIS dataset. \u2022 handle uncoordinated slots problem. Our anal- yses con\ufb01rm it is a better alternative than CRF in this task. \u2022 Our model infers nearly \u00d7 11 faster than exist- ing models ( \u00d7 13 for long sentences), indicat- ing that our model has great potential for the industry and academia. 2", "contribution": "The main contributions of this paper are:\n\n1. The introduction of a non-autoregressive approach called SlotRe\ufb01ne to jointly model intent detection and slot filling tasks, achieving state-of-the-art performance on the ATIS dataset.\n2. The introduction of a two-pass re\ufb01ne mechanism to explicitly model boundary prediction of each slot and handle uncoordinated slots problem.\n3. The demonstration that SlotRe\ufb01ne is a better alternative than CRF for handling uncoordinated slots problem.\n4. The achievement of significantly faster inference times compared to existing models, indicating potential for industry and academia.", "title": "SlotRe\ufb01ne: A Fast Non-Autoregressive Model forSlotRe\ufb01ne: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling", "words": 915, "abstract": "Slot \ufb01lling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRe\ufb01ne for joint intent detection and slot \ufb01lling. Be- sides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model. Experiments demonstrate that our model signi\ufb01cantly out- performs previous models in slot \ufb01lling task, while considerably speeding up the decoding (up to \u00d7 10.77). In-depth analyses show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots. 1"}
{"introduction": "chine translation (NAT) systems, which predict to- kens in the target language independently of each other conditioned on the source sentence, has been improving steadily in recent years ( , 2018 ; , 2019 ; , 2019 ). One common ingredient in getting non-autoregressive systems to perform well is to train them on a corpus of distilled translations ( , 2016 ). This distilled corpus consists of source sentences paired with the translations produced by a pretrained au- toregressive \u201cteacher\u201d system. translation systems on distilled corpora, we instead propose to train them to minimize the energy de- \ufb01ned by a pretrained autoregressive teacher model. \u2217 Work partly done at Toyota Technological Institute at Chicago and the University of Chicago. 1 Code is available at https://github.com/ lifu-tu/ENGINE lation systems as inference networks ( pel , 2018 , 2019 ; , 2019 ) trained to mini- mize the teacher\u2019s energy. This provides the non- autoregressive model with additional information related to the energy of the teacher, rather than just the approximate minimizers of the teacher\u2019s energy appearing in a distilled corpus. an energy function, the energy must be differen- tiable with respect to the inference network out- put. We describe several approaches for relax- ing the autoregressive teacher\u2019s energy to make it amenable to minimization with an inference network, and compare them empirically. We ex- periment with two non-autoregressive inference network architectures, one based on bidirectional RNNs and the other based on the transformer model of ( 2019 ). WMT 2016 RO-EN datasets, we show that train- ing to minimize the teacher\u2019s energy signi\ufb01cantly outperforms training with distilled outputs. Our approach, which we call ENGINE (ENerGy-based sults for non-autoregressive translation on these datasets, approaching the results of the autoregres- sive teachers. Our hope is that ENGINE will enable energy-based models to be applied more broadly for non-autoregressive generation in the future. 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a new approach called ENGINE (ENerGy-based INference for Non-autoregressive Machine Translation) for training non-autoregressive machine translation systems to minimize the energy defined by a pretrained autoregressive teacher model.\n\n2. Describing several approaches for relaxing the autoregressive teacher's energy to make it amenable to minimization with an inference network, and comparing them empirically.\n\n3. Experimentally showing that training to minimize the teacher's energy significantly outperforms training with distilled outputs on WMT 2016 RO-EN datasets.\n\n4. Demonstrating that the proposed approach, ENGINE, achieves competitive results for non-autoregressive translation on these datasets, approaching the results of the autoregressive teachers.\n\n5. Enabling energy-based models to be applied more broadly for non-autoregressive generation in the future.", "title": "ENGINE: Energy-Based Inference Networks for Lifu Tu Abstract Acknowledgments References", "words": 438, "abstract": "We propose to train a non-autoregressive ma- chine translation model to minimize the energy de\ufb01ned by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network ( Tu and Gimpel , 2018 ) trained to minimize the autoregressive teacher energy. This con- trasts with the popular approach of training a non-autoregressive model on a distilled cor- pus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based In- ference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, ap- proaching the performance of autoregressive models. 1 1"}
{"introduction": "( Bahdanau et al. , 2015 ) models decode tokens one- by-one, which ensure the robustness of the intrinsic language model but make the inference slow ( \u00a7 2.1 ). Recently, non-autoregressive (NAT) models gener- ate all outputs in parallel. It speeds up the inference, but at the cost of breaking the dependency between adjacent tokens, leading to worse performance than standard AT models (\u00a7 2.2 ). Knowledge distillation ( Hinton et al. , 2015 ) is a method that trains a student model to perform bet- ter by learning from a stronger teacher model. This method has been proved to be necessary for NAT models training in ( , 2020 ), and most NAT models are trained on distilled data generated by an AT teacher model to achieve competitive performance. This method of distilled data genera- tion is called sequence-level knowledge distillation ( Kim and Rush , 2016 ) (\u00a7 2.3 ). dent model, we call it Self-Distillation. the combination of the original data and the self distilled data to re-train the model itself is Self- Distillation Mixup (SDM) Training. ( Freitag et al. , 2017 ) has proved that SDM can improve the per- formance of AT models (\u00a7 2.4 ). egy to improve the NAT performance. A promis- ing attempt to address this issue is to improve the AT teacher model by using some strategies such as SDM Training. The problem is that, higher quality distilled data generated by a better AT teacher model may not improve the performance of NAT student models generally ( \u00a7 3.1 ). Another approach is to adopt SDM Training to NAT models directly. But the second problem occurs that, clas- sic SDM Training has no effect when NAT models are trained on AT-distilled data (\u00a7 3.2 ). named (SDMRT). In the stage of distilled data generation, we use Rerank Algorithm ( , 2021 ) to reduce the sity between AT and NAT models. In the stage of model training, we Fine-Tune the NAT models on the \ufb01ltered data to reduce the (\u00a7 4 ). arXiv:2112.11640v1  [cs.CL]  22 Dec 2021 Our contributions are as follows: \u2022 troduce an enhanced training strategy named SDMRT to signi\ufb01cantly improve translation quality of multiple NAT models (\u00a7 4.1 ; \u00a7 5.2 ). \u2022 NAT models trained on AT-distilled data. Modeling Diversity and Con\ufb01rmation Bias between the AT teacher model and the NAT student models (\u00a7 4.2 ; \u00a7 5.3 ; \u00a7 5.3 ). \u2022 data. Furthermore, we \ufb01nd our SDMRT can be regarded as an acceleration method to re- duce iterations for Iterative Re\ufb01nement NAT models (\u00a7 5.3 ; \u00a7 5.3 ; \u00a7 5.3 ). 2", "contribution": "The main contributions of this paper are:\n\n- Introducing an enhanced training strategy named SDMRT to significantly improve translation quality of multiple NAT models.\n- Addressing the issue of modeling diversity and confirmation bias between the AT teacher model and the NAT student models when training NAT models on AT-distilled data.\n- Finding that SDMRT can be regarded as an acceleration method to reduce iterations for Iterative Refinement NAT models.", "title": "Self-Distillation Mixup Training for Non-autoregressive Neural Machine", "words": 624, "abstract": "Recently, non-autoregressive (NAT) models predict outputs in parallel, achieving substan- tial improvements in generation speed com- pared to autoregressive (AT) models. While performing worse on raw data, most NAT mod- els are trained as student models on distilled data generated by AT teacher models, which is known as sequence-level Knowledge Distil- lation. An effective training strategy to im- prove the performance of AT models is Self- Distillation Mixup (SDM) Training, which pre-trains a model on raw data, generates dis- tilled data by the pre-trained model itself and \ufb01nally re-trains a model on the combination of raw data and distilled data. In this work, we aim to view SDM for NAT models, but \ufb01nd di- rectly adopting SDM to NAT models gains no improvements in terms of translation quality. Through careful analysis, we observe the in- validation is correlated to Modeling Diversity and Con\ufb01rmation Bias between the AT teacher model and the NAT student models. Based on these \ufb01ndings, we propose an enhanced strategy named SDMRT by adding two stages to classic SDM: one is Pre-Rerank on self- distilled data, the other is Fine-Tune on Fil- tered teacher-distilled data. Our results outper- form baselines by 0.6 \u223c 1.2 BLEU on multiple NAT models. As another bonus, for Iterative Re\ufb01nement NAT models, our methods can out- perform baselines within half iteration number, which means 2 \u00d7 acceleration. 1"}
{"introduction": "studied recently for ef\ufb01cient sequence genera- tion ( , 2021 ; , 2017 ). Different from classical Autoregressive (AR) approaches which sequentially decode output tokens ( et al. , 2019 ; Song et al. , 2019 ; Brown et al. , 2020b ; Zou et al. , 2021 ; He et al. , 2021 ), NAR approaches generate the sequence of tokens in parallel i.e. BANG ( , 2021 ), NAT ( , 2017 ) etc, to largely reduce the inference latency, which have been successfully applied in query generation, text summarization tasks ( , 2016 ; Narayan et al. , 2018 ; Rush et al. , 2015 ). cally, typical NAR models still signi\ufb01cantly under- perform AR models ( , 2021 ). Previous works analyze the issue of performance degrada- tion by NAR and attribute it to the multi-modality problem ( , 2016 ). modality problem in NAR is described as gen- erating target tokens from different possible an- swers and composing a chaotic confusing target sequence. It is not observed in AR models be- cause they would pick only one possible answer with step-by-step generation, with all previous gen- erated tokens as known information. To allevi- ate the multi-modality problem, sequence distil- lation ( , 2016 ; , 2017 ) is widely used to replace the original training tar- gets with the generated sequences by a well-trained AR model. Sequence distillation is analyzed to prove its ability to improve NAR performance by reducing the modality ( , 2019 ) and re- ducing the dependency between target sequence tokens ( , 2020 ). Besides sequence distil- lation, various techniques are proposed to improve arXiv:2205.11162v1  [cs.CL]  23 May 2022 the NAR generation including copy mechanism for translation ( , 2017 ), curriculum learning ( , 2020 ), glancing sampling ( , 2020 ), pre-training ( Qi et al. , 2021 ) etc. mixed distillation method. to instruct the NAR model to select one modal- ity to converge and focus on the samples with the same modality. At the beginning, NAR model will study all samples equally, then gradually select the easy samples with self-paced learning. We propose to use perplexity (PPL) to measure the modality- matching quality, and give rewards to the samples that agree with the converged modality. Secondly, we propose to generate soft labels from the BANG AR stream for teaching NAR stream. With the soft labels including rare words knowledge from origi- nal golden data rather than directly adding original data into training, it is less possible to hurt the NAR performance with increased modality problem. On the contrary, if we say the learned AR model regu- lates the data distribution to generalize a simpli\ufb01ed \ufb01tting function, instead of the hard outputs from AR models which are approximately sampled from beam search, directly predicted words distribution better describe the AR learned generation function. The AR teacher model is trained on original golden data but teaches the student NAR model soft la- bels with distilled data as contexts. Experimental results show that the proposed mixed distillation and self-paced learning signi\ufb01cantly improve NAR performance. rized as: 1. method to teach BANG NAR generation with soft labels knowledge from its AR knowledge with self-paced learning. 2. marization, question generation with obvious improvements. It is easy to deploy with sig- ni\ufb01cant performance improvements and no in\ufb02uence on inference latency. 3. cial tasks. It achieves signi\ufb01cantly perfor- mance improvement compared with BANG NAR. Compared with AR models, the pro- posed method meets the online requirement and also achieves comparable performance. 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a mixed distillation method to teach NAR generation with soft labels knowledge from its AR knowledge with self-paced learning.\n2. Demonstrating the effectiveness of the proposed method in improving NAR performance in various tasks such as text summarization and question generation.\n3. Showing that the proposed method achieves significant performance improvement compared with BANG NAR and meets the online requirement while achieving comparable performance with AR models.", "title": "A Self-Paced Mixed Distillation Method for Non-Autoregressive", "words": 831, "abstract": "Non-Autoregressive generation is a sequence generation paradigm, which removes the de- pendency between target tokens. It could ef\ufb01ciently reduce the text generation latency with parallel decoding in place of token- by-token sequential decoding. However, due to the known multi-modality problem, Non-Autoregressive (NAR) models signi\ufb01- cantly under-perform Auto-regressive (AR) models on various language generation tasks. Among the NAR models, BANG is the \ufb01rst large-scale pre-training model on En- glish un-labeled raw text corpus. It con- siders different generation paradigms as its pre-training tasks including Auto-regressive (AR), Non-Autoregressive (NAR), and semi- Non-Autoregressive (semi-NAR) information \ufb02ow with multi-stream strategy. It achieves state-of-the-art performance without any dis- tillation techniques. However, AR distillation has been shown to be a very effective solution for improving NAR performance. In this pa- per, we propose a novel self-paced mixed dis- tillation method to further improve the genera- tion quality of BANG. Firstly, we propose the mixed distillation strategy based on the AR stream knowledge. Secondly, we encourage the model to focus on the samples with the same modality by self-paced learning. The proposed self-paced mixed distillation algo- rithm improves the generation quality and has no in\ufb02uence on the inference latency. We carry out extensive experiments on summarization and question generation tasks to validate the effectiveness. To further illustrate the commer- cial value of our approach, we conduct exper- iments on three generation tasks in real-world advertisements applications. Experimental re- sults on commercial data show the effective- ness of the proposed model. Compared with BANG, it achieves signi\ufb01cant BLEU score im- provement. On the other hand, compared with Asia. \u2217 Work is done during internship at Microsoft Research \u2020 Corresponding Author. auto-regressive generation method, it achieves more than 7x speedup. We will make our code publicly available. 1"}
{"introduction": "State-of-the-art neural machine translation (NMT) systems use autoregressive decoding where the de- coder generates a target sentence word by word, and the generation of the latter words depends on previously generated ones ( , 2015 ; , 2017 ; , 2017 ). In- stead of sequential decoding as in the autoregres- sive translation (AT), non-autoregressive neural ma- chine translation (NAT) ( Gu et al. , 2018 ; Guo et al. , 2019 ; , 2019 ; , 2019 ; , 1 Our code is publicly available at https://github. com/xwgeng/RewriteNAT . Encoder x 1 x 2 x 3 x 4 Revisor y 1 y 4 y 5 encoder-decoder attention \u0302 y 2 \u0302 y 3 y 1 y 4 y 5 [MASK] [MASK] Locator Encoder x 1 x 2 x 3 x 4 Decoder y 5 encoder-decoder attention \u0302 y 2 \u0302 y 3 y 1 y 4 y 5 [MASK] Heuristic Rules \u0302 y 3 [MASK] \u0302 y 2 translation translation (a) Masked LM-based NAT Encoder x 1 x 2 x 3 x 4 Revisor y 1 y 4 y 5 encoder-decoder attention \u0302 y 2 \u0302 y 3 y 1 y 4 y 5 [MASK] [MASK] Locator Encoder x 1 x 2 x 3 x 4 Decoder y 5 encoder-decoder attention \u0302 y 2 \u0302 y 3 y 1 y 4 y 5 [MASK] Heuristic Rules \u0302 y 3 [MASK] \u0302 y 2 translation translation (b) RewriteNAT Figure 1: Illustration of the difference in masking words between (a) conventional masked LM-based NAT ( Ghazvininejad et al. , 2019 ) and (b) our proposed R EWRITE NAT. Instead of using inef\ufb01cient heuristic rules which perhaps mask correct words in some case ( e.g., y 1 y 4 ), R EWRITE NAT utilizes an additional loca- tor module to learn to explicitly distinguish erroneous translation pieces ( e.g., \u02c6 y 2 \u02c6 y 3 ), annotated as special sym- bol ( i.e., [MASK] ). 2019 ; , 2020a ; , 2020 ; , 2021a , b ) generates the whole target sentence simultaneously. To enable paral- lel decoding, NAT imposes a conditional indepen- dence assumption among words in target sentences, yielding signi\ufb01cantly faster inference speed than AT. However, since intrinsic dependencies within target sentence are omitted, NAT suffers from se- vere inconsistency problem ( , 2019 ), leading to inferior translation quality, especially when capturing highly multimodal distribution of target translations ( Gu et al. , 2018 ). iterative decoding ( , 2018 ; jad et al. , 2019 ; , 2019 ; , 2020b ; Ghazvininejad et al. , 2020b ) is proposed to improve NAT by repeatedly re\ufb01ning previously generated translation. Instead of enforcing NAT to generate accurate translation by one-pass decoding, these approaches are expected to revise incorrect transla- tion pieces through several re\ufb01nements ( , 2017 ; Zhang et al. , 2018 ; Geng et al. , 2018 ). With the introduction of iterative decoding, NAT further 3298 boosts translation quality, bridging performance gap between NAT and AT models. However, existing iterative NAT models expose the weakness in distinguishing the erroneous words. is mask-predict algorithm ( , 2019 ; Guo et al. , 2020b ), which employs inef\ufb01cient heuristic rules to roughly choose the least con\ufb01dent words as the erroneous. In some case, mask-predict may mistake to rewrite correct words while main- tain erroneous ones, acting as noises to make a negative impact on subsequent iterations. Without explicitly classifying translated words into wrong or right, the translations decode in constant number of iterations, hindering the further improvement of inference speed. Besides, decoder inputs of prevail- ing iterative NAT models ( , 2020 ; et al. , 2020b ) almost come from the ground-truth during training, while target sentences generated at different re\ufb01nement steps are taken as decoder inputs in inference, creating a discrepancy that can hurt performance. In this paper, we propose an architecture named R EWRITE NAT, which explicitly learns to rewrite erroneous translation pieces. Speci\ufb01cally, we in- troduce a locator module to locate incorrect words within previously generated translation. The lo- cated words will be masked out and revised by the revisor module in subsequent re\ufb01nement. We frame learning to rewrite, comprised of two steps: locate and revise , as an iterative training procedure, where locate and revise operations are supervised by com- paring the generated translation with the ground- truth. Towards keeping the consistency with itera- tive decoding, iterative training is utilized to further improve the training procedure. Experimental re- sults on several typical machine translation datasets demonstrate that R EWRITE NAT achieves consis- tent improvement over iterative decoding baselines, but with substantially less decoding time. Further analysis show that R EWRITE NAT prefers to gener- ate the \u201c easy \u201d words at the early decoding iteration, and leaves the more complicated choice later. 2", "contribution": "The main contribution of this paper is the proposal of a new architecture called R EWRITE NAT for non-autoregressive neural machine translation (NAT) that explicitly learns to rewrite erroneous translation pieces. This is achieved through the introduction of a locator module that identifies incorrect words within previously generated translations, which are then masked out and revised by the revisor module in subsequent refinements. The learning to rewrite process is framed as an iterative training procedure supervised by comparing the generated translation with the ground truth. Experimental results on several machine translation datasets demonstrate that R EWRITE NAT achieves consistent improvement over iterative decoding baselines, but with substantially less decoding time. Further analysis shows that R EWRITE NAT prefers to generate the \"easy\" words at the early decoding iteration and leaves the more complicated choices for later.", "title": "Learning to Rewrite for Non-Autoregressive Neural Machine Translation Xinwei Geng Abstract Acknowledgements References", "words": 1178, "abstract": "Non-autoregressive neural machine transla- tion, which decomposes the dependence on previous target tokens from the inputs of the decoder, has achieved impressive inference speedup but at the cost of inferior accuracy. Previous works employ iterative decoding to improve the translation by applying multiple re\ufb01nement iterations. However, a serious drawback is that these approaches expose the serious weakness in recognizing the erroneous translation pieces. In this paper, we propose an architecture named R EWRITE NAT to explic- itly learn to rewrite the erroneous translation pieces. Speci\ufb01cally, R EWRITE NAT utilizes a locator module to locate the erroneous ones, which are then revised into the correct ones by a revisor module. Towards keeping the consis- tency of data distribution with iterative decod- ing, an iterative training strategy is employed to further improve the capacity of rewriting. Extensive experiments conducted on several widely-used benchmarks show that R EWRITE - NAT can achieve better performance while sig- ni\ufb01cantly reducing decoding time, compared with previous iterative decoding strategies. In particular, R EWRITE NAT can obtain compet- itive results with autoregressive translation on WMT14 En \u2194 De, En \u2192 Fr and WMT16 Ro \u2192 En translation benchmarks 1 . 1"}
{"introduction": "The neural network based encoder-decoder framework has achieved very promising performance for machine trans- lation and different network architectures have been pro- posed, including RNNs (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014; Cho et al. 2014a; Wu et al. 2016), CNNs (Gehring et al. 2017), and self-attention based Transformer (Vaswani et al. 2017). All those models translate a source sentence in an autoregressive manner, i.e., they generate a target sentence word by word from left to right (Wu et al. 2018) and the generation of t -th token y t depends on previously generated tokens y 1: t \u2212 1 : y t = D ( y 1: t \u2212 1 , E ( x )) , (1) where E ( \u00b7 ) and D ( \u00b7 ) denote the encoder and decoder of the model respectively, x is the source sentence and E ( x ) is the output of the encoder, i.e., the set of hidden representations in the top layer of the encoder. \u2217 The work was done when the \ufb01rst author was an intern at Mi- crosoft Research Asia. Copyright c \u20dd 2019, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. Since AT models generate target tokens sequentially, the inference speed becomes a bottleneck for real-world trans- lation systems, in which fast response and low latency are expected. To speed up the inference of machine translation, non-autoregressive models (Gu et al. 2017) have been pro- posed, which generate all target tokens independently and simultaneously. Instead of using previously generated to- kens as in AT models, NAT models take other global sig- nals derived from the source sentence as input. Speci\ufb01cally, Non-AutoRegressive Transformer (NART) (Gu et al. 2017) takes a copy of source sentence x as the decoder input, and the copy process is guided by fertilities (Brown et al. 1993) which represents how many times each source token will be copied; after that all target tokens are simultaneously pre- dicted: y t = D (\u02c6 x, E ( x )) , (2) where \u02c6 x = (\u02c6 x 1 , ..., \u02c6 x T y ) is the copied source sentence and T y is the length of the target sentence y . While NAT models signi\ufb01cantly reduce the inference la- tency, they suffer from accuracy degradation compared with their autoregressive counterparts. We notice that the encoder of AT models and that of NAT models are the same; the differences lie in the decoder. In AT models, the genera- tion of the t -th token y t is conditioned on previously gener- ated tokens y 1: t \u2212 1 , which provides strong target side context information. In contrast, as NART models generate tokens in parallel, there is no such target-side information avail- able. Although the fertilities are learned to cover target-side information in NART (Gu et al. 2017), such information contained in the copied source tokens \u02c6 x guided by fertili- ties is indirect and weak because the copied tokens are still in the domain of source language, while the inputs of the decoder of AT models are target-side tokens y 1: t \u2212 1 . Con- sequently, the decoder of a NAT model has to handle the translation task conditioned on less and weaker information compared with its AT counterpart, thus leading to inferior accuracy. As veri\ufb01ed by our study (see Figure 2 and Ta- ble 3), NART performs poorly for long sentences, which need stronger target-side conditional information for correct translation than short sentences. In this paper, we aim to enhance the decoder inputs of NAT models so as to reduce the dif\ufb01culty of the task that the decoder needs to handle. Our basic idea is to directly feed target-side tokens as the inputs of the decoder. We pro- arXiv:1812.09664v1  [cs.CL]  23 Dec 2018 pose two concrete methods to generate the decoder input \u02c6 y = (\u02c6 y 1 , ..., \u02c6 y T y ) which contains coarse target-side informa- tion. The \ufb01rst one is based on a phrase table, and explicitly translates source tokens into target-side tokens through such a pre-trained phrase table. The second one linearly maps the embeddings of source tokens into the target-side embedding space and then the mapped embeddings are fed into the de- coder. The mapping is learned in an end-to-end manner by minimizing the L 2 distance of the mapped source and target embeddings in the sentence level as well as the adversary loss between the mapped source embeddings and target em- beddings in the word level. With target-side information as inputs, the decoder works as follows: y t = D (\u02c6 y, E ( x )) , (3) where \u02c6 y is the enhanced decoder input provided by our methods. The decoder now can generate all y t \u2019s in parallel conditioned on the global information \u02c6 y , which is more close to the target tokens y 1: t \u2212 1 as in the AT model. In this way, the dif\ufb01culty of the task for the decoder is largely reduced. We conduct experiments on three tasks to verify the proposed method. On WMT14 English-German, WMT16 English-Romanian and IWSLT14 German-English trans- lation tasks, our model outperforms all compared non- autoregressive baseline models. Speci\ufb01cally, we obtain BLEU scores of 24 . 28 and 34 . 51 which outperform the non- autoregressive baseline ( 19 . 17 and 29 . 79 reported in Gu et al. (2017)) on WMT14 En-De and WMT16 En-Ro tasks. 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing two methods to enhance the decoder inputs of non-autoregressive models for machine translation, which directly feed target-side tokens as inputs to the decoder, reducing the difficulty of the task that the decoder needs to handle.\n\n2. Conducting experiments on three tasks (WMT14 English-German, WMT16 English-Romanian, and IWSLT14 German-English) to verify the proposed method, and achieving better BLEU scores than non-autoregressive baseline models.", "title": "Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input", "words": 1280, "abstract": "Abstract Non-autoregressive translation (NAT) models, which remove the dependence on previous target tokens from the inputs of the decoder, achieve signi\ufb01cantly inference speedup but at the cost of inferior accuracy compared to autoregressive transla- tion (AT) models. Previous work shows that the quality of the inputs of the decoder is important and largely impacts the model accuracy. In this paper, we propose two methods to enhance the decoder inputs so as to improve NAT mod- els. The \ufb01rst one directly leverages a phrase table generated by conventional SMT approaches to translate source tokens to target tokens, which are then fed into the decoder as in- puts. The second one transforms source-side word embed- dings to target-side word embeddings through sentence-level alignment and word-level adversary learning, and then feeds the transformed word embeddings into the decoder as inputs. Experimental results show our method largely outperforms the NAT baseline (Gu et al. 2017) by 5 . 11 BLEU scores on WMT14 English-German task and 4 . 72 BLEU scores on WMT16 English-Romanian task. 1"}
{"introduction": "(NAT) ( , 2018 ) takes advantage of the parallel architecture of transformer ( Vaswani et al. , 2017 ) to alleviate the translation latency issue in neural machine translation (NMT), achieving sig- nificant speed-up. Yet it suffers from the multi- modality problem, where a target token could be a result of different possible translations. Word order errors are often resulted as compared to the autore- gressive counterparts ( Du et al. , 2021 ), arising from the lack of dependency amongst target tokens in NAT models. achieve comparable performance to autoregres- sive models. This can be attributed to various ap- proaches that reduce the dependency in handling word order errors via word alignment mechanisms ( Gu and Kong , 2021 ). In particular, latent variables and alignments have been adopted for implicitly modelling the dependencies among the target to- kens ( , 2021 ). While the latent align- ment approach assumes monotonic alignment be- tween the source and target language pair when handling token shifts in the output space ( , 2021 ), explicit modality reduction methods ( , 2020 ; , 2020 ; , 2021 ; , 2021 ) on the other hand sought to directly align the source and target language pair. Despite some previous work being sub-optimal, re- cent work in this direction achieves state-of-the-art (sota) results rivaling that of implicit dependency modeling methods. in parallel sentences of source and target languages typically involves fertility prediction and token re- ordering prediction. In this paper, we focus on the latter and argue that improving the reordering performance can contribute greatly towards the per- formance of NAT models. With the sole excep- tion of Shu et al. , 2020 , architectural design of the aforementioned NAT models includes a reorder- ing sub-module as a key component. We therefore set forward to review in detail the capabilities of the various reordering mechanisms proposed in the NAT models. We then propose a novel way to achieve the reordering prediction by learning a non- autoregressive language model (NALM) based on transformer with Viterbi decoding ( , 1967 ) combined. tracted from the various NAT models and variants of our proposed NALM using the PTB dataset ( , 1993 ) where sentences with words permuted in different ways are expected to have their ordering recovered. In particular, we adopt 2327 different degrees of permutation to mimic various levels of monotonicity (or reordering difficulty) between the source and target sentences. Our ex- perimental results show that the proposed NALM achieves significant and consistent improvement compared to the reordering sub-modules extracted from explicit modality reductionist NAT models in all word permutation settings. Our experiment also advances the sota performance of the word reorder- ing task in low beam setting and achieves compara- ble performance with autoregressive models even in high beam setting (b=64) while maintaining a constant time complexity. 2", "contribution": "The main contributions of this paper are:\n\n1. A detailed review of the capabilities of various reordering mechanisms proposed in non-autoregressive transformer (NAT) models.\n2. Proposal of a novel way to achieve reordering prediction by learning a non-autoregressive language model (NALM) based on transformer with Viterbi decoding.\n3. Experimental results showing that the proposed NALM achieves significant and consistent improvement compared to the reordering sub-modules extracted from explicit modality reductionist NAT models in all word permutation settings.\n4. Advancement of the state-of-the-art performance of the word reordering task in low beam setting and achieving comparable performance with autoregressive models even in high beam setting (b=64) while maintaining a constant time complexity.", "title": "Assessing Non-autoregressive Alignment in Neural Machine Translation via Tse Chun Hin Ester S.M. Leung William K. Cheung Abstract References", "words": 613, "abstract": "Recent work on non-autoregressive neural ma- chine translation (NAT) that leverages align- ment information to explicitly reduce the modality of target distribution has reported comparable performance with counterparts that tackle multi-modality problem by implicitly modeling dependencies. Effectiveness in han- dling alignment is vital for models that fol- low this approach, where a token reordering mechanism is typically involved and plays a vital role. We review the reordering capability of the respective mechanisms in recent NAT models, and our experimental results show that their performance is sub-optimal. We propose to learn a non-autoregressive language model (NALM) based on transformer which can be combined with Viterbi decoding to achieve better reordering performance. We evaluate the proposed NALM using the PTB dataset where sentences with words permuted in dif- ferent ways are expected to have their order- ing recovered. Our empirical results show that the proposed method can outperform the state- of-the-art reordering mechanisms under dif- ferent word permutation settings, with a 2-27 BLEU improvement, suggesting high potential for word alignment in NAT. 1"}
{"introduction": "Non-autoregressive machine translation models can signif-\nicantly improve decoding speed by predicting every word\nin parallel (Gu et al., 2018; Libovick\u00b4y & Helcl, 2018). This\nadvantage comes at a cost to performance since modeling\nword order is trickier when the model cannot condition on\nits previous predictions. A range of semi-autoregressive\nmodels (Lee et al., 2018; Stern et al., 2019; Gu et al., 2019;\nGhazvininejad et al., 2019) have shown there is a speed-\naccuracy tradeoff that can be optimized with limited forms\nof autoregression. However, increasing performance of the\npurely non-autoregressive models without sacri\ufb01cing de-\ncoding speed remains an open challenge. In this paper, we\npresent a new training loss for non-autoregressive machine\ntranslation that softens the penalty for word order errors, and\nsigni\ufb01cantly improves performance with no modi\ufb01cation to\nthe model or to the decoding algorithm.\n\nExisting models\nnon-\nautoregressive) are typically trained with cross entropy loss.\n\nautoregressive\n\n(both\n\nand\n\n1Facebook AI Research.\nGhazvininejad <ghazvini@fb.com>.\n\nCorrespondence to: Marjan\n\nTarget Y\n\nit\n\ntastes\n\npretty\n\ngood\n\nModel\nPredictions\nP (Top 5)\n\nbut\nhowever\nfor\nand\nthough\n\nit\nthat\nthis\nfor\nthe\n\ntastes\nmakes\nlooks\ntaste\nfeels\n\ndelicious\ngood\ntasty\n\ufb01ne\nexquisite\n\nthough\n\n.\n,\nso\nthough\n!\n\nFigure 1. The model predictions are quite similar to the target, but\nmisaligned by one token. The \ufb01rst and second target tokens (it\ntastes) are predicted in the second and third positions, respectively,\nleaving only the predictions in the fourth and \ufb01fth positions aligned\nwith the target. The cross-entropy loss will heavily penalize the\npredictions in the \ufb01rst, second, and third positions.\n\nCross entropy is a strict loss function, where a penalty is\nincurred for every word that is predicted out of position,\neven for output sequences with small edit distances (see\nFigure 1). Autoregressive models learn to avoid such\npenalties, since words are generated conditioned on the\nsentence pre\ufb01x. However, non-autoregressive models do\nnot know the exact sentence pre\ufb01x, and should (intuitively)\nfocus more on root errors (e.g. a missing word) while\nallowing more partial credit for cascading errors (the right\nword in the wrong place).\n\nTo achieve this more relaxed loss, we introduce aligned\ncross entropy (AXE), a new objective function that com-\nputes the cross entropy loss based on an alignment between\nthe sequence of token labels and the sequence of token dis-\ntribution predictions. AXE uses dynamic programming to\n\ufb01nd the monotonic alignment that minimizes the cross en-\ntropy loss. It provides non-autoregressive models with a\nmore accurate training signal by ignoring absolute positions\nand focusing on relative order and lexical matching. We\nef\ufb01ciently implement AXE via matrix operations, and use\nit to train conditional masked language models (CMLM;\nGhazvininejad et al., 2019) for machine translation. AXE\nonly slightly increases training time compared to cross en-\ntropy, and requires no changes to parallel argmax decoding.\n\nExtensive experiments on machine translation benchmarks\ndemonstrate that AXE substantially boosts the performance\nIn\nof CMLMs, while having the same decoding speed.\nWMT\u201914 EN-DE, training CMLMs with AXE (instead of\nthe regular cross entropy loss) increases performance by\n5 BLEU points; we observe similar trends in WMT\u201916\n\n \n \n \n \n \n \n\fAligned Cross Entropy for Non-Autoregressive Machine Translation\n\nAlgorithm 1 Aligned Cross Entropy\nInput: tokens Y , predictions P\nA0,0 = 0\nfor i = 1 to n do\n\nAi,0 = Ai\u22121,0 \u2212 \u03b4 \u00b7 log P1(Yi)\n\nend for\nfor j = 1 to m do\n\nA0,j = A0,j\u22121 \u2212 log Pj(\u03b5)\n\nend for\nfor i = 1 to n do\n\nfor j = 1 to m do\n\nalign = Ai\u22121,j\u22121 \u2212 log Pj(Yi)\nskip prediction = Ai,j\u22121 \u2212 log Pj(\u03b5)\nskip target = Ai\u22121,j \u2212 \u03b4 \u00b7 log Pj(Yi)\nAi,j = min{align, skip prediction, skip target}\n\nend for\n\nend for\nreturn An,m\n\nEN-RO and WMT\u201917 EN-ZH. Moreover, AXE CMLMs\nsigni\ufb01cantly outperform state-of-the-art non-autoregressive\nmodels, such as FlowSeq (Ma et al., 2019), as well as the\nrecent CRF-based semi-autoregressive model with bigram\nLM decoding (Sun et al., 2019). Our detailed analysis sug-\ngests that training with AXE makes models more con\ufb01-\ndent in their predictions, thus reducing multimodality, and\nalleviating a key problem in non-autoregressive machine\ntranslation.", "contribution": "The main contribution of this paper is the introduction of a new training loss for non-autoregressive machine translation called aligned cross entropy (AXE), which softens the penalty for word order errors and significantly improves performance with no modification to the model or decoding algorithm. AXE provides non-autoregressive models with a more accurate training signal by ignoring absolute positions and focusing on relative order and lexical matching. The authors efficiently implement AXE via matrix operations and use it to train conditional masked language models (CMLM) for machine translation. Extensive experiments on machine translation benchmarks demonstrate that AXE substantially boosts the performance of CMLMs, while having the same decoding speed. AXE CMLMs significantly outperform state-of-the-art non-autoregressive models and alleviate a key problem in non-autoregressive machine translation by reducing multimodality.", "title": "Aligned Cross Entropy for Non-Autoregressive Machine Translation", "words": 1147, "abstract": "Non-autoregressive machine translation models\nsigni\ufb01cantly speed up decoding by allowing for\nparallel prediction of the entire target sequence.\nHowever, modeling word order is more challeng-\ning due to the lack of autoregressive factors in\nthe model. This dif\ufb01cultly is compounded dur-\ning training with cross entropy loss, which can\nhighly penalize small shifts in word order. In this\npaper, we propose aligned cross entropy (AXE)\nas an alternative loss function for training of non-\nautoregressive models. AXE uses a differentiable\ndynamic program to assign loss based on the\nbest possible monotonic alignment between tar-\nget tokens and model predictions. AXE-based\ntraining of conditional masked language models\n(CMLMs) substantially improves performance on\nmajor WMT benchmarks, while setting a new\nstate of the art for non-autoregressive models."}
{"introduction": "Conditional neural sequence modeling has be- come a de facto standard in a variety of tasks (see, e.g., Cho et al. , 2015 , and references therein). Much of this recent success is built on top of au- toregressive sequence models in which the proba- bility of a target sequence is factorized as a prod- uct of conditional probabilities of next symbols given all the preceding ones. Despite its success, neural autoregressive modeling has its weakness in decoding, i.e., \ufb01nding the most likely sequence. Because of intractability, we must resort to sub- optimal approximate decoding, and due to its se- quential nature, decoding cannot be easily paral- lelized and results in a large latency (see, e.g., Cho , 2016 ). This has motivated the recent investiga- tion into non-autoregressive neural sequence mod- eling by Gu et al. ( 2017 ) in the context of machine translation and Oord et al. ( 2017 ) in the context of speech synthesis. In this paper, we propose a non-autoregressive neural sequence model based on iterative re\ufb01ne- ment, which is generally applicable to any se- quence generation task beyond machine transla- tion. The proposed model can be viewed as both \u21e4 Equal Contribution a latent variable model and a conditional denois- ing autoencoder. We thus propose a learning algo- rithm that is hybrid of lowerbound maximization and reconstruction error minimization. We further design an iterative inference strategy with an adap- tive number of steps to minimize the generation latency without sacri\ufb01cing the generation quality. We extensively evaluate the proposed condi- tional non-autoregressive sequence model and compare it against the autoregressive counterpart, using the state-of-the-art Transformer ( Vaswani et al. , 2017 ), on machine translation and im- age caption generation. In the case of ma- chine translation, the proposed deterministic non- autoregressive models are able to decode approx- imately 2 \u2212 3 \u21e5 faster than beam search from the autoregressive counterparts on both GPU and CPU, while maintaining 90-95% of translation quality on IWSLT\u201916 En $ De, WMT\u201916 En $ Ro and WMT\u201914 En $ De. On image caption genera- tion, we observe approximately 3 \u21e5 and 5 \u21e5 faster decoding on GPU and CPU, respectively, while maintaining 85% of caption quality. 1 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a non-autoregressive neural sequence model based on iterative refinement that is generally applicable to any sequence generation task beyond machine translation.\n2. Designing a learning algorithm that is a hybrid of lower bound maximization and reconstruction error minimization.\n3. Designing an iterative inference strategy with an adaptive number of steps to minimize the generation latency without sacrificing the generation quality.\n4. Extensively evaluating the proposed conditional non-autoregressive sequence model and comparing it against the autoregressive counterpart using the state-of-the-art Transformer on machine translation and image caption generation.\n5. Observing approximately 2-3x faster decoding on GPU and CPU for machine translation and approximately 3-5x faster decoding on GPU and CPU for image caption generation while maintaining a high level of quality.", "title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Re\ufb01nement", "words": 526, "abstract": "We propose a conditional non-autoregressive neural sequence model based on iterative re- \ufb01nement. The proposed model is designed based on the principles of latent variable mod- els and denoising autoencoders, and is gen- erally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En $ De and En $ Ro) and image caption generation, and observe that it signi\ufb01cantly speeds up decod- ing while maintaining the generation quality comparable to the autoregressive counterpart. 1"}
{"introduction": "gist of the input. It is an established setting of summarization, and has extensive real-world ap- plications such as generating news headlines [ 26 , 30 , 8 ] and being a key component of document summarization [ 40 , 21 ]. In previous work, researchers have developed various approaches to improve the ROUGE score, which is the main evaluation metric for summarization [ 17 ], whereas controlling the summary length has not drawn much attention. required by real-world applications [ 20 ]. Moreover, the ROUGE score is found to be sensitive to the summary length [ 33 , 32 ], and summarization systems can achieve higher scores by simply generating longer output. Previous work mainly addresses length control in the word level, i.e., restricting the summary length by a pre-de\ufb01ned number of words [33, 18]. different setting from previous work. In other words, we restrict the summary length by the number of characters, such as letters, punctuation marks, and whitespaces. We observe that this is a more realistic setting in real-world applications than word-level length control. For example, the headline shown in a mobile app or web page is constrained by the screen width (roughly speaking, the number of characters), rather than the number of words. Likewise, a commercial LED display allows a certain number of characters; ideally, the text shown should \ufb01t the character-level budget, or otherwise it will be scrolling, making it dif\ufb01cult to read. However, the character-level constraint is unlikely to be satis\ufb01ed if we only perform word-level control, despite the positive correlation between word and character lengths. Moreover, character-level length control is a common evaluation setting for summarization systems. For example, a sub-task in the DUC evaluation requires the maximum target length to be 75 bytes [ 27 ]. Such an important setting, unfortunately, is not adequately addressed 1 Our code, model, and output are released at: https://github.com/MANGA-UOFA/NACC 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2205.14522v2  [cs.CL]  16 Oct 2022 in previous summarization studies. Our work largely bridges the gap between the methodological summarization research and its evaluation. We further observe that controlling the summary length by characters cannot be easily addressed by previous approaches. For example, truncating is able to explicitly control the length, but the resulting summary is incomplete; Takase and Okazaki [ 36 ] feed length embeddings into the model as input, but such an approach cannot control the summary length in an explicit manner; and Schumann et al. [ 33 ] perform constrained discrete optimization by selecting a certain number of words from the source text as the output, but their generated summaries may vary to a large extent in terms of the number of characters. length Control. We adopt a non-autoregressive approach because it generates all tokens in parallel and is much faster than autoregressive models. More importantly, we observe that non-autoregressive models predict the output words independently. Such predicted probabilities are thus local, which provides us with the unique opportunity to design a dynamic programming algorithm to constrain the summary length. Speci\ufb01cally, we formulate length control as a knapsack-like problem, where the weight is the number of characters in a token and the value is the predicted probability of the token. In this way, we are able to explicitly control the summary length at the character level, while retaining the completeness of the output text. 10 ] and DUC2004 [ 27 ] datasets in two settings: supervised and unsupervised. In the latter setting, NACC learns from the pseudo-reference given by an unsupervised word-extraction method based on discrete search [ 33 ]. summarization under various target lengths in both settings; NACC even outperforms autoregressive 37 ] in the unsupervised setting, where the input and output have stronger correspon- dence. These all con\ufb01rm the effectiveness of our length-control algorithm. Regarding inference ef\ufb01ciency, we show that non-autoregressive models without length control are 10 times faster than autoregressive ones; even with our length-control dynamic programming, NACC is still several times more ef\ufb01cient. Further, our NACC is capable of length-transfer generation, i.e., generating summaries of different lengths from the training targets. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing a character-level length control approach for summarization, which is more realistic for real-world applications than word-level control.\n2. Developing a non-autoregressive approach for length-controlled summarization, which generates all tokens in parallel and is much faster than autoregressive models.\n3. Formulating length control as a knapsack-like problem, where the weight is the number of characters in a token and the value is the predicted probability of the token.\n4. Evaluating the proposed approach on supervised and unsupervised settings using the CNN/Daily Mail and DUC2004 datasets, and showing that it outperforms previous methods in terms of ROUGE score and inference efficiency.\n5. Demonstrating the capability of the proposed approach for length-transfer generation, i.e., generating summaries of different lengths from the training targets.", "title": "A Character-Level Length-Control Algorithm for Non-Autoregressive Sentence Summarization", "words": 919, "abstract": "Sentence summarization aims at compressing a long sentence into a short one that keeps the main gist, and has extensive real-world applications such as headline generation. In previous work, researchers have developed various approaches to improve the ROUGE score, which is the main evaluation metric for summarization, whereas controlling the summary length has not drawn much attention. In our work, we address a new problem of explicit character-level length control for summariza- tion, and propose a dynamic programming algorithm based on the Connectionist achieves higher ROUGE scores but also yields more complete sentences. 1 1"}
{"introduction": "\nState-of-the-art neural machine translation systems use au-\ntoregressive decoding where words are predicted one-by-\none conditioned on all previous words (Bahdanau et al.,\n2015; Vaswani et al., 2017). Non-autoregressive machine\ntranslation (NAT, Gu et al., 2018), on the other hand, gen-\nerates all words in one shot and speeds up decoding at the\nexpense of performance drop. Parallel decoding results in\n\n1Paul G. Allen School of Computer Science & Engi-\nneering, University of Washington. Work done at Facebook\nAI. 2Facebook AI. Correspondence to:\nJungo Kasai <jka-\nsai@cs.washington.edu>.\n\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\n\nconditional independence and prevents the model from prop-\nerly capturing the highly multimodal distribution of target\ntranslations (Gu et al., 2018). One way to remedy this fun-\ndamental problem is to re\ufb01ne model output iteratively (Lee\net al., 2018; Ghazvininejad et al., 2019). This work pursues\nthis iterative approach to non-autoregressive translation.1\n\nIn this work, we propose a transformer-based architecture\nwith attention masking, which we call Disentangled Context\n(DisCo) transformer, and use it for non-autoregressive de-\ncoding. Speci\ufb01cally, our DisCo transformer predicts every\nword in a sentence conditioned on an arbitrary subset of\nthe rest of the words. Unlike the masked language models\n(Devlin et al., 2019; Ghazvininejad et al., 2019) where the\nmodel only predicts the masked words, the DisCo trans-\nformer can predict all words simultaneously, leading to\nfaster inference as well as a substantial performance gain\nwhen training data are relatively large.\n\nWe also introduce a new inference algorithm for iterative\nparallel decoding, parallel easy-\ufb01rst, where each word is\npredicted by attending to the words that the model is more\ncon\ufb01dent about. This decoding algorithm allows for predict-\ning all tokens with different contexts in each iteration and\nterminates when the output prediction converges, contrast-\ning with the constant number of iterations (Ghazvininejad\net al., 2019). Indeed, we will show in a later section that\nthis method substantially reduces the number of required\niterations without loss in performance.\n\nOur extensive empirical evaluations on 7 translation direc-\ntions from standard WMT benchmarks show that our ap-\nproach achieves competitive performance to state-of-the-art\nnon-autoregressive and autoregressive machine translation\nwhile signi\ufb01cantly reducing decoding time on average.", "contribution": "The paper proposes a transformer-based architecture called Disentangled Context (DisCo) transformer for non-autoregressive decoding in machine translation. The DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words, allowing for faster inference and substantial performance gain. The paper also introduces a new inference algorithm for iterative parallel decoding, parallel easy-first, which reduces the number of required iterations without loss in performance. The extensive empirical evaluations on 7 translation directions from standard WMT benchmarks show that the proposed approach achieves competitive performance to state-of-the-art non-autoregressive and autoregressive machine translation while significantly reducing decoding time on average.", "title": "Non-autoregressive Machine Translation with Disentangled Context", "words": 600, "abstract": "\nState-of-the-art neural machine translation mod-\nels generate a translation from left to right and\nevery step is conditioned on the previously gen-\nerated tokens. The sequential nature of this\ngeneration process causes fundamental latency\nin inference since we cannot generate multiple\ntokens in each sentence in parallel. We pro-\npose an attention-masking based model, called\nDisentangled Context (DisCo) transformer, that\nsimultaneously generates all tokens given differ-\nent contexts. The DisCo transformer is trained\nto predict every output token given an arbitrary\nsubset of the other reference tokens. We also de-\nvelop the parallel easy-\ufb01rst inference algorithm,\nwhich iteratively re\ufb01nes every token in parallel\nand reduces the number of required iterations.\nOur extensive experiments on 7 translation di-\nrections with varying data sizes demonstrate that\nour model achieves competitive, if not better, per-\nformance compared to the state of the art in non-\nautoregressive machine translation while signif-\nicantly reducing decoding time on average. Our\ncode is available at https://github.com/\nfacebookresearch/DisCo."}
{"introduction": "(NAT) has been greatly advanced in recent years ( , 2022 ). NAT takes advantage from par- allel decoding to generate multiple tokens simulta- neously and speed up inference. This is often at the cost of a loss in translation quality when compared to autoregressive (AR) models ( , 2018a ). iterative re\ufb01nement ( , 2019 ; , 2019 ; , 2020 ) and on con- nectionist temporal classi\ufb01cation ( , 2018 ; , 2021 ) are now reporting BLEU scores similar to strong AR baselines. chine translation (MT) task, where the decoder starts from scratch, with the exception of et al. ( 2020 ); ( 2021 ), who use NAT to integrate lexical constraints in decoding. enshtein Transformer (LevT) of ( 2019 ), seem to be a natural candidate to perform MT with tively edit an initial target sequence by performing insertion and deletion operations until convergence. in MT, where given a source sentence, we aim to edit a candidate translation retrieved from the TM. calization industry and implemented into basic wishing to translate a sentence can bene\ufb01t from fuzzy matching techniques to retrieve similar seg- ments from the TM. These segments can then be revised, thereby improving productivity and consis- tency of the translation process ( lart , 2010 ; , 2011 ). The retrieval of similar examples from a TM has also proved useful in con- ventional (AR) neural MT systems; they can be injected into the encoder ( , 2019 ; , 2020 ) or as priming signals in the decoder ( , 2020 ) to in\ufb02uence the translation pro- cess. These studies report signi\ufb01cant gains in trans- lation performance in technical domains, where the translation of terms and phraseology greatly bene\ufb01ts from examples found in a TM. improved version of LevT suited to the revision part of TM use, where the translation retrieved from TM is modi\ufb01ed via edit operations in a non- autoregressive way. We \ufb01rst show that the original that this failure is a direct consequence of its train- ing design. We propose to \ufb01x this issue with TM- procedure in two ways: (a) by also including the re- trieved candidate translation on the source side, as done in AR TM-based approaches ( can , 2019 ; , 2020 ); (b) by simultaneously training with empty and non-empty initial target sentences. In our experiments, TM-LevT achieves performance that is on par with a strong AR ap- arXiv:2210.06020v2  [cs.CL]  18 Feb 2023 proach on various domains when translating with TMs, with a reduced decoding load. We also ob- serve that incorporating an initial translation both on the source and target sides makes Knowledge , 2016 ) useless. rely on KD to alleviate the multimodality issue ( et al. , 2018a ). As far as we know, this work is the \ufb01rst to study NAT with TMs in a controlled setting. that the original LevT training scheme is not suited to edit similar translations from a TM; (b) we pro- pose a variant of LevT, TM-LevT with an improved training procedure, which yields performance that are close, or even similar to AR approaches when translating with good TM matches, with a reduced decoding load; (c) we highlight the bene\ufb01ts of multi-task training (with and without TMs) to attain the best performance; (d) we discuss the reasons why KD hurts the training of NAT with TMs. 2", "contribution": "The main contributions of this paper are:\n\n1. The proposal of a non-autoregressive transformer model, TM-LevT, suited for the revision part of translation memory (TM) use, where the translation retrieved from TM is modified via edit operations in a non-autoregressive way.\n\n2. The demonstration that the original LevT training scheme is not suited to edit similar translations from a TM.\n\n3. The proposal of a variant of LevT, TM-LevT, with an improved training procedure, which yields performance that is close, or even similar to autoregressive approaches when translating with good TM matches, with a reduced decoding load.\n\n4. The highlighting of the benefits of multi-task training (with and without TMs) to attain the best performance.\n\n5. The discussion of the reasons why knowledge distillation hurts the training of non-autoregressive transformers with TMs.", "title": "Integrating Translation Memories into Non-Autoregressive Machine Translation", "words": 776, "abstract": "Non-autoregressive machine translation (NAT) has recently made great progress. However, most works to date have focused on standard translation tasks, even though some edit-based NAT models, such as the Levenshtein Trans- former (LevT), seem well suited to translate with a Translation Memory (TM). This is the scenario considered here. We \ufb01rst analyze the vanilla LevT model and explain why it does not do well in this setting. We then propose a new variant, TM-LevT, and show how to effec- tively train this model. By modifying the data presentation and introducing an extra deletion operation, we obtain performance that are on par with an autoregressive approach, while re- ducing the decoding load. We also show that incorporating TMs during training dispenses to use knowledge distillation, a well-known trick used to mitigate the multimodality issue. 1"}
{"introduction": "How <mask> <mask> solar <mask> function ? How   does   the   solar  system  function ? Predict & mask x N Student Teacher Gradient EMA DiMS training. The student is trained to match the predictions of the teacher after several iterative steps. Teacher is up- dated with an exponential moving average of the student. autoregressive decoding strategy, generating the target sen- tence one token at a time. This sequential nature makes the inference process slow and dependent on the output se- quence length. To address this limitation Gu et al. [2018] introduces the Non-Autoregressive Transformer (NAT). NAT generates the entire target sentence in parallel, re- ducing the latency by an order of magnitude. NAT can be considered as a member of a broader family of iterative non-autoregressive Transformers (iNAT) [Lee et al., 2020, number of decoding steps is \ufb01xed and independent of the sequence length. By tuning the number of decoding steps, one can control the trade-off between speed and quality. to their autoregressive counterparts, Kasai et al. [2020b] shows that autoregressive models can be sped up without loss in accuracy by combining shallow decoders with deep encoders. This diminishes the computational advantage of iNATs and challenges their motivation. 2020a, Qian et al., 2021, Du et al., 2021]. \u2217 Equal contribution. 1 We release our code at https://github.com/layer6ai-labs/DiMS . Preprint. Under review. arXiv:2206.02999v1  [cs.CL]  7 Jun 2022 introduce Di still M ultiple S teps ( DiMS ), a distillation algorithm applicable to a wide range of iterative models. Given a pre-trained iNAT, referred to as teacher , a student aims to replicate the behavior of multiple iterative steps of the teacher with one decoding pass. This process resembles the well-known knowledge distillation framework [Hinton et al., 2015]. However, instead of reducing the number of parameters, we aim to decrease the number of decoding passes. The \ufb01nal model then enjoys the translation quality of multi-steps iNAT with the computational ef\ufb01ciency of single-step translation. optimized student becomes the next teacher. While effective, iterative distillation is slow as it requires multiple rounds of training until convergence. Alternatively, we propose updating the parameters of the teacher with an exponential moving average (EMA) of the student. This gradually transfers the new knowledge learned by the student to the teacher and can be viewed as a continuous variant of iterative distillation. Figure 1 depicts the DiMS algorithm on a high level. obtains substantial improvements on single-step translation with gains of up to 7 BLEU points on the distilled training dataset, while the gains on raw datasets are even greater. Notably, we are able to surpass many leading NAT models designed speci\ufb01cally for single-step translation. We further show that EMA considerably speeds up training and converges to a comparable accuracy with iterative distillation in a fraction of epochs. 2", "contribution": "The main contributions of this academic paper are:\n\n1. Introducing the DiMS algorithm, a distillation algorithm that aims to decrease the number of decoding passes required for iterative models while maintaining translation quality.\n2. Proposing the use of an exponential moving average (EMA) of the student to update the parameters of the teacher, which transfers new knowledge learned by the student to the teacher and can be viewed as a continuous variant of iterative distillation.\n3. Showing that the DiMS algorithm obtains substantial improvements on single-step translation with gains of up to 7 BLEU points on the distilled training dataset, while the gains on raw datasets are even greater.\n4. Demonstrating that EMA considerably speeds up training and converges to a comparable accuracy with iterative distillation in a fraction of epochs.", "title": "DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers", "words": 652, "abstract": "The computational bene\ufb01ts of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Di still M ultiple S teps ( DiMS ), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational bene\ufb01ts of early iterations while preserving the enhancements from several iterative steps. DiMS relies on two models namely student and teacher . The student is optimized to predict the output of the teacher after multiple decoding steps while the teacher follows the student via a slow-moving average. The moving average keeps the teacher\u2019s knowledge updated and enhances the quality of the labels provided by the teacher. During inference, the student is used for translation and no additional computation is added. We verify the effectiveness of DiMS on various models obtaining improvements of up to 7 BLEU points on distilled and 12 BLEU points on raw WMT datasets for single-step translation. 1 1"}
{"introduction": "Neural machine translation (NMT) models with encoder- decoder framework (Sutskever, Vinyals, and Le 2014; Bah- danau, Cho, and Bengio 2014) signi\ufb01cantly outperform conventional statistical machine translation models (Koehn, Och, and Marcu 2003; Koehn et al. 2007). Despite their suc- cess, the state-of-the-art NMT models usually suffer from the slow inference speed, which has become a bottleneck to apply NMT in real-world translation systems. The slow in- ference speed of NMT models is due to their autoregressive property, i.e., decoding the target sentence word-by-word according to the translation history. Recently, Gu et al. (2018) introduced non-autoregressive NMT (NAT) which can simultaneously decode all target words to break the bottleneck of the autoregressive NMT (AT) models. To this end, NAT models (Gu et al. 2018; Wei et al. 2019; Wang et al. 2019; Guo et al. 2019a) usu- ally directly copy the source word representations to the in- put of the decoder, instead of using previous predicted tar- get word representations. Hence, the inference of different * indicates equal contribution Copyright \u00a9 2021, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. target words are independent, which enables parallel com- putation of the decoder in NAT models. NAT models could achieve 10 - 15 times speedup compared to AT models while maintaining considerable translation quality. However, NAT models still suffer from the multimodal- ity problem (Gu et al. 2018): it discards the dependencies among the target words, and therefore the target words may be chosen from multiple feasible translations, resulting in duplicate, missing or even wrong words. For example, the German phrase \u201c Vielen Dank \u201d can be translated as both \u201c thank you \u201d and \u201c many thanks \u201d. Unfortunately, as each tar- get word is generated independently, \u201c thank thanks \u201d and \u201c many you \u201d may also be assigned high probabilities, result- ing in inferior translation quality. In this work, we argue re- ordering information is essential for NAT models and help- ful for alleviating the multimodality problem. To this end, we propose a novel NAT framework named ReorderNAT in this work, which explicitly models the re- ordering information to guide the decoding of NAT. To be speci\ufb01c, as shown in Figure 1, ReorderNAT \ufb01rst reorders the source sentence into a pseudo-translation formed by source words but in the target language word order, and then trans- lates the source sentence conditioned on it. We further intro- duce two guiding decoding strategies which utilize the re- ordering information (i.e. pseudo-translation) to guide the word selection in decoding. The \ufb01rst one is deterministic guiding decoding which \ufb01rst generates a most likely pseudo- translation and then generates the target sentence based on it. The second one is non-deterministic guiding decoding which utilizes the conditional distribution of the pseudo- translation as a latent variable to guide the decoding of target sentences. Ideally, the pseudo-translation can be viewed as a \ufb01nal translation written in source language. Guiding decoding with it could help to model the conditional dependencies of the target words and encourage the decoder to choose words belonging to the same translation, which naturally reduces the multimodality problem. Moreover, the decoding space of generating pseudo-translation is limited to the permutation of words in the source sentence, which can be well modeled by a small model. Therefore, ReorderNAT could effectively alleviate the multimodality problem by introducing the re- ordering information in NAT. Experimental results on several widely-used benchmarks arXiv:1911.02215v2  [cs.CL]  16 Dec 2020 I        want      to      thank    my     friends Emb Emb Emb Emb Emb Emb \u00d7\ud835\udc41 Emb Emb Emb Emb Emb \u2460 Inter-Attention \u00d7\ud835\udc3e I     want     my    friends  thank Reordering Module Emb Emb Emb Emb Emb \u2461 \u00d7\ud835\udc41 - \ud835\udc3e Ich  m\u00f6chte meinem Freund danken Decoder Module Encoder Module \u2462 \u2462 Figure 1: The architecture of our ReorderNAT model. Different from original NAT models, our model adds a reordering module between the encoder and decoder modules to explicitly model the reordering information. For original NAT models, the decoder inputs are the copied embeddings of source sentence (No.1 dashed arrow), and for our ReorderNAT model, the decoder inputs are the embeddings of pseudo-translation generated by reordering module (No. 2 dashed arrow). The encoder and decoder blocks are the same as existing NMT models (e.g., Transformer block). show that our proposed ReorderNAT model achieves signi\ufb01- cant and consistent improvements compared to existing NAT models by explicitly modeling the reordering information to guide the decoding. Moreover, by introducing a simple but effective AT module to model reordering information, our ReorderNAT immensely narrows the translation quality gap between AT and NAT models, while maintaining consider- able speedup (nearly six times faster). The source codes are available at https://github.com/ranqiu92/ReorderNAT.", "contribution": "The main contributions of this paper are:\n\n1. Introducing a novel non-autoregressive neural machine translation (NAT) framework named ReorderNAT that explicitly models the reordering information to guide the decoding of NAT.\n\n2. Proposing two guiding decoding strategies that utilize the reordering information to guide the word selection in decoding: deterministic guiding decoding and non-deterministic guiding decoding.\n\n3. Demonstrating that ReorderNAT significantly and consistently improves translation quality compared to existing NAT models by explicitly modeling the reordering information to guide the decoding.\n\n4. Introducing a simple but effective autoregressive (AT) module to model reordering information, which immensely narrows the translation quality gap between AT and NAT models while maintaining considerable speedup.", "title": "Guiding Non-Autoregressive Neural Machine Translation Decoding", "words": 1127, "abstract": "Abstract Non-autoregressive neural machine translation (NAT) gener- ates each target word in parallel and has achieved promis- ing inference acceleration. However, existing NAT models still have a big gap in translation quality compared to autore- gressive neural machine translation models due to the mul- timodality problem: the target words may come from mul- tiple feasible translations. To address this problem, we pro- pose a novel NAT framework ReorderNAT which explicitly models the reordering information to guide the decoding of NAT. Specially, ReorderNAT utilizes deterministic and non- deterministic decoding strategies that leverage reordering in- formation as a proxy for the \ufb01nal translation to encourage the decoder to choose words belonging to the same translation. Experimental results on various widely-used datasets show that our proposed model achieves better performance com- pared to most existing NAT models, and even achieves com- parable translation quality as autoregressive translation mod- els with a signi\ufb01cant speedup."}
{"introduction": "\nWhen translating a word, translation models need to spend a substantial amount of its capacity in\ndisambiguating its sense in the source language and choose a lexeme in the target language which\nadequately express its meaning (Choi et al., 2017; Tamchyna, 2017). However, neural machine\ntranslation (NMT) has a severe problem on lexical choice, since it usually has mistranslation errors\non low-frequency words (Koehn & Knowles, 2017; Nguyen & Chiang, 2018).\n\nKD-TGT Today, Newmargot\u2019s runway is soft ...\n\nSRC \u4eca\u5929 \u7ebd\u9a6c \u57fa\u7279 \u7684 \u8dd1\u9053 \u6e7f\u8f6f \u3002\nRAW-TGT The going at Newmarket is soft ...\n\nSRC \u7ebd\u9a6c \u57fa\u7279 \u8d5b\u9a6c \u603b\u662f \u5438\u5f15 ...\nRAW-TGT The Newmarket stakes is always ...\nKD-TGT The Newmarquette races always ...\n\nIn recent years, there has been a grow-\ning interest in non-autoregressive transla-\ntion (NAT, Gu et al., 2018), which im-\nproves decoding ef\ufb01ciency by predicting\nall tokens independently and simultane-\nously. Well-performed NAT models are\ngenerally trained on synthetic data dis-\ntilled by autoregressive translation (AT)\nteachers instead of the raw training data\n(Figure 1(a)) (Stern et al., 2019; Lee\net al., 2018; Ghazvininejad et al., 2019;\nGu et al., 2019). Recent studies have re-\nvealed that knowledge distillation (KD)\nreduces the modes (i.e. multiple lexical\nchoices for a source word) in the raw\ndata by re-weighting the training exam-\nples (Furlanello et al., 2018; Tang et al., 2020), which lowers the intrinsic uncertainty (Ott et al.,\n2018) and learning dif\ufb01culty for NAT (Zhou et al., 2020; Ren et al., 2020). However, the side effect\n\nTable 1: All samples that contain the source word \u201c\u7ebd\n\u9a6c \u57fa\u7279\u201d in raw and distilled training corpora, which\nare different in target sides (RAW-TGT vs. KD-TGT).\n\nRAW-TGT I\u2019ve ... in the 3.45 at Newmarket.\nKD-TGT I ... at 3:45 a.m. in Newmarquite.\n\nSRC \u5728 \u7ebd\u9a6c \u57fa\u7279 3 \u65f6 45 \u5206 \u90a3\u573a \u4e2d , \u6211 ...\n\n\u2217Work was done when Liang Ding and Xuebo Liu were interning at Tencent AI Lab.\n\n \n \n \n \n \n \n\fof KD has not been fully studied. In this work, we investigate this problem from the perspective of\nlexical choice, which is at the core of machine translation.\n\nWe argue that the lexical choice errors of AT teacher can be propagated to the NAT model via the\ndistilled training data. To verify this hypothesis, we qualitatively compare raw and distilled training\ncorpora. Table 1 lists all samples whose source sentences contain the place name \u201c\u7ebd\u9a6c\u57fa\u7279\u201d. In the\nraw corpus (\u201cRAW-TGT\u201d), this low-frequency word totally occurs three times and corresponds to\ncorrect translation \u201cNewmarket\u201d. However, in the KD corpus (\u201cKD-TGT\u201d), the word is incorrectly\ntranslated into a person name \u201cNewmargot\u201d (Margot Robbie is an Australian actress) or organization\nname \u201cNewmarquette\u201d (Marquette is an university in Wisconsin) or even invalid one \u201cNewmarquite\u201d.\n\nMotivated by this \ufb01nding, we explore NAT from the lexical choice perspective. We \ufb01rst validate our\nhypothesis by analyzing the lexical choice behaviors of NAT models (\u00a73). Concretely, we propose\na new metric AoLC (accuracy of lexical choice) to evaluate the lexical translation accuracy of a\ngiven NAT model. Experimental results across different language pairs show that NAT models\ntrained on distilled data have higher accuracy of global lexical translation (AoLC\u2191), which results in\nbetter sequence generation. However, \ufb01ne-grained analyses revealed that although KD improves the\naccuracy on high-frequency tokens, it meanwhile harms performance on low-frequency ones (Low\nfreq. AoLC\u2193). And with the improvement of teacher models, this issue becomes more severe. We\nconclude that the lexical choice of the low-frequency tokens is a typical kind of lost information\nwhen using knowledge distillation from AT model.\n\nIn order to rejuvenate this lost information in raw data, we propose to expose the raw data to\nthe training of NAT models, which augments NAT models the ability to learn the lost knowledge\nby themselves. Speci\ufb01cally, we propose two bi-lingual lexical-level data-dependent priors (Word\nAlignment Distribution and Self-Distilled Distribution) extracted from raw data, which is integrated\ninto NAT training via Kullback-Leibler divergence. Both approaches expose the lexical knowledge in\nthe raw data to NAT, which makes it learn to restore the useful information of low-frequency words\nto accomplish the translation.\n\nWe validated our approach on several datasets that widely used in previous studies (i.e. WMT14\nEn-De, WMT16 Ro-En, WMT17 Zh-En, and WAT17 Ja-En) and model architectures (i.e. MaskPre-\ndict (Ghazvininejad et al., 2019) and Levenshtein Transformer (Gu et al., 2019)). Experimental results\nshow that the proposed method consistently improve translation performance over the standard NAT\nmodels across languages and advanced NAT architectures. The improvements come from the better\nlexical translation accuracy (low-frequency tokens in particular) of NAT models (AoLC\u2191), which\nleads to less mis-translations and low-frequency words prediction errors. The main contributions of\nthis work are:\n\n\u2022 Our study reveals the side effect of NAT models\u2019 knowledge distillation on low-frequency lexicons,\n\nwhich makes the standard NAT training on the distilled data sub-optimal.\n\n\u2022 We demonstrate the necessity of letting NAT models learn to distill lexical choices from the raw\n\ndata by themselves.\n\n\u2022 We propose an simple yet effective approach to accomplish this goal, which are robustly applicable\n\nto several model architectures and language pairs.", "contribution": "The main contributions of this paper are: \n1. Revealing the side effect of NAT models' knowledge distillation on low-frequency lexicons, which makes the standard NAT training on the distilled data sub-optimal.\n2. Demonstrating the necessity of letting NAT models learn to distill lexical choices from the raw data by themselves.\n3. Proposing a simple yet effective approach to accomplish this goal, which is robustly applicable to several model architectures and language pairs.", "title": "UNDERSTANDING AND IMPROVING LEXICAL CHOICE IN NON-AUTOREGRESSIVE TRANSLATION", "words": 1341, "abstract": ""}
{"introduction": "Encoder-decoder based neural machine translation (NMT) models have achieved impressive success ( Sutskever et al. , 2014 ; Bahdanau et al. , 2014 ; Cho et al. , 2014 ; , 2016 ; et al. , 2017 ; , 2017 ). Since tokens are predicted one-by-one, this decoding strategy is called autoregressive translation (AT). Regardless of its simplicity, the autoregressive property makes the model slow to run, and thus it is often a bottleneck in parallel computing with GPUs. Many attempts have tried to address this issue by replacing AT with non-autoregressive translation (NAT) during decoding, namely, generating multiple or even all tokens at once ( Gu et al. , 2018 ; , 2019 ; , 2018 ). However, the conditional independence between target tokens in NAT makes it dif\ufb01cult for the model to capture the complete distribution of the target sequence. multimodality problem ( , 2018 ). multimodality problem often causes two types of translation errors during inference ( , 2019 ): repeated translation , i.e., the same token is generated repeatedly at consecutive time steps, and incomplete translation , i.e., the semantics of several source tokens are not fully translated. We hypothesize that this problem is related to the limitation of NAT systems in modeling the relations between positions and tokens (the position-token mismatch issue ) and the relations between target tokens (the token-token independence issue ). For the former, current NAT approaches do not explicitly model the positions of the output words, and may ignore the reordering issue in generating output sentences ( , 2020 ). For the latter, the independence between tokens directly results in repeated or missing semantics in the output sequence since each token has little knowledge of what tokens have been produced at other positions. novel methods to address these problems: (1) \ufb01rst predicts its neighbor tokens on both sides and then use the two-way tokens to guide the decoding of the token at the current position; and (2) Vocabulary Attention (VA), where each position in intermediate decoder layers attends to reorder prediction labels of a word based on its contextual information. decoder in different ways. VA emphasizes the arXiv:2002.03084v1  [cs.CL]  8 Feb 2020 relations between tokens, whereas LA models the relations between positions and tokens. The combination of the two methods leads to more accurate translation. We also introduce a dynamic bidirectional decoding strategy to improve the translation quality with high ef\ufb01ciency, which further enhances the performance of the translation model. models tokens and their orders within the decoded sequence, and thus greatly reducing the negative impact of the multimodality issue. tasks, including WMT14 En \u2192 WMT14 De \u2192 En, WMT16 Ro \u2192 En and IWSLT14 De \u2192 En. method achieves competitive performance compared with existing state-of-the-art non-autoregressive and autoregressive neural machine translation models while signi\ufb01cantly reducing the decoding time. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing two novel methods to address the multimodality problem in non-autoregressive translation: (a) two-way decoding, where the model predicts neighbor tokens on both sides to guide the decoding of the current token, and (b) Vocabulary Attention (VA), where each position in intermediate decoder layers attends to reorder prediction labels of a word based on its contextual information.\n\n2. Introducing a dynamic bidirectional decoding strategy to improve translation quality with high efficiency.\n\n3. Combining the two methods mentioned above to model the relations between tokens and positions, leading to more accurate translation.\n\n4. Achieving competitive performance compared to existing state-of-the-art non-autoregressive and autoregressive neural machine translation models while significantly reducing decoding time.", "title": "LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Encoder Decoder", "words": 625, "abstract": "Non-autoregressive translation (NAT) models generate multiple tokens in one forward pass and is highly ef\ufb01cient inference stage compared with autoregressive at translation (AT) methods. However, NAT models often suffer from the multimodality problem, i.e., generating duplicated tokens or missing tokens. In this paper, we propose two novel methods to address this issue, the Look-Around (LA) strategy and the Vocabulary Attention (VA) mechanism. The Look-Around strategy predicts the neighbor tokens in order to predict the current token, and the Vocabulary Attention models long-term token dependencies inside the decoder by attending the whole vocabulary for each position to acquire knowledge of which token is about to generate. Our proposed model uses signi\ufb01cantly less time during inference compared with autoregressive models and most other NAT models. Our experiments on four benchmarks (WMT14 En \u2192 De, WMT14 De \u2192 En, WMT16 Ro \u2192 En and IWSLT14 De \u2192 En) show that the proposed model achieves competitive performance compared with the state-of-the-art non-autoregressive and autoregressive models while signi\ufb01cantly reducing the time cost in inference phase. 1"}
{"introduction": "020 021 guage processing (NLP) task, aiming at generating 022 concise summaries for given texts while preserving 023 the key information. It has extensive real-world 024 applications such as headline generation ( 025 et al. , 2011 ). 026 027 typically trained in a supervised way with large 028 training corpora, comprising pairs of long texts and 029 their summaries ( , 2020 ; 030 et al. , 2020 , 2021 ). However, such parallel data are 031 expensive to obtain, preventing the applications to 032 less popular domains and less spoken languages. 033 034 increasing interest, because it does not require par- 035 allel data for training. One widely used approach 036 is to compress a long text into a short one, and to 037 reconstruct it to the long text by a cycle consis- 038 tency loss ( , 2016 ; 039 1 https://github.com/ARR-NAUS/NAUS , 2018 ; , 2019 ). Due to the in- 040 differentiability of the compressed sentence space, 041 such an approach requires reinforcement learning 042 (or its variants), which makes the training dif\ufb01cult 043 ( , 2021 ). 044 ( 2020 ) propose an 045 edit-based approach for unsupervised summariza- 046 tion. Their model maximizes a scoring function 047 that evaluates the quality (\ufb02uency and semantics) 048 of the generated summary, achieving higher perfor- 049 mance than cycle-consistency methods. However, 050 the search approach is slow in inference because 051 hundreds of search steps are needed for each data 052 sample. Moreover, their approach can only select 053 words from the input sentence with the word order 054 preserved. Thus, it is restricted and may gener- 055 ate noisy summaries due to the local optimality of 056 search algorithms. 057 058 a Non-Autoregressive approach to Unsupervised 059 060 search as in ( 2020 ) and, inspired 061 by ( 2020 ), to train a machine learning 062 model to smooth out such noise and to speed up the 063 inference process. Different from ( 2020 ), 064 we propose to utilize non-autoregressive text gen- 065 erators, which generate all tokens in the output in 066 parallel, based on our following observations: 067 \u2022 068 faster than autoregressive generation, which is im- 069 portant when the system is deployed. 070 \u2022 071 have a strong correspondence. Non-autoregressive 072 generation supports encoder-only architectures, 073 which can better utilize such input\u2013output cor- 074 respondence and even outperform autoregressive 075 models for summarization. 076 \u2022 077 a length-control algorithm based on dynamic pro- 078 gramming. This can satisfy the output length con- 079 straint, which is typical in summarization but can- 080 1 not be easily achieved with autoregressive models. 081 082 line generation ( , 2003 ) and DUC2004 083 ( , 2004 ) datasets. Experiments show 084 that our NAUS achieves state-of-the-art perfor- 085 mance on unsupervised summarization; especially, 086 it outperforms its teacher (i.e., the search approach), 087 con\ufb01rming that NAUS can indeed smooth out the 088 search noise. Regarding inference ef\ufb01ciency, our 089 NAUS with truncating is 1000 times more ef\ufb01cient 090 than the search approach; even with dynamic pro- 091 gramming for length control, NAUS is still 100 092 times more ef\ufb01cient than search and several times 093 more ef\ufb01cient than autoregressive models. Our 094 NAUS is also able to perform length-transfer sum- 095 mary generation, i.e., generating summaries of dif- 096 ferent lengths from training. 097 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a non-autoregressive approach to unsupervised summarization that utilizes non-autoregressive text generators, which generate all tokens in the output in parallel, and a length-control algorithm based on dynamic programming.\n\n2. Demonstrating that the proposed approach achieves state-of-the-art performance on unsupervised summarization, outperforming its teacher (i.e., the search approach) and smoothing out search noise.\n\n3. Showing that the proposed approach is significantly more efficient than the search approach, even with dynamic programming for length control, and several times more efficient than autoregressive models.\n\n4. Demonstrating that the proposed approach is able to perform length-transfer summary generation, i.e., generating summaries of different lengths from training.", "title": "Learning Non-Autoregressive Models from Search Anonymous ACL submission Abstract References Generation", "words": 854, "abstract": "001 summary for an input text. In this work, we 002 propose a Non-Autoregressive Unsupervised 003 004 not require parallel data for training. 005 NAUS \ufb01rst performs edit-based search to- 006 wards a heuristically de\ufb01ned score, and gener- 007 ates a summary as pseudo-groundtruth. Then, 008 we train an encoder-only non-autoregressive 009 010 also propose a dynamic programming ap- 011 proach for length-control decoding, which is 012 important for the summarization task. 013 periments on two datasets show that NAUS 014 achieves state-of-the-art performance for unsu- 015 pervised summarization, yet largely improving 016 inference ef\ufb01ciency. Further, our algorithm is 017 able to perform explicit length-transfer sum- 018 mary generation. 1 019 1"}
{"introduction": "systems are mainly autoregressive (AR) mod- els ( , 2015 ; , 2017 ), which decompose the joint probability of a se- quence of tokens in a left-to-right order, model- ing dependencies of each token with its preceding ones. Despite having strong performance, such sequential decoding causes considerable latency, thereby unsatisfactory ef\ufb01ciency. tion models ( , 2018 ) permit potentially more ef\ufb01cient parallel decoding. To do so, NAR 1 Code will be released at https://github.com/ wxy-nlp/MultiTaskNAT . Encoder NAR Decoder Layer N \u00d7 x 1 x 2 x 3 x 4 y 1 Linear+Softmax y 2 y 3 y 4 y 5 e 1 e 2 e 3 e 4 h 1 h 2 h 3 h 4 h 5 Self-Attention Cross-Attention Feed Forward Network <bos> y 1 y 2 y 3 y 4 Weak AR Decoder y 1 y 2 y 3 y 4 y 5 Query Key Value forward pass backward pass Figure 1: Illustration of our approach, where we in- troduce a set of auxiliary weak AR decoders, each of which must make its predictions solely relying on the information contained in the NAR decoder hidden states. Thus, the information provided by the NAR de- coder must be suf\ufb01ciently useful for the AR decoders to be capable of predicting the target sequence because the AR decoders are parameterized as weakly as possi- ble, which will in turn let the NAR decoder learn to get stronger. models necessitate a notorious conditional indepen- dence assumption on target sequences as a trade- off. This assumption, however, is probalistically insuf\ufb01cient to describe the highly multi-modal na- ture of human language data, imposing severe chal- lenges for NAR models in a way of yielding less informative learning signals and gradients under the conventional MLE training. As a result, NAR models often manifest implausible neural repre- sentations, especially in the decoder part as the decoder governs the generation, resulting in signi\ufb01- cant performance sacri\ufb01ce. To close the accuracy gap, a majority of previous studies aim at improv- ing the modeling of dependencies with more condi- tional information ( , 2021 ; jad et al. , 2019 ). We argue that these research efforts are equivalent to providing better alternative learning signals without changing the NAR mod- els\u2019 probabilistic framework. However, most of these methods require a speci\ufb01c modi\ufb01cation to the arXiv:2211.06075v1  [cs.CL]  11 Nov 2022 commonly-used Transformer model architecture. A natural question may arise: can we encourage the NAR decoder to learn from sources of signals that are more informative than that of the condi- tional independence assumption, in order to better capture target dependencies? It would be more ad- vantageous if it is also modi\ufb01cation-free regarding model architectures and could also used with all current NAR systems. learning framework that introduces auxiliary weak AR decoders to make NAR models stronger. The key idea is that we parameterize the auxiliary AR decoders as weakly as possible and force them to predict target sequences solely based on the in- formation from NAR decoder\u2019s hidden representa- tions, such that they can no longer model the target sequence on their own unless the knowledge pro- vided by the NAR decoder is suf\ufb01ciently useful. become stronger so as to support the AR partners that are poorly parameterized. Additionally, our approach is plug-and-play and model-agnostic, and the weak AR decoders that we introduce are dis- carded during the inference stage, resulting in no additional decoding overhead. eral classes of NAR model, including vanilla NAR Transformer ( , 2018 ) and its CTC- based variant ( , 2018 ; haria et al. , 2020 ). Experiments on widely-used WMT14 English-to-German, WMT16 English- to-Romanian, and IWSLT14 German-to-English benchmarks show that our approach consistently helps build more accurate NAR models over strong baselines. 2", "contribution": "The main contribution of this paper is the introduction of a learning framework that uses auxiliary weak autoregressive (AR) decoders to make neural autoregressive (NAR) models stronger. The weak AR decoders are parameterized as weakly as possible and are forced to predict target sequences solely based on the information from the NAR decoder's hidden representations. This approach encourages the NAR decoder to learn from sources of signals that are more informative than the conditional independence assumption, resulting in better capture of target dependencies. The proposed approach is plug-and-play and model-agnostic, and the weak AR decoders are discarded during the inference stage, resulting in no additional decoding overhead. The experiments on widely-used benchmarks show that this approach consistently helps build more accurate NAR models over strong baselines.", "title": "Helping the Weak Makes You Strong: Simple Multi-Task Learning", "words": 907, "abstract": "Recently, non-autoregressive (NAR) neural machine translation models have received in- creasing attention due to their ef\ufb01cient parallel decoding. However, the probabilistic frame- work of NAR models necessitates conditional independence assumption on target sequences, falling short of characterizing human language data. This drawback results in less informa- tive learning signals for NAR models under conventional MLE training, thereby yielding unsatisfactory accuracy compared to their au- toregressive (AR) counterparts. In this paper, we propose a simple and model-agnostic multi- task learning framework to provide more in- formative learning signals. During training stage, we introduce a set of suf\ufb01ciently weak AR decoders that solely rely on the informa- tion provided by NAR decoder to make pre- diction, forcing the NAR decoder to become stronger or else it will be unable to support its weak AR partners. Experiments on WMT and IWSLT datasets show that our approach can consistently improve accuracy of multiple NAR baselines without adding any additional decoding overhead. 1"}
{"introduction": "framework ( Sutskever et al. , 2014 ; Bahdanau et al. , 2014 ) has achieved great success on the task of , 2016 ; , 2017 ; , 2017 ; , 2018 ; , 2020 ). In this framework, the encoder takes the source sentence as input and extracts its hidden representation, based on which the decoder generates the target sentence word by word and from left to right, i.e., \u2217 Corresponding author. in an autoregressive manner, which is a natural bot- tleneck for the inference speed due to the sequential conditional dependence. been substantially promoted, the translation ef\ufb01- ciency is becoming a new research hotspot. Non- autoregressive neural machine translation (NAT) models are proposed to reduce the translation la- tency while inference, by removing the conditional dependence between target tokens and predicting all tokens in parallel ( , 2017 ). As the con- text dependency cannot be utilized while decoding, the inference speedup of NAT models comes at the cost of the degradation in performance. As studied by previous works ( , 2019 ; , 2019 ), the inferior accuracy of NAT models mainly occurs from two aspects: 1) the source-side infor- mation is not adequately encoded which results in incomplete translation; 2) the decoder cannot handle the task well which leads to repeated trans- lations and poor performance on long sentences. formance of NAT models, in this paper, we empiri- cally conduct a thorough study on the functional- ities of the encoder and decoder in NAT models, and conclude that the encoder has a more direct in- \ufb02uence on the \ufb01nal translation performance, and is harder to train than the decoder. Therefore, we pro- pose a jointly masked sequence-to-sequence model which is inspired by the idea of masked language modeling ( , 2018 ). Speci\ufb01cally, for the encoder, we follow the masking strategy of BERT ( , 2018 ) and randomly mask a number of tokens of the source sentence. This strat- egy trains the encoder more rigorously by forcing it to encode the complete information with residual input. For the decoder, we mask the consecutive fragment of the target sentence to make the decoder concentrate more on predicting adjacent tokens, and propose an n -gram based loss function to learn 377 the consecutive tokens as a whole objective. In this way, we can alleviate the problem of repeated trans- lations of NAT models. During inference, we adopt a mask-and-predict ( , 2019 ) strategy to iteratively generate the translation result, which masks and predicts a subset of the current translation candidates in each iteration. We verify the effectiveness of our model on \ufb01ve benchmark translation tasks including WMT14 En- glish \u2194 \u2194 and IWSLT14 German \u2192 English. Our model out- performs all the NAT models in comparison, and can achieve comparative performance with its au- toregressive counterpart while enhanced with 5+ times speedup on inference ( 27 . 69 / 32 . 24 BLEU scores and 5 . 73 times speedup on the WMT14 En- 28 . 04 / 32 . 69 BLEU scores). follows: \u2022 While previous works only concentrate on ma- nipulating the decoder, we illustrate and em- phasize the importance of the encoder in NAT models and propose the encoder masking strat- egy to improve its training. \u2022 We propose the consecutive masking strategy of the decoder input and the n -gram loss func- tion to alleviate the problem of repetitive trans- lations of NAT models. \u2022 We integrate the two parts above in the jointly masked sequence-to-sequence model which shows strong performance on benchmark ma- chine translation datasets. 2", "contribution": "The main contributions of this paper are:\n\n- Illustrating and emphasizing the importance of the encoder in non-autoregressive neural machine translation (NAT) models and proposing an encoder masking strategy to improve its training.\n- Proposing a consecutive masking strategy of the decoder input and an n-gram loss function to alleviate the problem of repetitive translations of NAT models.\n- Integrating the above two parts in a jointly masked sequence-to-sequence model that shows strong performance on benchmark machine translation datasets.", "title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Junliang Guo Anhui Province Key Laboratory of Big Data Analysis and Application, Abstract Decoder Encoder Acknowledgements References", "words": 784, "abstract": "The masked language model has received re- markable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this tech- nique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural ma- chine translation (NAT). Speci\ufb01cally, we \ufb01rst empirically study the functionalities of the en- coder and the decoder in NAT models, and \ufb01nd that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the en- coder input while training. As for the decoder, we propose to train it based on the consecu- tive masking of the decoder input with an n - gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on \ufb01ve benchmark machine translation tasks, and our model can achieve 27 . 69 / 32 . 24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model. 1"}
{"introduction": "systems are based on autoregressive models ( danau et al. , 2015 ; , 2017 ) where each generation step depends on the previously gener- ated tokens. This sequential nature inevitably leads to inherent latency at inference time. On the other hand, non-autoregressive neural machine transla- tion models (NAT, , 2018a ) attempt to generate output sequences in parallel to speed-up \u2217 Equal contribution. 1 https://github.com/pytorch/fairseq/ tree/master/examples/nonautoregressive_ translation up on WMT\u201914 En \u2192 achieves the best trade-off. the decoding process. The incorrect independence assumption nevertheless prevents NAT models to properly learn the dependency between target to- kens in real data distribution, resulting in poorer performance compared to autoregressive (AT) mod- els. One popular solution to improve the NAT trans- lation accuracy is to sacri\ufb01ce the speed-up by incor- porating an iterative re\ufb01nement process, through which the model explicitly learns the conditional distribution over partially observed reference to- kens ( , 2019 ; , 2019 ). , 2020b ) indi- cated that iterative NAT models seem to lose the speed advantage compared to AT models with care- ful tuning of the layer allocation. For instance, an AT model with deep encoder and shallow decoder obtains similar latency as iterative NAT models without hurting the translation accuracy. model without iterative re\ufb01nements calls for more exploration. Several works ( , 2020a ; , 2020 ; , 2020 ) have recently been proposed to improve the training of NAT, though the performance gap compared to the iterative ones remains. In this work, we \ufb01rst ar- 121 gue that the key to successfully training a fully NAT model is to perform dependency reduction in the learning space of output tokens ( \u00a7 2 ) from all aspects. With this guidance, we revisit various methods which are able to reduce the dependen- cies among target tokens as much as possible in- cluding four different perspectives, i.e., training corpus ( \u00a7 3.1 ), model architecture ( \u00a7 3.2 ), training objective ( \u00a7 3.3 ) and learning strategy ( \u00a7 3.4 ). The performance gap can not be near closed unless we combine these techniques\u2019 advantages. standard translation benchmarks including 5 trans- lation directions where our system achieves new state-of-the-art results for fully NAT models on all directions. We also demonstrate the quality-speed trade-off comparing with AT and recent iterative NAT models in Figure 1 . Moreover, compared to the Transformer baseline, our model achieves 16.5 \u00d7 inference speed-up under the same soft- ware/hardware conditions while maintaining com- parable translation quality. 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a method for successfully training a fully non-autoregressive neural machine translation model by reducing dependencies among target tokens from various perspectives, including training corpus, model architecture, training objective, and learning strategy.\n\n2. Demonstrating the effectiveness of the proposed method on standard translation benchmarks, achieving new state-of-the-art results for fully non-autoregressive models on all directions.\n\n3. Comparing the quality-speed trade-off of the proposed model with autoregressive and recent iterative non-autoregressive models, showing a 16.5x inference speed-up while maintaining comparable translation quality to the Transformer baseline.", "title": "Fully Non-autoregressive Neural Machine Translation: Jiatao Gu Facebook AI Research Xiang Kong Language Technologies Institute Abstract Acknowledgements References Appendix", "words": 571, "abstract": "lation (NAT) simultaneously predicts tokens with single forward of neural networks, which signi\ufb01cantly reduces the inference latency at the expense of quality drop compared to the on closing the performance gap while main- taining the latency advantage. We \ufb01rst inspect the fundamental issues of fully NAT models, and adopt dependency reduction in the learn- ing space of output tokens as the primary guid- ance. ferent aspects that have been proven effective for improving NAT models, and carefully com- bine these techniques with necessary modi\ufb01- cations. Our extensive experiments on three translation benchmarks show that the proposed system achieves the state-of-the-art results for fully NAT models, and obtains comparable performance with the autoregressive and itera- tive NAT systems. For instance, one of the pro- posed models achieves 27 . 49 BLEU points on WMT14 En-De with 16 . 5 \u00d7 speed-up com- pared to similar sized autoregressive baseline under the same inference condition. The im- plementation of our model is available here 1 . 1"}
{"introduction": "adopted in NLP tasks ( Devlin et al. , 2019 ; Radford and Narasimhan , 2018 ). For example, XLM ( Con- neau and Lample , 2019 ) demonstrated that cross- lingual pre-training is effective in improving neu- ral machine translation (NMT), especially on low- resource languages. These methods all directly pre- train a bidirectional encoder or an unidirectional decoder. The encoder and decoder in NMT models are then independently initialized with them and 1 Code, data, and pre-trained models are avail- able at https://github.com/huawei-noah/ Pretrained-Language-Model/CeMAT Approach Enc. Dec. Mono. Para. mBERT ( Devlin et al. , 2019 ) \u2022 \u2022 XLM ( Conneau and Lample , 2019 ) \u2022 \u2022 \u2022 MASS ( Song et al. , 2019 ) \u2022 \u2192 \u2022 mBART ( Liu et al. , 2020 ) \u2022 \u2192 \u2022 mRASP ( Lin et al. , 2020 ) \u2022 \u2192 \u2022 CeMAT (Ours) \u2022 \u21d0\u21d2 \u2022 \u2022 Table 1: Comparison and summary of existing pre- trained models for machine translation. Enc: encoder; Dec: decoder; Mono: monolingual; Para: bilingual. \u201c \u2022 \u201d denotes the corresponding model is pre-trained or the corresponding data is used. \u201c \u2192 \u201d denotes the de- coder of model is unidirectional, \u201c \u21d0\u21d2 \u201d denotes the de- coder is bidirectional. \ufb01ne-tuned ( Guo et al. , 2020 ; Zhu et al. , 2020 ). Re- cently, pre-training standard sequence-to-sequence (Seq2Seq) models has shown signi\ufb01cant improve- ments and become a popular paradigm for NMT tasks ( Song et al. , 2019 ; Liu et al. , 2020 ; Lin et al. , 2020 ). However, some experimental results from XLM ( , 2019 ) have shown that the decoder module initialized by the pre-trained bidi- rectional masked language model (MLM) ( et al. , 2019 ), rather than the unidirectional causal language model (CLM, , 2018 ), would achieve better results on Autore- gressive NMT (AT). Especially, compared to ran- dom initialization, initialized by GPT ( Radford and , 2018 ) might result in performance degradation sometimes. We conjecture that when \ufb01ne-tuning on generation tasks (e.g., NMT), the representation capability of the pre-trained models may be more needed than the generation capability. Therefore, during pre-training, we should focus on training the representation capability not only for the encoder, but also for the decoder more explic- itly. mul- tilingual Conditional masked language prE- training model for MAchine Translation , which arXiv:2203.09210v1  [cs.CL]  17 Mar 2022 [en] Cat sat on the mat [en] We dance on the grass [de] Wir tanzen auf dem gras mono para Original [en] Cat sat on the mat [en] [mask] sat on the [mask] [en] We danse [mask] the grass [de] Wir [mask] auf dem  [mask] mono para Masked [en] Kedi sat on the [mask] Encoder Self-Attention Feed Forward Bidirectional Decoder Cross-Attention Self-Attention Feed Forward Encoder Decoder [en] who are you [de] Wer bist du Wer bist du </s> Autoregressive NMT Encoder Bidirectional Decoder [en] who are you [de] [mask] bist [mask] Wer du Non-autoregressive NMT Cat mat on tanzen gras mat Pre-training Fine-tuning Aligned code-switching & masking dynamic dual-masking mono para Figure 1: The framework for CeMAT, which consists of an encoder and a bidirectional decoder . \u201cmono\u201d denotes monolingual, \u201cpara\u201d denotes bilingual. During the pre-training (left), the original monolingual and bilingual inputs in many languages are augmented (the words are replaced with new words with same semantics or \u201c[mask]\u201d, please see Figure 2 for more details) and fed into the model. Finally, we predict all the \u201c[mask]\u201d words on the source side and target side respectively. For \ufb01ne-tuning (right), CeMAT provides uni\ufb01ed initial parameter sets for AT and NAT. consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridg- ing them. Speci\ufb01cally, the model is jointly trained by MLM on the encoder and Conditional MLM (CMLM) on the decoder with large-scale monolin- gual and bilingual texts in many languages. Table 1 compares our model with prior works. Bene\ufb01ting from the structure, CeMAT can provide uni\ufb01ed initialization parameters not only for AT task, but also for Non-autoregressive NMT (NAT) directly. NAT has been attracting more and more attention because of its feature of parallel decoding, which helps to greatly reduce the translation latency. the model, the masking operations are applied in two steps. First, some source words that have been aligned with target words are randomly selected and then substituted by new words of similar mean- ings in other languages, and their corresponding tar- get words are masked. We call this method aligned code-switching & masking . Then, the remaining words in both source and target languages will be masked by dynamic dual-masking . NAT tasks show signi\ufb01cant gains over prior works. bitext pairs), our system gains up to 14.4 BLEU points over baselines. Even for extremely high- resource settings (>25M), CeMAT still achieves signi\ufb01cant improvements. In addition, experiments on the WMT16 Romanian \u2192 strate that our system can be further improved (+2.1 BLEU) by the Back-Translation (BT; nrich et al. , 2016a ). The main contributions of our work can be sum- marized as follows: \u2022 coder, a bidirectional decoder. The model is pre-trained on both monolingual and bilingual corpora and then used for initializing down- stream AT and NAT tasks. To the best of our knowledge, this is the \ufb01rst work to pre-train a uni\ufb01ed model suitable for both AT and NAT. \u2022 enhance the model training under the setting of bidirectional decoders. Based on a multi- lingual translation dictionary and word align- ment between source and target sentences, aligned code-switching & masking is \ufb01rstly applied. Then, dynamic dual-masking is used. \u2022 and NAT tasks with data of varied sizes. Con- sistent improvements over strong competitors demonstrate the effectiveness of CeMAT. [en] We dance on the grass [de] Wir tanzen auf dem gras Spanish : danza German : tanzen French  : danse \u2026 dance 2. \ud835\udc39\ud835\udc39 \ud835\udc5a\ud835\udc5a (\ud835\udc65\ud835\udc65 \ud835\udc5a\ud835\udc5a \ud835\udc56\ud835\udc56 ) [en] We danse on the grass [de] Wir [mask] auf dem gras CSR CSM [en] We danse [mask] the grass [de] Wir [mask] auf dem  [mask] DM DM 1.Aligned Figure 2: The details of our two-step masking. We \ufb01rst obtain the aligned pair set \u039b = {(\u201cdance\u201d,\u201ctanzen\u201d),...} (marked with \ufffd\ufffd\ufffd ) from the original inputs by looking up the cross-lingual dictionary (denote as 1.Aligned ), and then randomly select a subset (marked as \u201cdance\u201d \ufffd\ufffd\ufffd \u201ctanzen\u201d with red color) from it, in the lower left of the \ufb01gure. For each element in the subset, we select a new word by F m ( x i m ) , and perform CSR to replace the source fragment (\u201cdanse\u201d marked as red color) and CSM for target (\u201c[mask]\u201d marked as red color) respectively. Finally, we do the DM process to mask the contents of the source and target respectively (\u201c[mask]\u201d marked as light-blue color). 2", "contribution": "The main contributions of this paper are:\n\n1. The development of a pre-trained model, called CeMAT, that is suitable for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. CeMAT consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and then used for initializing downstream AT and NAT tasks.\n\n2. The proposal of a two-step masking method, called aligned code-switching & masking and dynamic dual-masking, to enhance the model training under the setting of bidirectional decoders. Based on a multilingual translation dictionary and word alignment between source and target sentences, aligned code-switching & masking is firstly applied. Then, dynamic dual-masking is used.\n\n3. The demonstration of consistent improvements over strong competitors in AT and NAT tasks with data of varied sizes. CeMAT achieves significant gains over baselines, even for extremely low-resource settings (<100k bitext pairs).", "title": "Universal Conditional Masked Language Pre-training for Neural Machine Translation", "words": 1736, "abstract": "Pre-trained sequence-to-sequence models have signi\ufb01cantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence- to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non- autoregressive NMT. Speci\ufb01cally, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. 1 We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking . We conduct extensive experi- ments and show that our CeMAT can achieve signi\ufb01cant performance improvement for all scenarios from low to extremely high resource, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. As far as we know, this is the \ufb01rst work to pre-train a uni\ufb01ed model for \ufb01ne-tuning on both NMT tasks. 1"}
{"introduction": "Transformer-based models (Vaswani et al. 2017) have been proven effective for various sequence to sequence gener- ation tasks, such as machine translation (Wu et al. 2019; Liang et al. 2021), text summarization (Savelieva, Au- Yeung, and Ramani 2020; Elsaid et al. 2022), dialogue sys- tems (Zhang et al. 2020; Ma et al. 2020), code genera- tion (Wang et al. 2020), etc. Despite the excellent perfor- mance of Transformer-based models, they usually adopt the autoregressive (AR) decoding paradigm in which the decod- ing of a target sequence is decomposed into multi-step pre- dictions in left-to-right order, i.e., the next prediction is con- ditioned on the previously generated part. Such an attribute increases the inference time cost linearly with the target se- quence length, which is time-consuming for long sequences. * Corresponding Author Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1 https://github.com/amom-nar/AMOM To alleviate this problem, many recent works explore non- autoregressive (NAR) methods (Gu et al. 2018; Qian et al. 2021; Xiao et al. 2022) to predict a target sequence in paral- lel, which can dramatically increase inference speed. As the cost of increasing decoding speed, NAR models remove the internal dependency of the target sequence and perform each decoding prediction depending entirely upon the source/in- put sequence. Inevitably, the generation quality of NAR methods falls behind their AR counterparts without target- side information in decoding (Gu et al. 2018). To achieve a better trade-off between inference speedup and generation quality, the conditional masked language model (CMLM) (Ghazvininejad et al. 2019) has been pro- posed and has already become one of the most competitive and widely-used NAR frameworks, which exploits an iter- ative mask-predict decoding strategy. In the training stage, CMLM leverages a masked language model objective to generate the masked subset of the target sequence in par- allel conditioned on the source input and unmasked part in target sequence. During inference, CMLM \ufb01rst generates the whole target sequence in parallel (the \ufb01rst iteration) and then iteratively masks and predicts low-con\ufb01dence tokens. Based on CMLM, many recent works have achieved performance improvements with advanced enhancement strategies from different perspectives, e.g., improving the inference strat- egy (Kasai et al. 2020a; Geng, Feng, and Qin 2021), bene\ufb01t- ing from the AT counterpart (Hao et al. 2021), training with better criterion (Marjan et al. 2020; Du, Tu, and Jiang 2021), introducing self-correction mechanism (Huang, Perez, and Volkovs 2022) and pre-training (Li et al. 2022b). In this paper, we further introduce a simple yet very effec- tive strategy to enhance the re\ufb01nement capability of CMLM without changing the model structure and the inference al- gorithm, named adaptive masking over masking (AMOM). Concretely, we present two adaptive masking operations for both the source and target sequence based on the conven- tional one-time masking in CMLM. The masking operation for the source sequence can make the encoder optimization easier by adaptively masking a proportion of tokens based on the masked target sequence. In contrast, the vanilla CMLM constructs multiple masked target sequences for each source sequence in model training, making the encoder dif\ufb01cult to converge (Guo, Xu, and Chen 2020). Another potential merit of the source-side masking is to improve the stability of the arXiv:2303.07457v1  [cs.CL]  13 Mar 2023 CMLM model against different decoder inputs by prevent- ing the internal co-adaptation (akin to dropout (Hinton et al. 2012)). Moreover, cooperating it with the masking condi- tion of the target sentence can better improve the ability rather than \ufb01xed masking. Notice that JM-NAT (Guo, Xu, and Chen 2020) also explores the source-side masking op- eration but has a clear difference from our strategy. It in- troduces a BERT-like masked language model task on the encoder side to enhance the encoder training, whereas our adaptive strategy does not introduce any extra task and can dynamically capture target-side information. The target-side adaptive masking operation is presented to enhance the re- \ufb01nement process of CMLM, motivated by the masking ra- tio changes of the target sequence in different inference it- erations, which cannot be captured by the one-time mask- ing. Simultaneously, unlike the adaptive target-side masking strategy in GLAT (Qian et al. 2021) to achieve curriculum learning, we design the masking strategy specially to en- courage the model to perform steadily and conduct re\ufb01ne- ments effectively. We focus on the promotion of each itera- tion rather than only enhancing the \ufb01rst iteration in GLAT. More comparisons between our strategy and the counter- parts used in GLAT can be found in the experiments part. Though AMOM is simple, i.e., only two extra masking operations in model training, we \ufb01nd it is surprisingly effec- tive on different sequence generation tasks, including neu- ral machine translation, summarization, and code genera- tion ( 15 datasets in total). It achieves state-of-the-art per- formance on multiple datasets based on the vanilla CMLM, e.g., 34.62 BLEU score on WMT16 EN \u2192 RO, 34.82 BLEU on WMT16 RO \u2192 EN, and 34.84 BLEU on IWSLT De \u2192 En. AMOM even performs better than the strong autoregressive Transformer on 7 datasets with at least 2.2 \u00d7 speedup.", "contribution": "The main contribution of this paper is the introduction of a simple yet effective strategy called Adaptive Masking over Masking (AMOM) to enhance the refinement capability of the Conditional Masked Language Model (CMLM) without changing the model structure and the inference algorithm. AMOM presents two adaptive masking operations for both the source and target sequence based on the conventional one-time masking in CMLM. The masking operation for the source sequence can make the encoder optimization easier by adaptively masking a proportion of tokens based on the masked target sequence. Another potential merit of the source-side masking is to improve the stability of the CMLM model against different decoder inputs by preventing the internal co-adaptation. The target-side adaptive masking operation is presented to enhance the refinement process of CMLM, motivated by the masking ratio changes of the target sequence in different inference iterations, which cannot be captured by the one-time masking. AMOM achieves state-of-the-art performance on multiple datasets based on the vanilla CMLM, even outperforming the strong autoregressive Transformer on 7 datasets with at least 2.2 \u00d7 speedup.", "title": "AMOM: Adaptive Masking over Masking for Conditional Masked Language Model", "words": 1313, "abstract": "Abstract Transformer-based autoregressive (AR) methods have achieved appealing performance for varied sequence-to- sequence generation tasks, e.g., neural machine translation, summarization, and code generation, but suffer from low inference ef\ufb01ciency. To speed up the inference stage, many non-autoregressive (NAR) strategies have been proposed in the past few years. Among them, the conditional masked language model (CMLM) is one of the most versatile frame- works, as it can support many different sequence generation scenarios and achieve very competitive performance on these tasks. In this paper, we further introduce a simple yet effec- tive adaptive masking over masking strategy to enhance the re\ufb01nement capability of the decoder and make the encoder optimization easier. Experiments on 3 different tasks (neural machine translation, summarization, and code generation) with 15 datasets in total con\ufb01rm that our proposed simple method achieves signi\ufb01cant performance improvement over the strong CMLM model. Surprisingly, our proposed model yields state-of-the-art performance on neural machine translation ( 34.62 BLEU on WMT16 EN \u2192 RO, 34.82 BLEU on WMT16 RO \u2192 EN, and 34.84 BLEU on IWSLT De \u2192 En) and even better performance than the AR Transformer on 7 benchmark datasets with at least 2.2 \u00d7 speedup. Our code is available at GitHub 1 ."}
{"introduction": "operations has received increasing attention in recent years. There are two major advantages of insertion-based generation over the prevalent left-to-right auto-regressive paradigm: 1) It reduces the decoding cost to sub-linear w.r.t. the sequence length with parallel decoding (Stern et al., 2019; Gu et al., 2019b), and 2) the \ufb02exible insertion orders may better recover/utilize the underlying linguistic structures of languages (Welleck et al., 2019; Gu et al., 2019a). ef\ufb01ciency. Unlike left-to-right auto-regressive decoders which monotonically expand the context, the insertion operations complicate the position information of each token as the context expands. along with the insertion operations. As a result, a naive implementation of insertion-based models (e.g., Stern et al. (2019); Gu et al. (2019b)) needs to re-encode the context with updated positional information for each token as the insertions proceed, yielding inef\ufb01cient training with O ( n ) times of context re-encoding (with n indicating the sequence length). (InsT) (Stern et al., 2019) and Levenshtein Transformer (LevT) (Gu et al., 2019b) propose parallel token insertion to reduce the insertion/re-encoding steps from O ( n ) to \u0398(log n ) for both training and inference. However, while it works well for machine translation, such parallel insertion falls short on high-entropy generation tasks such as open-domain dialogue systems(Li et al., 2017a), creative 1 The code will be publicly released soon on github. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2102.11008v4  [cs.CL]  15 Oct 2022 pos 0 B pos 1 D pos 2 F pos 0 pos 1 B pos 1 pos 3 D pos 4 E pos 0 A pos 2 C pos 2 pos 5 F Step t Step t+1 of each token is volatile for insertion-based models. Thus, naive models with absolute position encoding have to re-encode the se- quence after each insertion operation during training. Model #Re-Enc. #Re-Enc. PosInfo w/ ParaDec w/ SeqDec Ins. Trans. \u0398(log n ) O ( n ) Absolute Lev. Trans. \u0398(log n ) \u2126 ( n ) Absolute NMSTG O ( n ) O ( n ) Markovian /Absolute InDIGO N/A O (1) Direction-only I NS N ET (Ours) O (1) O (1) Relative NS N ET and existing insertion-based models regard- ing training-time re-encoding steps ( #Re-Enc. columns) and how the models encode posi- tional information (the column). generation such as stories (Yao et al., 2019; Goldfarb-Tarrant et al., 2020; Han et al., 2022), poetry (Manurung et al., 2000; Tian and Peng, 2022), and humor generation (Hempelmann, 2008; He et al., 2019; Mittal et al., 2022; Tian et al., 2022; Sun et al., 2022), where the output is open-ended and the tokens are usually highly dependent on each other, thus cannot be inserted in parallel. Another thread of work seeks to maintain sequential insertions but simplify the position encoding for the tokens so that they do not change as the insertion operations proceed (Gu et al., 2019a) to save computation. NS N ET , which addresses the training ef\ufb01ciency issue of insertion-based text generators by enabling computation sharing similar as that in vanilla decoder transformers. is an insertion-oriented, relative positional encoding coined offset that allows I NS N ET to achieve computation sharing without compromising model capacity. At each insertion step, previously computed position encodings of the existing tokens remain unchanged, while the position encodings of newly inserted tokens accurately recording the updated pairwise token spatial relations of all the inserted tokens. A corresponding offset can be ef\ufb01ciently computed for any given insertion order with a novel process named offset compression . In this way, we avoid expensive re-encoding of the context, and based on the encoded context, our second contribution is the design of an effective aggregation strategy that allows us to parallelly generate expressive slot representations for every slot in the partially generated sequence, \ufb02exibly support both sequential and parallel decoding. , inspired by the layerization idea of Dinic\u2019s algorithm(Dinitz, 2006), we propose an algorithm for I NS N ET to better determine the parallelization of insertion operations in order to reduce the likelihood discrepancy before and after the parallelization. With all the components, I NS N ET is a novel framework that enables ef\ufb01cient training, \ufb02exible decoding, and expressive positional encoding. NS N ET and all prior insertion-based models. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing I NS N ET, an insertion-based text generator that enables efficient training and flexible decoding while maintaining expressive positional encoding.\n2. Proposing a novel offset compression process that allows I NS N ET to achieve computation sharing without compromising model capacity.\n3. Designing an effective aggregation strategy that allows for the parallel generation of expressive slot representations for every slot in the partially generated sequence.\n4. Proposing an algorithm for I NS N ET to better determine the parallelization of insertion operations in order to reduce the likelihood discrepancy before and after the parallelization.", "title": "INSNET: An Ef\ufb01cient, Flexible, and Performant Insertion-based Text Generation Model", "words": 1055, "abstract": "NS N ET , an expressive insertion-based text generator with ef\ufb01cient training and \ufb02exible decoding (parallel or sequential). insertion-based text generation works that require re-encoding of the context after each insertion operation and thus are inef\ufb01cient to train, I NS N ET only requires one pass of context encoding for the entire sequence during training by introducing a novel insertion-oriented position encoding and a light-weighted slot representation strategy to enable computation sharing. Furthermore, we propose an algorithm I NS N ET -Dinic to better determine the parallelization of insertion operations that provides a controllable switch between parallel and sequential decoding, making it \ufb02exible to handle more parallelizable tasks such as machine translation with ef\ufb01cient decoding, or less parallelizable tasks such as open-domain text generation to guarantee high-quality outputs. text generation datasets and three machine translation datasets demonstrate I N - S N ET \u2019s advantages over previous insertion-based methods in terms of training speed, inference ef\ufb01ciency, and generation quality. 1 1"}
{"introduction": "Pre-trained models, like BERT (Devlin et al. 2018), UniLM (Dong et al. 2019) and RoBERTa (Liu et al. 2019) have been widely applied on natural language process tasks by transferring the knowledge learned from large amount of unlabeled corpus to downstream tasks. Some pre-trained models (Dong et al. 2019; Song et al. 2019) also have proven effective for natural language generation tasks in autoregres- sive generation. However, low latency is required by an in- creasing number of real-world generating applications, such as online query rewriting in search engines, limiting the use of these autoregressive models (Mohankumar, Begwani, and Singh 2021). Non-autoregressive models (Gu et al. 2017; Gu, Wang, and Zhao 2019; Ghazvininejad et al. 2019) are proposed to reduce latency by predicting whole sequence simul- taneously. Compared with autoregressive models, non- * Work done during internship at microsoft. \u2020 Corresponding Author autoregressive models can achieve more than ten times speedup at the cost of accuracy loss (Gu et al. 2017). We observe that little progress has been made to directly \ufb01ne- tune an off-the-shelf pre-trained encoder model for non- autoregressive generation. How to effectively leverage them into non-autoregressive generation tasks remains a non- trivial problem. To fully exploit the power of pre-trained models, we pro- pose a new iterative decoding method by mixing source and pseudo target called MIST. Current iterative decoding meth- ods such as Mask-Predict (Ghazvininejad et al. 2019) or Levenshtein transformer (Gu, Wang, and Zhao 2019) focus on improving decoding performance by multiple decoding iterations. However, these methods sacri\ufb01ce the inference speed. MIST is a simple and effective iterative training strat- egy that works during the training stage and has no effect on inference speed. During the training stage, the model pre- dicts the entire target sequence \ufb01rst, then we treat the gen- erated target sequence as part of the source tokens and feed the source and pseudo target as source into the model. MIST can be regarded as a dynamical data augmentation method in the training stage. Unlike the traditional data aug- mentation method, which needs to prepare data before train- ing, MIST enables dynamical data augmentation in the train- ing stage. The term \u201cdynamic\u201d in MIST refers to the fact that as model training progresses, training data will be changed per epoch. For example, MIST can ensure that the quality of the pseudo target matches the current training model and prevent model over\ufb01tting with static pseudo targets gener- ated. For increased data, pseudo targets can provide insight into which tokens can be successfully predicted and help models focus on what is done incorrectly. These Pseudo tar- gets also enables the conditional dependence to help conver- gence. As our experiments show, we evaluate our method on three generation tasks including question generation, sum- marization and paraphrase generation. Our method achieves signi\ufb01cant performance improvements on all tasks with the lower latency than NAT (Gu et al. 2017). To further eval- uate the effect of our method, we compare a variety of pre-trained models including BERT (Devlin et al. 2018), UniLM (Dong et al. 2019), and MiniLM (Wang et al. 2020) in non-autoregressive setting. The experiments show that arXiv:2110.11115v1  [cs.CL]  21 Oct 2021 our method achieves consistent gains in different generation tasks and pre-trained models. Our contributions are listed as follows: \u2022 We propose a new paradigm, adopting pre-trained en- coder for non-autoregressive generation without modify- ing model architectures. \u2022 We propose a simple and novel training method to im- prove the performance. \u2022 We empirically verify the effectiveness of our method in different generation tasks and pre-trained models.", "contribution": "- The paper proposes a new paradigm for adopting pre-trained encoder models for non-autoregressive generation tasks without modifying model architectures.\n- The paper proposes a simple and novel training method called MIST to improve the performance of pre-trained encoder models for non-autoregressive generation tasks.\n- The paper empirically verifies the effectiveness of the proposed method in different generation tasks and pre-trained models.", "title": "Improving Non-autoregressive Generation with Mixup Training", "words": 863, "abstract": "Abstract While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into non- autoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST). Unlike other iterative de- coding methods, which sacri\ufb01ce the inference speed to achieve better performance based on multiple decod- ing iterations, MIST works in the training stage and has no effect on inference time. Our experiments on three generation benchmarks including question gener- ation, summarization and paraphrase generation, show that the proposed framework achieves the new state-of- the-art results for fully non-autoregressive models. We also demonstrate that our method can be used to a va- riety of pre-trained models. For instance, MIST based on the small pre-trained model also obtains comparable performance with seq2seq models. Our code is available at https://github.com/kongds/MIST."}
{"introduction": "Neural machine translation (NMT), as the state-of- the-art machine translation paradigm, has recently been approached with two different sequence de- coding strategies. The \ufb01rst type autoregressive \u2217 The \ufb01rst two authors contributed equally to this work. This work was conducted when Yongchang Hao, Shilin He, and Wenxiang Jiao were interning at Tencent AI Lab. 1 Code is publicly available at https://github. com/yongchanghao/multi-task-nat translation (AT) models generate output tokens one by one following the left to right direction ( Vaswani et al. , 2017 ; , 2015 ), but it is often criticized for its slow inference speed ( , 2018 ). The second type non-autoregressive transla- tion (NAT) models adopt a parallel decoding algo- rithm to produce output tokens simultaneously ( Gu et al. , 2019 ; , 2019 ; , 2020 ), but the translation quality of it is often infe- rior to auto-regressive models ( Gu et al. , 2018 ). Many researchers have investigated the collabo- ration between AT and NAT models. For instance, E NCODER -NAD-AD ( Zhou et al. , 2020 ) leverages NAT models to improve the performance of AT. between the conventional AT encoder and decoder to generate coarse target sequences for the \ufb01nal autoregressive decoding. A line of research ( Wang et al. , 2019b ; , 2020 ; , 2020 ) holds the opinion that the lack of contextual de- pendency on target sentences potentially leads to the deteriorated performance of NAT models. To boost the NAT translation performance, many re- cent works resort to the knowledge transfer from a well-trained AT model. Typical knowledge transfer methods include sequence-level knowledge distil- lation with translation outputs generated by strong AT models ( , 2019 ; , 2019 ), word-level knowledge distillation with AT decoder representations ( Wei et al. , 2019 ; Li et al. , 2019 ), and \ufb01ne-tuning on AT model by curriculum learning ( Guo et al. , 2020 ), etc. that AT and NAT encoders \u2013 although they belong to the same sequence-to-sequence learning task \u2013 capture different linguistic properties of source sentences. We conduct our veri\ufb01cation by evaluat- ing the encoder on a set of probing tasks ( Conneau 3990 Task AT NAT Surface SeLen 91.7 93.4 WC 76.0 79.1 Syntactic TrDep 45.8 46.0 ToCo 78.3 79.7 BShif 74.8 73.4 Semantic Tense 89.2 89.2 SubN 86.2 87.5 ObjN 85.2 85.3 SoMo 54.0 53.0 CoIn 64.9 62.8 Table 1: Performance on the probing tasks of evaluat- ing linguistic properties embedded in the learned repre- sentations of AT and NAT models. et al. , 2018 ; Raganato and Tiedemann , 2018 ) for AT and NAT models. Further, by leveraging the linguis- tic differences, we then adopt a multi-task learning framework with a shared encoder (i.e., M ULTI - T ASK NAT) to transfer the AT model knowledge into the NAT model. Speci\ufb01cally, we employ an additional AT task as the auxiliary task of which the encoder parameters are shared with the NAT task while parameters of the decoder are exclusive. , 2018 ; , 2019 ) suggest that the weights for each task are critical to the multi-task learning, in this work, the multi-task weight assigned to the AT task is dynam- ically annealed from 1 to 0 . We name this scheme importance annealing . We empirically show the bene\ufb01t of importance annealing in both directions of the original WMT14 English \u21d4 German dataset. posed M ULTI -T ASK NAT achieves signi\ufb01cant im- provements on WMT14 English \u21d4 WMT16 English \u21d4 \ufb01rms the effectiveness of our proposed model on machine translation tasks. Our contributions are as follows: \u2022 to boost NAT translation quality by transfer- ring the AT knowledge to the NAT model. \u2022 is necessary for capturing more linguistic and semantic information. \u2022 Experiments on standard benchmark datasets demonstrate the effectiveness of the proposed M ULTI -T ASK NAT. 2", "contribution": "The main contributions of this paper are:\n\n1. Proposing a multi-task learning framework, called MULTI-TASK NAT, to transfer the knowledge from an autoregressive (AT) model to a non-autoregressive (NAT) model for improving the NAT translation quality.\n\n2. Showing that the linguistic and semantic information captured by the AT model is necessary for improving the NAT translation quality.\n\n3. Introducing a dynamic annealing scheme, called importance annealing, for assigning multi-task weights to the AT task, which improves the performance of the proposed MULTI-TASK NAT model.\n\n4. Conducting experiments on standard benchmark datasets, including WMT14 English \u21d4 German and WMT16 English \u21d4 Romanian, to demonstrate the effectiveness of the proposed MULTI-TASK NAT model.", "title": "Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation", "words": 958, "abstract": "Non-Autoregressive machine Translation (NAT) models have demonstrated signi\ufb01cant inference speedup but suffer from inferior translation accuracy. The common prac- tice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties of source sentences. Therefore, we propose to adopt multi-task learning to transfer the AT knowledge to NAT models through encoder sharing. Speci\ufb01cally, we take the AT model as an auxiliary task to enhance NAT model performance. Experi- mental results on WMT14 English \u21d4 German and WMT16 English \u21d4 Romanian datasets show that the proposed MULTI - TASK NAT achieves signi\ufb01cant improvements over the baseline NAT models. Furthermore, the performance on large-scale WMT19 and WMT20 English \u21d4 German datasets con\ufb01rm the consistency of our proposed method. In addition, experimental results demonstrate that our MULTI - TASK NAT is complemen- tary to knowledge distillation, the standard knowledge transfer method for NAT.  1 1"}
