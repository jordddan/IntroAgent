Natural language generation has been a challenging task in the field of machine learning due to the complexity of language structures and the need for generating coherent and meaningful sentences. Existing models for natural language generation, such as autoregressive (AR) and non-autoregressive (NAR) models, have their limitations. AR models generate sentences sequentially, which can be time-consuming, while NAR models generate sentences in parallel, but often suffer from lower quality outputs. 

The objective of this paper is to propose a new large-scale pretraining model, BANG, that bridges the gap between AR and NAR models by supporting NAR and semi-NAR generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder that enables parallelization and supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG also supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

Related work in the field has explored various approaches to address the limitations of AR and NAR models. JANUS proposed a joint AR and NAR training method, while XLNet introduced a generalized autoregressive pretraining method. ProphetNet introduced a novel self-supervised objective, and MPNet leveraged the dependency among predicted tokens through permuted language modeling. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigated the performance gap between AR and NAR models, and Universal Conditional Masked Language Pre-training proposed a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora.

The main contributions of this paper are fourfold. Firstly, BANG is proposed as the first large-scale pretraining model designed for NAR and semi-NAR generation, bridging the gap between AR and NAR models. Secondly, BANG is pretrained using an efficient cross-stream visible n-stream decoder that enables parallelization and supports predicting tokens with arbitrary previous golden tokens or [MASK]. Thirdly, BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Finally, BANG achieves significant performance improvements on all tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with strong AR pretrained models.

In this paper, we provide a detailed explanation of the proposed approach and its components, including technical details and precision to support the approach and how it addresses the identified research gaps. We also preview the experimental setup and results, including the methodology and data used, with a clear structure and logical flow, and how they support the proposed approach. The methodology and experimental setup provide sufficient detail to enable replication of the study.