Machine translation has been a popular research topic in the field of natural language processing (NLP) for decades. Despite significant progress, machine translation still faces challenges in handling low-resource languages and achieving high translation quality. In recent years, pre-training models have shown great potential in improving machine translation performance. However, most pre-training models are designed for either autoregressive NMT (AT) or non-autoregressive NMT (NAT) tasks, and few models can provide unified initialization parameters for both tasks.

In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. We also introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders.

Our proposed model is evaluated on both low-resource and high-resource settings, and the results show that CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our work contributes to the development of a unified pre-training model for machine translation that can improve translation quality and handle low-resource languages.

Related works in this field include BANG, ProphetNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, JANUS, XLNet, MPNet, and Improving Non-Autoregressive Translation Models Without Distillation. Our work differs from these works in terms of the proposed model architecture and the techniques used for model training. We provide a comprehensive review of related work and highlight the main differences from our work.

The main contributions of this paper are the development of CeMAT, the introduction of aligned code-switching & masking and dynamic dual-masking techniques, and the demonstration of the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. The research questions and objectives are clearly stated, and the methodology and data used are described. The experimental setup and results are presented, and a clear and concise roadmap for the rest of the paper is provided.