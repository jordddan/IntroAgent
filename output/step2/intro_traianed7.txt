Machine translation has been a long-standing challenge in the field of natural language processing. While autoregressive (AR) models have achieved impressive results, non-autoregressive (NAR) models have been proposed to accelerate inference by translating blocks of tokens in parallel. However, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC) that addresses the problems of indistinguishability of tokens and mismatch between training and inference. Our proposed model achieves state-of-the-art NAR performance when trained on raw data without distillation, and approaches AR performance on multiple datasets. 

Our work builds upon related works such as BANG, ProphetNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, JANUS, XLNet, and MPNet. BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Universal Conditional Masked Language Pre-training demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both AR and NAR NMT. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. JANUS is a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. 

Our main contributions are four-fold. First, we propose the Conditional Masked Language Model with Correction (CMLMC) which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. Second, we modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Third, we propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. Fourth, we achieve new state-of-the-art undistilled NAR results and approach AR performance on multiple NMT benchmarks. 

In this paper, we aim to answer the research questions of whether our proposed CMLMC model can improve the performance of NAR machine translation and how it compares to existing state-of-the-art models. We describe the methodology and data used, and provide a description of the experimental setup and results. The rest of the paper is structured as follows: Section 2 provides an overview of related work. Section 3 describes the proposed CMLMC model in detail. Section 4 presents the experimental setup and results. Section 5 provides a discussion of the results and Section 6 concludes the paper.