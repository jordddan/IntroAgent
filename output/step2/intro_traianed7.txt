Natural language generation is a challenging task in machine learning, with existing models facing limitations in terms of performance and efficiency. In this paper, we propose a novel approach to machine translation that addresses these limitations by pre-training a bidirectional encoder-decoder model on both monolingual and bilingual corpora. Our approach, called CeMAT, provides unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

Related work in the field has explored various approaches to machine translation, including autoregressive and non-autoregressive models. However, these models have limitations in terms of performance and efficiency, and there is a need for a more effective approach. Our proposed approach builds on existing work by introducing aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders.

The main contributions of this paper are threefold. Firstly, we introduce CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. Secondly, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Thirdly, we demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings.

Our proposed approach is motivated by the need for a more effective and efficient approach to machine translation. We provide a detailed explanation of the technical details and precision of our approach, including how it addresses the identified research gaps. We also preview the experimental setup and results, including the methodology and data used, with a clear structure and logical flow, and how they support the proposed approach. 

In summary, this paper proposes a novel approach to machine translation that addresses the limitations of existing models. Our approach, CeMAT, provides a more effective and efficient approach to machine translation, with demonstrated improvements in performance on both low-resource and high-resource settings.