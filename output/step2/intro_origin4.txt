The field of natural language generation (NLG) has seen significant advancements in recent years, with the development of large-scale pre-trained models such as BERT, T5, and PEGASUS. However, these models are limited in their ability to plan for future tokens and prevent overfitting on strong local correlations. To address these limitations, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which utilizes a novel self-supervised objective future n-gram prediction. 

Our proposed method simultaneously predicts the future n-gram at each time step during the training phase, encouraging the model to plan for future tokens and preventing overfitting on strong local correlations. Additionally, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. 

We pre-trained ProphetNet on two scale pre-trained datasets and achieved new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, we fine-tuned ProphetNet on several NLG tasks and achieved the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Our work is motivated by the need for more advanced NLG models that can plan for future tokens and prevent overfitting on strong local correlations. The proposed ProphetNet model addresses these limitations and achieves state-of-the-art results on several NLG tasks. The relevance and importance of our work to the AI community lie in its potential to improve the quality and efficiency of NLG systems. 

In terms of related work, our approach builds upon previous work in the field of pre-trained models such as BERT, T5, and PEGASUS. However, our proposed method differs significantly in its use of future n-gram prediction and n-stream self-attention, which enable the model to plan for future tokens and prevent overfitting on strong local correlations.