The field of natural language processing has seen significant advancements in recent years, particularly in the area of sequence generation tasks. Autoregressive (AR) models have been widely used for sequence generation, but they suffer from slow decoding speed due to their sequential nature. Non-autoregressive (NAR) models, on the other hand, can generate sequences in parallel, but they often sacrifice quality for speed. Bridging the gap between AR and NAR models is an important research direction, and our work proposes a novel solution to this problem.

In this paper, we introduce BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. This allows BANG to bridge the gap between AR and NAR models by pretraining a generative model that can support both NAR and AR finetuning.

Our proposed model supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. We evaluate BANG on various tasks, including question generation, summarization, and dialogue generation, and show that it achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning. For AR finetuning, BANG can attain comparable performance with strong AR pretrained models.

Our work builds upon prior research in this area, including JANUS, XLNet, and ProphetNet, which have proposed joint AR and NAR training methods, generalized autoregressive pretraining, and future n-gram prediction, respectively. However, our proposed model differs from these approaches in several key ways, including the use of a cross-stream visible n-stream decoder and the ability to support both NAR and AR finetuning.

In summary, our work proposes a novel solution to bridge the gap between AR and NAR models for sequence generation tasks. Our proposed model, BANG, achieves significant performance improvements on various tasks and supports both NAR and AR finetuning. This work has important implications for the AI community, as it provides a new approach to improve the quality and speed of sequence generation models.