Machine translation has been a long-standing challenge in the field of artificial intelligence. With the increasing demand for multilingual communication, there is a growing need for accurate and efficient translation systems. In recent years, pre-training models have shown great potential in improving the performance of machine translation. However, most pre-training models are designed for either Autoregressive NMT (AT) or Non-autoregressive NMT (NAT) tasks, and there is a lack of a unified model that can provide initialization parameters for both tasks.

In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. We also introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders.

Our proposed model is evaluated on both low-resource and high-resource settings, and the results show consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Compared to the related works, our model achieves up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT, and up to +5.3 BLEU for Non-autoregressive NMT. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks.

Related works in this field include BANG, ProphetNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, JANUS, XLNet, MPNet, and Improving Non-Autoregressive Translation Models Without Distillation. While these works have made significant contributions to the field, our proposed model differs in its ability to provide unified initialization parameters for both AT and NAT tasks, and the introduction of aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders.

In summary, our proposed CeMAT model provides a promising solution to the challenge of machine translation, and our experimental results demonstrate its effectiveness in improving translation performance on both low-resource and high-resource settings.