Machine translation has been a long-standing challenge in the field of artificial intelligence. While autoregressive (AR) models have achieved impressive results, they suffer from slow inference times due to their sequential nature. Non-autoregressive (NAR) models have been proposed as a solution to this problem, but they still lag behind AR models in terms of performance. Recent work has explored the use of distillation to improve NAR models, but this approach has limitations. In this paper, we propose a new approach to NAR machine translation that does not rely on distillation.

Our main contribution is the Conditional Masked Language Model with Correction (CMLMC), which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. We modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. Our approach achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks.

To provide context for our work, we briefly review related work in the field. BANG proposes a new pre-training model to bridge the gap between AR and NAR generation. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms. JANUS is a joint AR and NAR training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. Finally, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes CMLMC as a solution.

Our work differs from previous approaches in that it does not rely on distillation to improve NAR performance. Instead, we propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. We also modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Our approach achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks.