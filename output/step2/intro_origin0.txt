Recent advances in natural language processing have led to the development of large-scale pretraining models that have significantly improved the performance of various NLP tasks. However, most of these models are autoregressive, which means they generate one token at a time, leading to slow inference times. Non-autoregressive models, on the other hand, generate tokens in parallel, resulting in faster inference times. 

In this paper, we propose BANG, a large-scale pretraining model designed for non-autoregressive and semi-non-autoregressive generation. BANG bridges the gap between autoregressive and non-autoregressive models by pretraining a generative model using an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

Our proposed model achieves significant performance improvements on all tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong NAR baselines. 

Our work builds upon previous research in the field of pretraining models, such as ProphetNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, JANUS, XLNet, MPNet, and Improving Non-Autoregressive Translation Models Without Distillation. However, our proposed model differs from these models in several ways. Firstly, BANG is the first large-scale pretraining model designed for non-autoregressive and semi-non-autoregressive generation. Secondly, BANG uses an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. Finally, BANG achieves significant performance improvements on all tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with strong AR pretrained models. 

In summary, our proposed model, BANG, bridges the gap between autoregressive and non-autoregressive models and achieves significant performance improvements on various NLP tasks. Our work contributes to the development of large-scale pretraining models that can support both autoregressive and non-autoregressive generation, leading to faster inference times and improved performance.