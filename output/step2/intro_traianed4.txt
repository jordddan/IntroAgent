Natural language generation (NLG) is a challenging task in the field of machine learning, with the goal of generating coherent and fluent text that conveys a specific message or meaning. Despite significant progress in recent years, existing NLG models still face limitations in terms of scalability, efficiency, and accuracy. In this paper, we propose a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and a method to simultaneously predict the future n-gram at each time step during the training phase. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations.

Our approach extends the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. We pre-trained ProphetNet on two scale pre-trained datasets and achieved new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. We also fine-tuned ProphetNet on several NLG tasks and achieved the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Related work in the field has explored various approaches to address the limitations of existing NLG models. JANUS proposed a joint autoregressive and non-autoregressive training method, while XLNet introduced a generalized autoregressive pretraining method. BANG bridged the gap between autoregressive and non-autoregressive generation, and MPNet proposed a novel pre-training method that inherits the advantages of BERT and XLNet. Improving Non-Autoregressive Translation Models Without Distillation investigated possible reasons behind the performance gap between autoregressive and non-autoregressive models, and Universal Conditional Masked Language Pre-training pre-trained a sequence-to-sequence model with a bidirectional decoder.

The main contributions of our work are: 1) introducing a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction, 2) developing a method to simultaneously predict the future n-gram at each time step during the training phase, 3) extending the two-stream self-attention proposed in XLNet to n-stream self-attention, 4) achieving new state-of-the-art results on CNN/DailyMail and Gigaword, and 5) achieving the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. Our proposed approach has significant implications for the AI community, as it addresses the limitations of existing NLG models and provides a more efficient and accurate approach to generating coherent and fluent text. In the following sections, we provide a detailed explanation of our approach and experimental setup, as well as the results and analysis of our experiments.