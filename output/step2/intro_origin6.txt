The field of machine translation has seen significant progress in recent years, with Transformer-based autoregressive (AR) models achieving state-of-the-art performance on multiple benchmarks. However, AR models have a significant drawback in that they translate one token at a time, making them computationally expensive for long sequences. To address this issue, non-autoregressive (NAR) models have been proposed, which translate blocks of tokens in parallel, resulting in faster inference times. Despite recent progress, leading NAR models still lag behind their AR counterparts and only become competitive when trained with distillation.

In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC), which addresses the limitations of the Conditional Masked Language Model (CMLM) in NAR machine translation. Specifically, we modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence.

Our proposed CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. The significance of our work lies in the fact that it provides a more efficient alternative to AR models without sacrificing translation quality. Our proposed method can be applied to various NAR models, making it a valuable contribution to the field of machine translation.

Related work in this area includes JANUS, XLNet, BANG, Attention Is All You Need, ProphetNet, MPNet, and Universal Conditional Masked Language Pre-training. However, our proposed CMLMC differs from these works in its focus on addressing the limitations of CMLM in NAR machine translation and achieving state-of-the-art undistilled NAR results.