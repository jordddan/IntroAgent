The field of natural language processing has seen significant advancements in recent years, with pre-training models such as BERT and XLNet achieving state-of-the-art results on various downstream tasks. However, these models have limitations, such as neglecting the dependency among predicted tokens and suffering from position discrepancy between pre-training and fine-tuning. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that addresses these limitations and unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM).

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, allowing the model to consider the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB, achieving better results than MLM and PLM by a large margin and outperforming previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Our proposed method builds upon previous works such as BERT and XLNet, which have significantly advanced the field of natural language processing. However, MPNet introduces a novel approach that addresses the limitations of these models and unifies their advantages. Other related works, such as ProphetNet and Universal Conditional Masked Language Pre-training, have also explored new pre-training methods for sequence-to-sequence models. Still, our proposed method differs in its approach to splitting tokens and considering the dependency among predicted tokens.

In summary, our work proposes a new pre-training method that unifies the advantages of MLM and PLM while addressing their limitations. We introduce a novel approach to splitting tokens and considering the dependency among predicted tokens, achieving state-of-the-art results on various downstream tasks. Our work contributes to the advancement of natural language processing and has significant implications for various applications, such as question answering and sentiment analysis.