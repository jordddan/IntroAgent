Machine learning has revolutionized the field of natural language processing (NLP) by enabling computers to understand and generate human language. One of the key challenges in NLP is to develop models that can capture the complex dependencies between words in a sentence. While autoregressive language models have been successful in capturing these dependencies, they suffer from slow inference times due to their sequential nature. On the other hand, non-autoregressive models can generate text in parallel, but they struggle to capture long-range dependencies.

In this paper, we propose XLNet, a generalized autoregressive pretraining method that combines the best of both autoregressive and non-autoregressive models while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach overcomes the limitations of previous pretraining methods, such as BERT, which suffer from the pretrain-finetune discrepancy.

We also propose architectural improvements for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. Additionally, we propose a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Our empirical results demonstrate that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. We also provide a comprehensive review of related work, highlighting the main differences from our work and how our proposed solution fills the gap.

In summary, our contributions include introducing XLNet, a generalized autoregressive pretraining method that overcomes the limitations of previous pretraining methods, proposing architectural improvements for pretraining, and demonstrating superior performance on a wide range of NLP tasks. The rest of the paper is organized as follows: Section 2 provides a detailed review of related work, Section 3 describes our proposed method, Section 4 presents our experimental setup and results, and Section 5 concludes the paper with a summary of our findings and future directions.