The field of natural language processing has seen significant advancements in recent years, particularly in the area of sequence generation tasks. Autoregressive (AR) models have been successful in generating high-quality sequences, but suffer from slow decoding speed due to their sequential nature. Non-autoregressive (NAR) models, on the other hand, can generate sequences in parallel, but often sacrifice quality for speed. Bridging the gap between AR and NAR models is an important research direction, and our work proposes a novel solution to this problem.

In this paper, we introduce BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. This allows BANG to support NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

Our proposed solution addresses the limitations of both AR and NAR models, and achieves significant performance improvements on all tasks for NAR and semi-NAR finetuning. For AR finetuning, BANG can attain comparable performance with strong AR pretrained models. Our work is motivated by the need for a model that can generate high-quality sequences in parallel, and BANG represents a significant step forward in this direction.

Related work in this area includes JANUS, which proposes a joint AR and NAR training method, and XLNet, which introduces a generalized autoregressive pretraining method. However, BANG differs from these approaches in its focus on large-scale pretraining for NAR and semi-NAR generation, and its ability to support AR finetuning. Our work represents a novel contribution to the field of sequence generation, and has important implications for the development of more efficient and effective natural language processing models.