Machine translation is a fundamental task in natural language processing, and recent advances in deep learning have led to significant improvements in translation quality. However, traditional neural network architectures for sequence modeling and transduction problems, such as recurrent networks, suffer from slow training times and limited parallelization. In this paper, we propose a new neural network architecture called the Transformer, which relies entirely on an attention mechanism to draw global dependencies between input and output, without the need for recurrent networks. 

Our contributions in this paper are fourfold. First, we introduce the Transformer architecture, which allows for significantly more parallelization and achieves state-of-the-art translation quality after being trained for as little as twelve hours on eight P100 GPUs. Second, we propose scaled dot-product attention, multi-head attention, and the parameter-free position representation, which are key components of the Transformer architecture. Third, we develop tensor2tensor, a library for training deep learning models in a distributed and efficient manner, which was used to implement and evaluate the Transformer. Finally, we demonstrate the effectiveness of the Transformer on several machine translation tasks, achieving significant improvements in translation quality compared to existing methods.

Related work in this field includes autoregressive and non-autoregressive models, which have played an essential role in sequence generation tasks. Autoregressive models can obtain excellent performance, while non-autoregressive models bring fast decoding speed for inference. However, these models suffer from distribution discrepancy and neglect dependency between the masked positions. To address these issues, recent works have proposed joint autoregressive and non-autoregressive training methods, generalized autoregressive pretraining methods, and bridging autoregressive and non-autoregressive generation methods.

Our proposed approach builds upon these existing methods by introducing a novel neural network architecture that relies entirely on an attention mechanism, allowing for more efficient training and improved translation quality. We pre-train our model using a large corpus and fine-tune it on several machine translation tasks, achieving state-of-the-art results. 

In this paper, we provide a detailed explanation of the proposed approach, including the Transformer architecture, scaled dot-product attention, multi-head attention, and the parameter-free position representation. We also present experimental results to support our claims, comparing the proposed approach with existing methods and addressing the limitations of the study. Our main contribution is the introduction of the Transformer architecture, which achieves state-of-the-art translation quality and allows for more efficient training and improved parallelization. 

The remainder of this paper is organized as follows. In Section 2, we provide a detailed overview of related work in this field. In Section 3, we describe the proposed approach in detail, including pre-training and fine-tuning. In Section 4, we present experimental results to support our claims. Finally, in Section 5, we conclude the paper and discuss future directions for research.