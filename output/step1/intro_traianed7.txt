Machine translation is a fundamental task in natural language processing, and recent advances in deep learning have led to significant improvements in translation quality. However, there are still challenges in achieving high-quality translation, particularly for low-resource languages. In this paper, we propose a novel pre-training model for machine translation, called CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

To address the limitations of existing methods, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages.

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our proposed approach builds upon existing methods and provides a more comprehensive overview of the strengths and weaknesses of existing methods.

In this paper, we present a detailed explanation of the proposed approach, pre-training, and fine-tuning, and provide experimental results to support the claims made in the paper, comparing the proposed approach with existing methods and addressing the limitations of the study. Our main contribution is the development of CeMAT, which can provide a unified initialization for both AT and NAT tasks, and the introduction of aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training. The paper is structured as follows: Section 2 provides an overview of related work, Section 3 presents the proposed approach, Section 4 presents experimental results, and Section 5 concludes the paper.