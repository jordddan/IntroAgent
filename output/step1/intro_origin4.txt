The field of natural language generation (NLG) has seen significant advancements in recent years, with the development of large-scale pre-trained models such as BERT, T5, and PEGASUS. However, these models are limited in their ability to plan for future tokens and prevent overfitting on strong local correlations. To address these limitations, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which utilizes a novel self-supervised objective future n-gram prediction. 

Our proposed method simultaneously predicts the future n-gram at each time step during the training phase, encouraging the model to plan for future tokens and preventing overfitting on strong local correlations. Additionally, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. 

We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Our work is motivated by the need for more advanced NLG models that can plan for future tokens and prevent overfitting on strong local correlations. The importance and relevance of our work lies in its potential to significantly improve the performance of NLG models, particularly in tasks such as summarization and question generation. Our proposed solution, ProphetNet, addresses these limitations and achieves new state-of-the-art results on several benchmarks. 

In the context of related work, our approach differs from previous models such as BERT, T5, and PEGASUS in its use of future n-gram prediction and n-stream self-attention. Additionally, our model achieves better performance than previous state-of-the-art models such as BART and XLNet, using significantly less pre-training epochs and corpus.