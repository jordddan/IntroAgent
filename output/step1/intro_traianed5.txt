Machine learning has revolutionized the field of natural language processing (NLP) by enabling computers to understand and generate human language. Pre-training models on large-scale text corpora has become a popular approach to improve the performance of NLP tasks. However, existing pre-training methods such as Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have limitations that hinder their effectiveness. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations.

Our proposed approach splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-trained MPNet on a large-scale text corpus and fine-tuned it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

In the related works, we reviewed several pre-training models such as JANUS, XLNet, BANG, Attention Is All You Need, ProphetNet, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, and Universal Conditional Masked Language Pre-training. We compared our proposed approach with these models and highlighted the gaps in the literature. Our proposed approach addresses the limitations of existing pre-training methods and achieves state-of-the-art performance on various NLP tasks.

The main contributions of this paper are: 1) proposing a new pre-training method called MPNet that unifies the advantages of MLM and PLM while addressing their limitations, 2) introducing a new approach that splits the tokens in a sequence into non-predicted and predicted parts, and 3) pre-training MPNet on a large-scale text corpus and fine-tuning it on various downstream benchmark tasks, achieving state-of-the-art performance. The rest of the paper is organized as follows: Section 2 provides a detailed review of related work, Section 3 explains the proposed method in detail, Section 4 presents the experimental setup and results, and Section 5 concludes the paper and discusses future work.