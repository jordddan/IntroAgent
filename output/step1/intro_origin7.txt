Machine translation has been a long-standing challenge in the field of artificial intelligence, with the goal of achieving human-level accuracy and fluency. Recent advances in deep learning have led to significant improvements in machine translation performance, particularly with the use of neural machine translation (NMT) models. However, NMT models still face challenges in handling low-resource languages and achieving fast inference speed.

To address these challenges, this paper proposes a novel pre-training model for machine translation called CeMAT. CeMAT consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. It is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. Additionally, aligned code-switching & masking and dynamic dual-masking techniques are introduced to enhance the model training under the setting of bidirectional decoders.

The effectiveness of CeMAT is demonstrated through extensive experiments on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks.

Related works in the field of machine translation include JANUS, XLNet, BANG, Attention Is All You Need, ProphetNet, MPNet, and CMLMC. JANUS proposes a joint autoregressive and non-autoregressive training method, while XLNet introduces a generalized autoregressive pretraining method. BANG bridges autoregressive and non-autoregressive generation with a novel model structure for large-scale pretraining. Attention Is All You Need proposes a new simple network architecture based solely on attention mechanisms. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet leverages the dependency among predicted tokens through permuted language modeling, and takes auxiliary position information as input to make the model see a full sentence. CMLMC investigates possible reasons behind the performance gap between autoregressive and non-autoregressive approaches and proposes a conditional masked language model with correction.

Compared to these related works, CeMAT stands out for its pre-training on both monolingual and bilingual corpora, its use of aligned code-switching & masking and dynamic dual-masking techniques, and its consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.