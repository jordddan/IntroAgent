Machine translation has been a long-standing challenge in the field of artificial intelligence. With the increasing demand for multilingual communication, there is a growing need for accurate and efficient translation systems. In recent years, neural machine translation (NMT) has emerged as a promising approach, achieving state-of-the-art performance on various language pairs. However, NMT models still face challenges in handling low-resource languages and achieving fast inference speed.

In this paper, we propose CeMAT, a pre-training model for machine translation that addresses these challenges. CeMAT consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. It is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. Our proposed model is designed to improve the performance of machine translation on both low-resource and high-resource settings.

To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques are simple but effective in improving the performance of our proposed model.

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our proposed model is a significant step towards improving the accuracy and efficiency of machine translation systems.

Related work in the field of NMT includes JANUS, XLNet, BANG, ProphetNet, MPNet, and CMLMC. While these models have made significant contributions to the field, our proposed model differs in its pre-training approach, bidirectional decoder, and the use of aligned code-switching & masking and dynamic dual-masking techniques. 

In summary, this paper proposes CeMAT, a pre-training model for machine translation that addresses the challenges of low-resource languages and fast inference speed. Our proposed model is designed to improve the performance of machine translation on both AT and NAT tasks. The effectiveness of CeMAT is demonstrated through extensive experiments, and our proposed techniques of aligned code-switching & masking and dynamic dual-masking are shown to be simple but effective in improving the performance of our model.