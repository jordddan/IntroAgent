Machine translation is a fundamental task in natural language processing, and it has been widely studied in recent years. Autoregressive (AR) models, such as the Transformer, have achieved state-of-the-art performance in machine translation. However, AR models suffer from slow decoding speed due to their sequential nature. Non-autoregressive (NAR) models, on the other hand, can generate translations in parallel, but they often lag behind AR models in terms of translation quality. Therefore, there is a need to combine the strengths of both AR and NAR models to improve performance.

In this paper, we propose a novel method called JANUS, which combines the strengths of AR and NAR models while avoiding their weaknesses. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Moreover, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

We conduct a comprehensive review of related work, highlighting the gaps in the literature and comparing with existing literature. We compare JANUS with other related works, such as XLNet, BANG, Attention Is All You Need, ProphetNet, MPNet, and Universal Conditional Masked Language Pre-training. We show that JANUS is a novel and effective method that outperforms existing approaches in terms of translation quality and decoding speed.

The main contributions of this paper are: 1) introducing JANUS, a method that combines the strengths of both AR and NAR generation models while avoiding their weaknesses to improve performance; 2) proposing an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other; 3) demonstrating the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average; 4) exceeding the non-autoregressive pretraining model BANG on the same GLGE tasks and achieving comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

The rest of the paper is organized as follows. Section 2 provides a comprehensive review of related work. Section 3 presents the proposed method in detail, including the technical details and experimental setup. Section 4 presents the experimental results and analysis. Finally, Section 5 concludes the paper and discusses future work.