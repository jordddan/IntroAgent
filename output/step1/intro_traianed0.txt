Machine translation is a fundamental task in natural language processing, and recent advances in deep learning have led to significant improvements in translation quality. However, there are still challenges in achieving high-quality translation while maintaining fast inference speed. Autoregressive (AR) models have achieved excellent performance but suffer from slow decoding speed, while non-autoregressive (NAR) models have faster decoding speed but lower translation quality. In this paper, we propose a novel method called JANUS that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

Our contributions are fourfold. First, we introduce JANUS, a method that uses an auxiliary distribution to bridge the discrepancy between AR and NAR models, allowing them to learn from each other. Second, we propose an auxiliary loss to enhance the model performance in both AR and NAR manners simultaneously. Third, we demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Fourth, we exceed the non-autoregressive pretraining model BANG on the same GLGE tasks and achieve comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

We provide a comprehensive overview of related work, highlighting the strengths and weaknesses of existing methods and how the proposed approach builds upon or differs from them. We also present the methodology used to address the research question, including a detailed explanation of the proposed approach, pre-training, and fine-tuning. We then present experimental results to support the claims made in the paper, comparing the proposed approach with existing methods and addressing the limitations of the study. Finally, we provide a clear and concise statement of the main contribution of the paper, emphasizing the novelty and significance of the proposed approach. The paper is organized as follows: Section 2 provides an overview of related work, Section 3 describes the proposed approach, Section 4 presents experimental results, and Section 5 concludes the paper.