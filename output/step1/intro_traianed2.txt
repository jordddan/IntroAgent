Machine learning has made significant progress in recent years, particularly in the field of natural language processing (NLP). One of the most significant challenges in NLP is the generation of high-quality text, which requires a deep understanding of language structure and context. Autoregressive (AR) models have been widely used for text generation, but they suffer from slow inference speed due to their sequential nature. Non-autoregressive (NAR) models have been proposed to address this issue, but they often sacrifice quality for speed. 

In this paper, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pretraining a generative model. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

We conduct a comprehensive review of related work, highlighting the gaps in the literature and comparing with existing literature. Our proposed approach is compared with several state-of-the-art models, including JANUS, XLNet, Attention Is All You Need, ProphetNet, MPNet, and Universal Conditional Masked Language Pre-training. We show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. 

The main contributions of the paper are: 1) proposing BANG as the first large-scale pretraining model designed for NAR and semi-NAR generation, 2) designing an efficient cross-stream visible n-stream decoder to realize parallelization, 3) supporting NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure, and 4) achieving significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. 

The remainder of the paper is organized as follows. Section 2 provides a detailed description of related work. Section 3 presents the proposed BANG model and its technical details. Section 4 describes the experimental setup and results. Section 5 provides a discussion of the results and their implications. Finally, Section 6 concludes the paper and discusses future research directions.