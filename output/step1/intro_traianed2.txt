Machine translation has been a long-standing challenge in the field of natural language processing. While autoregressive models have achieved impressive performance, they suffer from slow decoding speed due to their sequential nature. Non-autoregressive models, on the other hand, offer faster decoding but often sacrifice performance. In this paper, we propose BANG, a novel pretraining model that bridges the gap between autoregressive and non-autoregressive generation. BANG is designed to support non-autoregressive, semi-non-autoregressive, and autoregressive fine-tuning, making it a versatile model that can meet different requirements.

Related work in this field has focused on either autoregressive or non-autoregressive models, with few attempts to bridge the gap between the two. JANUS and XLNet are two notable examples of models that have attempted to address this issue. JANUS proposes a joint autoregressive and non-autoregressive training method, while XLNet introduces a generalized autoregressive pretraining method. However, these models still suffer from limitations such as distribution discrepancy and neglecting dependency between masked positions.

BANG addresses these limitations by proposing an efficient cross-stream visible n-stream decoder for parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG achieves significant performance improvements on all tasks for non-autoregressive and semi-non-autoregressive fine-tuning, and for autoregressive fine-tuning, it can attain comparable performance with strong autoregressive pretrained models.

The main contribution of this paper is the proposal of BANG, the first large-scale pretraining model designed for non-autoregressive and semi-non-autoregressive generation. BANG bridges the gap between autoregressive and non-autoregressive models via pretraining a generative model, and supports different types of fine-tuning to meet various requirements. The experimental results demonstrate the effectiveness of BANG in improving non-autoregressive and semi-non-autoregressive performance, as well as achieving comparable performance with strong autoregressive pretrained models.

The rest of the paper is organized as follows. Section 2 provides an overview of related work in this field. Section 3 presents the methodology used in this study, including a detailed explanation of the proposed approach, pretraining, and fine-tuning. Section 4 presents the experimental results and compares the proposed approach with existing methods. Finally, Section 5 concludes the paper and discusses future research directions.