Machine learning has revolutionized the field of artificial intelligence by enabling computers to learn from data and make predictions or decisions without being explicitly programmed. One of the most significant challenges in machine learning is sequence generation, where the goal is to generate a sequence of tokens that maximizes a given objective function. Autoregressive (AR) models have been widely used for sequence generation, but they suffer from slow inference due to their sequential nature. Non-autoregressive (NAR) models have been proposed to address this issue, but they often sacrifice performance for speed.

In this paper, we propose BANG, a new pretraining model that bridges the gap between AR and NAR generation. BANG is the first large-scale pretraining model designed explicitly for NAR and semi-NAR generation. It is pretrained using an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

Our proposed model addresses the limitations of existing models by achieving significant performance improvements on all tasks for NAR and semi-NAR finetuning. For AR finetuning, BANG can attain comparable performance with strong AR pretrained models. We compare our model with several state-of-the-art models, including JANUS, XLNet, ProphetNet, MPNet, and Universal Conditional Masked Language Pre-training, and show that BANG outperforms them on various tasks.

The main contributions of this paper are fourfold. First, we propose BANG, a novel pretraining model that bridges the gap between AR and NAR generation. Second, we introduce an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. Third, we demonstrate that BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Finally, we show that BANG achieves significant performance improvements on all tasks for NAR and semi-NAR finetuning and can attain comparable performance with strong AR pretrained models for AR finetuning.

In summary, our proposed model, BANG, represents a significant step forward in the field of sequence generation by bridging the gap between AR and NAR generation. Our experimental results demonstrate the effectiveness of our approach and highlight its potential for future research in this area.