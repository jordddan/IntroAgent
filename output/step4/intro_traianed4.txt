Machine learning has revolutionized the field of natural language processing (NLP) by enabling the development of models that can generate human-like text. However, existing models often suffer from overfitting and lack the ability to plan for future tokens, leading to suboptimal performance on complex NLP tasks. In this paper, we propose a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and a method to simultaneously predict the future n-gram at each time step during the training phase. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations.

To provide a comprehensive overview of related work, we discuss the limitations of existing models, including Transformer-based autoregressive and non-autoregressive models, and highlight the gaps in the literature that our research aims to fill. We also introduce related work such as JANUS, XLNet, BANG, Attention Is All You Need, MPNet, and CMLMC, which have contributed to the development of pre-trained models for NLP tasks.

Our research questions focus on developing a pre-trained model that can plan for future tokens and prevent overfitting on strong local correlations. We propose a solution that extends the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. We describe the proposed model and its components, including the experimental setup, evaluation metrics, and limitations of existing models, to provide a clear understanding of our methodology.

Our experimental results show that ProphetNet achieves new state-of-the-art results on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. We also fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. The main contributions of this paper are introducing a new large-scale pre-trained Seq2Seq model, developing a method to plan for future tokens, and achieving state-of-the-art results on several NLG tasks. We discuss the significance of our proposed method and potential future directions for research.