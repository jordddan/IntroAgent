Machine translation has been a long-standing challenge in the field of artificial intelligence. With the increasing demand for multilingual communication, there is a growing need for accurate and efficient translation systems. In recent years, neural machine translation (NMT) has emerged as a promising approach, achieving state-of-the-art performance on various language pairs. However, NMT models still face challenges in low-resource settings and suffer from slow decoding speed in autoregressive NMT (AT) tasks.

To address these challenges, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and non-autoregressive NMT (NAT) tasks. Our proposed model is designed to improve the performance of machine translation in both low-resource and high-resource settings.

To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques are shown to be effective in improving the performance of CeMAT.

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our work is different from previous studies in that we pre-train a unified model for fine-tuning on both AT and NAT tasks, and we introduce novel techniques to enhance the model training under the setting of bidirectional decoders.

In summary, our work proposes a novel pre-training model for machine translation that can improve the performance of both AT and NAT tasks in low-resource and high-resource settings. Our proposed techniques for enhancing the model training under the setting of bidirectional decoders are shown to be effective. Our work contributes to the development of more accurate and efficient machine translation systems, which is of great importance and relevance to the AI community.