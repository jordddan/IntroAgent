Machine learning has revolutionized the field of natural language processing (NLP) by enabling the development of powerful models for various NLP tasks. One of the key challenges in NLP is to develop models that can generate coherent and informative text. In this paper, we propose a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction. The main contributions of this paper are: (1) developing a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations; (2) extending the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction; (3) pre-training ProphetNet on two scale pre-trained datasets and achieving new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS; (4) fine-tuning ProphetNet on several NLG tasks and achieving the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Previous work in this area has focused on developing pre-trained models that can generate coherent and informative text. However, these models often suffer from overfitting on strong local correlations and neglect the dependency between the masked positions. To address these limitations, we propose ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and extends the two-stream self-attention proposed in XLNet to n-stream self-attention. Our proposed model is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. This encourages the model to plan for the future tokens and prevents overfitting on strong local correlations.

Our proposed model is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. We also fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. Our proposed model outperforms previous state-of-the-art models and provides a significant improvement in generating coherent and informative text.

In summary, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and extends the two-stream self-attention proposed in XLNet to n-stream self-attention. Our proposed model achieves state-of-the-art results on several NLG tasks and outperforms previous state-of-the-art models. The rest of the paper is organized as follows: Section 2 provides a detailed overview of related work, Section 3 describes the proposed model and techniques, Section 4 presents the experimental results, and Section 5 concludes the paper.