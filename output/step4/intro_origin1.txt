Recent advances in pre-trained sequence-to-sequence models have significantly improved the performance of natural language generation tasks. However, traditional sequence-to-sequence models optimize one-step-ahead prediction, which may lead to overfitting on strong local correlations and fail to plan for future tokens. To address these issues, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which utilizes a novel self-supervised objective future n-gram prediction. 

ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. Additionally, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. 

We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. 

Our work is motivated by the need for a more effective and efficient pre-trained sequence-to-sequence model that can plan for future tokens and prevent overfitting on strong local correlations. Our proposed solution, ProphetNet, achieves state-of-the-art results on several NLG tasks and outperforms existing pre-trained models such as BART, T5, and PEGASUS. 

Related works such as CeMAT, XLNet, CMLMC, JANUS, BANG, MPNet, and Transformer have also contributed to the development of pre-trained sequence-to-sequence models. However, our work differs from these approaches in terms of the self-supervised objective future n-gram prediction and the n-stream self-attention mechanism. These novel features enable ProphetNet to achieve better performance on NLG tasks and overcome the limitations of existing pre-trained models. 

In summary, our work introduces a new pre-trained sequence-to-sequence model called ProphetNet, which utilizes a novel self-supervised objective future n-gram prediction and n-stream self-attention mechanism. We demonstrate the effectiveness of ProphetNet on several NLG tasks and achieve state-of-the-art results. Our work contributes to the development of more effective and efficient pre-trained sequence-to-sequence models for natural language generation tasks.