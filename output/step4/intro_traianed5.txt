Machine learning has made significant progress in recent years, particularly in the field of natural language processing (NLP). One of the key challenges in NLP is generating high-quality text, which can be achieved through autoregressive (AR) and non-autoregressive (NAR) models. While AR models have achieved excellent performance, they suffer from slow decoding speed, making them unsuitable for real-time applications. On the other hand, NAR models can generate text in parallel, but they often produce lower quality text due to the lack of sequential dependencies.

To address this challenge, we propose BANG, a large-scale pretraining model designed for NAR and semi-NAR generation. BANG bridges the gap between AR and NAR by pretraining a generative model using an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

Our proposed model achieves significant performance improvements on all tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with strong AR pretrained models. We compare our model with related works, including Universal Conditional Masked Language Pre-training, ProphetNet, XLNet, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, JANUS, MPNet, and Attention Is All You Need. Our model outperforms these related works in terms of performance and efficiency.

In this paper, we aim to bridge the gap between AR and NAR generation by proposing a novel model structure for large-scale pretraining. We demonstrate the effectiveness of our proposed model through extensive experiments on question generation, summarization, and dialogue generation tasks. Our proposed model achieves significant improvements in NAR and semi-NAR performance, as well as comparable performance with strong AR pretrained models. The rest of the paper is organized as follows: Section 2 provides an overview of related work, Section 3 describes our proposed model in detail, Section 4 presents experimental results, and Section 5 concludes the paper with a summary of our contributions and future work.