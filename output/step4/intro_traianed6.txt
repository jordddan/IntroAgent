Machine translation has been a long-standing challenge in the field of artificial intelligence, with the goal of developing models that can accurately translate text from one language to another. While autoregressive (AR) models have achieved impressive results, they suffer from slow inference times due to their sequential nature. Non-autoregressive (NAR) models have been proposed to address this issue, but they often lag behind their AR counterparts in terms of performance. In this paper, we propose a novel approach to NAR machine translation that achieves state-of-the-art results without the need for distillation.

Previous work has explored various NAR approaches, including the Conditional Masked Language Model (CMLM). However, CMLM suffers from the indistinguishability of tokens and a mismatch between training and inference. To address these issues, we propose the Conditional Masked Language Model with Correction (CMLMC), which incorporates a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. We also modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens.

Our proposed method achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. We provide a comprehensive overview of related work, highlighting the gaps in the literature that our research aims to fill. We also discuss the main contributions and significance of the paper, emphasizing how the proposed method addresses the research questions and contributes to the field. Finally, we present experimental results that support the effectiveness of our proposed method in addressing the research questions. Our work has significant implications for the development of more efficient and accurate NAR machine translation models, and we discuss potential future directions for research in this area.