Machine translation has been a long-standing challenge in the field of artificial intelligence, with the goal of achieving human-level accuracy in translating natural language. Recent advances in pre-training models have significantly improved the performance of neural machine translation (NMT) systems. However, most pre-trained models adopt an unidirectional decoder, which limits their ability to capture bidirectional dependencies in the input sequence. 

To address this limitation, this paper proposes a novel pre-training model called CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. 

In addition, this paper introduces aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. 

The effectiveness of CeMAT is demonstrated through extensive experiments on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Compared to related works, CeMAT is the first to pre-train a unified model for fine-tuning on both NMT tasks. 

Related works in this field include Universal Conditional Masked Language Pre-training, ProphetNet, XLNet, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, JANUS, BANG, MPNet, and Attention Is All You Need. While these works have made significant contributions to the field of NMT, CeMAT stands out for its bidirectional decoder and the use of aligned code-switching & masking and dynamic dual-masking techniques. 

In summary, this paper proposes a novel pre-training model, CeMAT, that significantly improves the performance of NMT systems. The proposed model and techniques are demonstrated to be effective in both low-resource and high-resource settings, and outperform related works in the field.