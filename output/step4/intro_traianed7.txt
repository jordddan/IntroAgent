Machine translation has been a long-standing challenge in the field of artificial intelligence, with the goal of developing models that can accurately translate text from one language to another. Despite significant progress in recent years, there are still many challenges to be addressed, particularly in low-resource settings where training data is limited. In this paper, we propose a novel pre-training model for machine translation, CeMAT, which aims to improve translation performance in both autoregressive and non-autoregressive settings.

Related work in the field of machine translation has focused on developing autoregressive and non-autoregressive models, each with its advantages and limitations. Autoregressive models generate translations one word at a time, while non-autoregressive models generate translations in parallel. However, both approaches have limitations, such as slow inference times for autoregressive models and lower accuracy for non-autoregressive models. Our proposed model aims to address these limitations by providing a unified initialization for both autoregressive and non-autoregressive models.

CeMAT consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module that bridges them. The model is pre-trained on both monolingual and bilingual corpora, and we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings, achieving consistent improvements over strong competitors in both autoregressive and non-autoregressive tasks with data of varied sizes.

The main contributions of this paper are the development of CeMAT, the introduction of aligned code-switching & masking and dynamic dual-masking techniques, and the demonstration of the effectiveness of CeMAT in improving machine translation performance. Our proposed model addresses the limitations of existing models and provides a unified initialization for both autoregressive and non-autoregressive models. The experimental results show that CeMAT achieves significant improvements in translation performance, particularly in low-resource settings. We believe that our proposed model has the potential to advance the field of machine translation and open up new avenues for future research.