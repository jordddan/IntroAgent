Machine translation has been a long-standing challenge in the field of natural language processing. While autoregressive (AR) models have achieved impressive results, they suffer from slow inference times due to their sequential nature. Non-autoregressive (NAR) models have been proposed to address this issue, but they still lag behind AR models in terms of performance. In this paper, we propose a novel model, the Conditional Masked Language Model with Correction (CMLMC), which achieves state-of-the-art NAR performance without the need for distillation.

Our proposed CMLMC model builds upon the Conditional Masked Language Model (CMLM) by incorporating a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. We also modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Our model achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks.

In the related work, we discuss several pre-training models, including ProphetNet, MPNet, and Universal Conditional Masked Language Pre-training, which have significantly improved NMT performance. We also discuss the Attention Is All You Need and XLNet models, which have achieved state-of-the-art results in machine translation tasks. Additionally, we discuss JANUS and BANG, which bridge the gap between AR and NAR generation. Finally, we discuss our previous work on improving NAR translation models without distillation.

Our proposed CMLMC model is unique in its incorporation of a novel correction loss and its modification of the decoder structure of CMLM. We demonstrate the effectiveness of our model on multiple NMT benchmarks, achieving state-of-the-art NAR performance without the need for distillation. Our work has significant implications for the field of machine translation, as it provides a promising approach to improving NAR models and reducing inference times.