Machine translation has been a long-standing challenge in the field of artificial intelligence. With the increasing demand for multilingual communication, there is a growing need for accurate and efficient translation systems. Recent advances in neural machine translation (NMT) have shown promising results, but there are still limitations in terms of performance and efficiency. In this paper, we propose a novel pre-training model for machine translation, called CeMAT, which aims to improve the performance of both autoregressive and non-autoregressive NMT.

The main motivation for our research is to address the limitations of existing NMT models, particularly in low-resource settings. Our proposed model is pre-trained on both monolingual and bilingual corpora, and it consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT provides unified initialization parameters for both autoregressive and non-autoregressive NMT tasks, which can significantly improve the performance of machine translation.

To enhance the training of our model, we introduce two novel techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques help to improve the accuracy and efficiency of our model, particularly in low-resource settings.

Our proposed model is evaluated on both low-resource and high-resource settings, and the results show consistent improvements over strong competitors in both autoregressive and non-autoregressive NMT tasks. The main contributions of this paper are the development of CeMAT, the introduction of aligned code-switching & masking and dynamic dual-masking techniques, and the demonstration of the effectiveness of our model in improving machine translation performance.

In terms of related work, our proposed model builds upon recent advances in NMT, particularly in the use of pre-training models. We compare our model with other pre-training models, such as JANUS, XLNet, BANG, ProphetNet, MPNet, and Universal Conditional Masked Language Pre-training, and highlight the main differences and advantages of our approach.

In summary, our proposed model, CeMAT, provides a novel solution to improve the performance of machine translation, particularly in low-resource settings. The introduction of aligned code-switching & masking and dynamic dual-masking techniques further enhances the accuracy and efficiency of our model. The results of our experiments demonstrate the effectiveness of our approach and its potential for practical applications in multilingual communication.