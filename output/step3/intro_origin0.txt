The field of natural language generation has seen significant advancements in recent years, with pre-trained models such as BERT and XLNet achieving state-of-the-art results on various tasks. However, these models have limitations, such as neglecting dependency among predicted tokens and suffering from pretrain-finetune discrepancy. To address these limitations, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which incorporates a novel self-supervised objective future n-gram prediction and a method to simultaneously predict the future n-gram at each time step during training. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. Additionally, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Our work builds upon previous research in the field, such as BERT, XLNet, and Transformer-based autoregressive and non-autoregressive models. However, our approach differs in its focus on future n-gram prediction and the use of n-stream self-attention. We also achieve better results with less pre-training epochs and corpus compared to previous models. Our contributions have significant implications for the AI community, as they demonstrate the potential for improving natural language generation models and advancing the field.