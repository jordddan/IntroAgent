Machine translation has been a long-standing challenge in the field of artificial intelligence, with the goal of achieving human-level accuracy and fluency. Recent advances in pre-training models have shown promising results in improving machine translation performance. In this paper, we propose a novel pre-training model called CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

Our work also introduces aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques have shown to be effective in improving the performance of CeMAT on both low-resource and high-resource settings.

To provide context, we briefly mention related work in the field of pre-training models for machine translation. ProphetNet, XLNet, and BERT are some of the most successful pre-training models that have been proposed. ProphetNet introduces a novel self-supervised objective named future n-gram prediction, while XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. BERT adopts masked language modeling (MLM) for pre-training, but neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy.

Compared to these models, CeMAT is unique in its bidirectional encoder-decoder architecture and its ability to provide unified initialization parameters for both AT and NAT tasks. Our work also introduces novel techniques to enhance the model training under the setting of bidirectional decoders. Experimental results show that CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks.