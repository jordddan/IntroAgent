Recent advances in natural language processing have led to the development of autoregressive (AR) and non-autoregressive (NAR) models for sequence generation tasks. While AR models have shown excellent performance, they suffer from slow decoding speed due to their sequential nature. On the other hand, NAR models can generate sequences in parallel, but their performance lags behind AR models, especially when trained without distillation. 

To address these limitations, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. Our approach achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average. 

Our work is motivated by the need for faster and more accurate sequence generation models. The proposed JANUS method is relevant to the AI community as it provides a novel solution to the problem of combining AR and NAR models. Our research questions include: Can we improve the performance of AR and NAR models by combining their strengths? Can we bridge the gap between AR and NAR models without distillation data? 

To provide context, we briefly mention related work in this area. ProphetNet introduces a novel self-supervised objective named future n-gram prediction, while MPNet leverages the dependency among predicted tokens through permuted language modeling. CeMAT pre-trains a sequence-to-sequence model with a bidirectional decoder, while BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. Finally, CMLMC addresses the problems of indistinguishability of tokens and mismatch between training and inference in NAR models. 

The main differences between our work and related work are that JANUS combines the strengths of AR and NAR models without distillation data, introduces an auxiliary distribution to bridge the discrepancy between P AR and P NAR, and achieves significant improvement on multiple generation tasks. 

In summary, our work proposes a novel method for combining AR and NAR models, which improves performance without the need for distillation data. The proposed JANUS method has the potential to significantly impact the field of natural language processing and advance the development of faster and more accurate sequence generation models.