The field of natural language generation (NLG) has seen significant advancements in recent years, with the development of large-scale pre-trained models such as BERT, T5, and PEGASUS. However, these models are limited in their ability to plan for future tokens and prevent overfitting on strong local correlations. To address these limitations, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which utilizes a novel self-supervised objective future n-gram prediction. 

Our proposed method simultaneously predicts the future n-gram at each time step during the training phase, encouraging the model to plan for future tokens and preventing overfitting on strong local correlations. Additionally, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. 

We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Our work is motivated by the need for more advanced NLG models that can plan for future tokens and prevent overfitting on strong local correlations. The proposed ProphetNet model addresses these limitations and achieves state-of-the-art results on several NLG tasks. Our research questions and objectives include developing a new pre-trained model that can predict future n-grams and extending the self-attention mechanism to include n-stream self-attention. 

Related work in this field includes XLNet, BERT, and T5, which have all made significant contributions to the development of pre-trained models for NLG. However, our proposed ProphetNet model differs from these models in its ability to plan for future tokens and prevent overfitting on strong local correlations. 

In summary, our work introduces a new pre-trained model for NLG that addresses limitations in existing models and achieves state-of-the-art results on several NLG tasks. The proposed ProphetNet model utilizes a novel self-supervised objective future n-gram prediction and extends the self-attention mechanism to include n-stream self-attention.