Machine learning has revolutionized the field of natural language processing (NLP) by enabling the development of powerful models that can learn from large amounts of data. However, pre-training models for NLP tasks remains a challenging problem due to the complexity of language and the need to capture long-range dependencies. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations.

Our proposed MPNet approach splits the tokens in a sequence into non-predicted and predicted parts, which allows the model to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-trained MPNet on a large-scale text corpus and fine-tuned it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting.

Our proposed MPNet approach builds upon and extends the existing literature on pre-training models for NLP tasks. Prior work has shown that pre-training models based on autoregressive language modeling can achieve better performance than pre-training approaches based on MLM. However, relying on corrupting the input with masks, MLM neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In contrast, XLNet introduces PLM for pre-training to address this problem, but it does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. Our proposed MPNet approach inherits the advantages of BERT and XLNet and avoids their limitations by leveraging the dependency among predicted tokens through permuted language modeling and taking auxiliary position information as input to make the model see a full sentence.

In summary, our proposed MPNet approach represents a significant contribution to the field of pre-training models for NLP tasks. By unifying the advantages of MLM and PLM while addressing their limitations, MPNet achieves state-of-the-art performance on various downstream benchmark tasks. Our proposed approach has the potential to significantly impact the field of NLP and advance the development of more powerful pre-training models for machine learning.