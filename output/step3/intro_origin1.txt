The field of natural language processing has seen significant advancements in recent years, with pre-training models such as BERT and XLNet achieving state-of-the-art results on various downstream tasks. However, these models have limitations, such as neglecting the dependency among predicted tokens and suffering from position discrepancy between pre-training and fine-tuning. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that addresses these limitations by unifying the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM). 

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, allowing the model to consider the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Our proposed method builds upon previous works such as BERT and XLNet, which have achieved significant success in pre-training models for natural language processing. However, our approach differs from these models by leveraging the dependency among predicted tokens through permuted language modeling and taking auxiliary position information as input to reduce the position discrepancy. Other related works, such as ProphetNet, CeMAT, and JANUS, have also explored different pre-training methods for sequence-to-sequence models and achieved notable performance gains. 

In summary, our work proposes a novel pre-training method that unifies the advantages of MLM and PLM while addressing their limitations. Our experimental results demonstrate the effectiveness of MPNet on various downstream tasks, outperforming previous state-of-the-art models. This work contributes to the ongoing efforts to improve pre-training models for natural language processing and has significant implications for various applications in this field.