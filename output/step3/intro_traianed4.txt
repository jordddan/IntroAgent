Machine learning has revolutionized the field of artificial intelligence by enabling computers to learn from data and make predictions or decisions without being explicitly programmed. One of the most important applications of machine learning is natural language generation (NLG), which involves generating human-like text from structured data or unstructured text. Despite significant progress in NLG, there is still a need for more accurate and efficient models that can generate high-quality text with minimal human intervention.

In this paper, we propose a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and a method to simultaneously predict the future n-gram at each time step during the training phase. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. We also extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

Our proposed approach is motivated by the limitations of existing pre-trained models such as BART, T5, and PEGASUS, which require a large amount of pre-training epochs and corpus to achieve state-of-the-art results. In contrast, ProphetNet achieves new state-of-the-art results on CNN/DailyMail and Gigaword using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. We also fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Our proposed approach differs from prior works such as JANUS, XLNet, BANG, Attention Is All You Need, MPNet, and CMLMC by introducing a novel self-supervised objective future n-gram prediction and a method to simultaneously predict the future n-gram at each time step during the training phase. We also extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

The main contributions of this paper are: 1) introducing a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction, 2) developing a method to simultaneously predict the future n-gram at each time step during the training phase, 3) extending the two-stream self-attention proposed in XLNet to n-stream self-attention, 4) pre-training ProphetNet on two scale pre-trained datasets and achieving new state-of-the-art results on CNN/DailyMail and Gigaword, and 5) fine-tuning ProphetNet on several NLG tasks and achieving the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

The rest of the paper is organized as follows. Section 2 provides an overview of related work in pre-trained models for NLG. Section 3 describes the technical details of our proposed approach, including the self-supervised objective future n-gram prediction and the n-stream self-attention mechanism. Section 4 presents experimental results on several NLG tasks, including CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks. Section 5 discusses the limitations and future directions of our proposed approach. Finally, Section 6 summarizes the key contributions of our research and highlights the potential implications for future research.