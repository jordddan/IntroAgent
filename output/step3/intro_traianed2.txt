Machine learning has revolutionized the field of artificial intelligence by enabling computers to learn from data and make predictions or decisions without being explicitly programmed. One of the most important applications of machine learning is natural language processing, which involves teaching computers to understand and generate human language. In this paper, we address the problem of Non-autoregressive (NAR) and semi-NAR generation, which is a challenging task in natural language processing.

Previous research has focused on either Autoregressive (AR) or NAR generation, but there is a gap between these two approaches. AR models generate one token at a time, which can be slow and computationally expensive, while NAR models generate tokens in parallel, but they often sacrifice accuracy for speed. To bridge this gap, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation.

Our approach differs from prior works in several ways. First, we use an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. Second, BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Third, BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.

The main contribution of our research is the development of BANG, which bridges the gap between AR and NAR via pretraining a generative model. BANG supports NAR, semi-NAR, and AR finetuning, and achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning. Our approach has the potential to improve the accuracy and speed of natural language processing tasks, such as question generation, summarization, and dialogue generation.

In this paper, we provide a detailed description of the technical details of BANG, including its novel model structure and pretraining method. We also discuss the limitations and assumptions of our approach. The rest of the paper is organized as follows. In Section 2, we provide an overview of related work in the field of NAR and semi-NAR generation. In Section 3, we describe the technical details of BANG. In Section 4, we present the experimental results and compare our approach to other state-of-the-art models. Finally, in Section 5, we summarize the key contributions of our research and highlight the potential implications for future research.