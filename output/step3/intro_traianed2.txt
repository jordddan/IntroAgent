Machine translation has been a long-standing challenge in the field of natural language processing. Despite significant progress in recent years, there is still a need for more effective and efficient models that can handle low-resource languages and improve translation quality. In this paper, we propose a novel pre-training model called CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

Our work builds upon and extends the existing literature by introducing aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.

Our proposed model is unique in its ability to pre-train a unified model for fine-tuning on both NMT tasks. We compare our work with several related works, including ProphetNet, MPNet, Attention Is All You Need, JANUS, BANG, and XLNet. Our work stands out by achieving significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. 

In conclusion, our work presents a novel pre-training model that can significantly improve machine translation performance on both low-resource and high-resource settings. Our proposed model is unique in its ability to pre-train a unified model for fine-tuning on both NMT tasks. The proposed aligned code-switching & masking and dynamic dual-masking techniques further enhance the model training under the setting of bidirectional decoders. Our work has the potential to make a significant impact on the field of machine learning and natural language processing.