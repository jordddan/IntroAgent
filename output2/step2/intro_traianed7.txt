Machine translation has been a popular research area in natural language processing, and various models have been proposed to improve the translation quality. Autoregressive (AR) models generate translations sequentially, which can achieve high-quality translations but suffer from slow inference speed. Non-autoregressive (NAR) models generate translations in parallel, which can achieve fast inference speed but often result in lower translation quality. To address the limitations of both AR and NAR models, this paper proposes a novel method called JANUS, which combines the strengths of both models while avoiding their weaknesses.

The proposed method introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. The effectiveness of JANUS is demonstrated on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Moreover, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Related works in this area are also discussed, including MPNet, XLNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, BANG, Improving Non-Autoregressive Translation Models Without Distillation, and ProphetNet. These works are compared and contrasted with the proposed JANUS method, highlighting the need for a new approach and how it addresses the limitations of existing methods.

In summary, this paper proposes a novel method called JANUS, which combines the strengths of both AR and NAR models while avoiding their weaknesses. The proposed method is demonstrated to achieve state-of-the-art performance on multiple NMT datasets and pretraining models. The experimental setup and results are thoroughly analyzed and interpreted, highlighting the key findings and implications of the study.