Recent advances in machine learning have led to significant improvements in natural language processing tasks such as machine translation, summarization, and dialogue generation. However, most of these approaches rely on autoregressive (AR) generation, which generates tokens sequentially and suffers from slow inference speed. Non-autoregressive (NAR) generation, on the other hand, generates tokens in parallel and is much faster but often suffers from lower quality due to the lack of dependency among predicted tokens.

To bridge the gap between AR and NAR generation, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

In this paper, we provide a comprehensive overview of related work, highlighting the limitations of existing methods and how BANG differs from them. We describe the proposed model in detail, emphasizing the novel aspects of the approach and how it addresses the research question. We also provide a detailed description of the experimental setup, including the dataset used, evaluation metrics, and any preprocessing steps. 

Our experiments on question generation (SQuAD 1.1), summarization (XSum), and dialogue generation (PersonaChat) show that BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong NAR baselines. 

Our proposed approach fills the gap in the existing literature by providing a novel pretraining model that bridges the gap between AR and NAR generation. The results of our experiments demonstrate the effectiveness of our approach and its potential impact on the field of natural language processing.