The field of natural language generation has seen significant advancements in recent years, with pre-trained models such as BERT and XLNet achieving state-of-the-art results on various tasks. However, these models still suffer from limitations such as neglecting dependency among predicted tokens and pretrain-finetune discrepancy. To address these limitations, we propose a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and the proposed n-stream self-attention mechanism. 

Our proposed method simultaneously predicts the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. We extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. 

In addition, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. Our proposed approach fills the gap in the existing literature by introducing a novel self-supervised objective and n-stream self-attention mechanism, which significantly improves the performance of pre-trained models on various NLG tasks. 

Related works such as MPNet, XLNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, BANG, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, and JANUS have also proposed various pre-training models and techniques to improve the performance of NLG tasks. However, our proposed approach differs from these works by introducing a novel self-supervised objective and n-stream self-attention mechanism, which significantly improves the performance of pre-trained models on various NLG tasks. 

In this paper, we provide a comprehensive overview of our proposed approach, including the technical details of the model and the experimental setup. We conduct a thorough analysis and interpretation of the experimental results, including comparison with existing literature, and highlight the key findings and implications of the study. The results of our study demonstrate the effectiveness of our proposed approach and its potential impact on the field of natural language generation.