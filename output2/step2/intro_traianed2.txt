Machine translation has been a popular research topic in natural language processing (NLP) for decades. Recently, pre-training models have shown great potential in improving machine translation performance. However, most pre-training models are designed for either autoregressive NMT (AT) or non-autoregressive NMT (NAT) tasks, and few models can provide unified initialization parameters for both tasks. In this paper, we propose CeMAT, a pre-training model that can provide unified initialization parameters for both AT and NAT tasks. CeMAT consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora, and we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders.

We provide a comprehensive overview of related work, highlighting the limitations of existing methods and how the proposed approach differs from them. We compare CeMAT with several state-of-the-art pre-training models, including BERT, XLNet, and ProphetNet. We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.

Our contribution is three-fold. First, we propose CeMAT, a pre-training model that can provide unified initialization parameters for both AT and NAT tasks. Second, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Third, we demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings.

We conduct extensive experiments on several datasets, including WMT14 English-German, WMT16 English-Romanian, and IWSLT14 German-English. We evaluate the performance of CeMAT on both AT and NAT tasks and compare it with several state-of-the-art pre-training models. Our experimental results show that CeMAT achieves significant improvements over strong competitors in both AT and NAT tasks with data of varied sizes. We also conduct ablation studies to analyze the effectiveness of the proposed techniques. Our results demonstrate that aligned code-switching & masking and dynamic dual-masking techniques can significantly improve the performance of CeMAT.