The field of natural language processing has seen significant advancements in recent years, with pretraining-based approaches like BERT achieving state-of-the-art performance on a wide range of tasks. However, these approaches suffer from limitations such as the pretrain-finetune discrepancy and the independence assumption made in BERT. To address these limitations, we propose XLNet, a generalized autoregressive pretraining method that combines the best of both autoregressive language modeling and autoencoding.

XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach overcomes the limitations of BERT and provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

We also improve architectural designs for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. Additionally, we propose a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Empirical results demonstrate that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. We compare our approach with related works such as MPNet, CeMAT, BANG, and CMLMC, and show that XLNet achieves state-of-the-art performance on these tasks.

In summary, our work proposes a novel approach to pretraining that overcomes the limitations of existing methods and achieves state-of-the-art performance on a wide range of natural language processing tasks. The proposed approach has significant implications for the field and opens up new avenues for future research.