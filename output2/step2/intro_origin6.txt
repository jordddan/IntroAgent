Machine learning has become an essential tool in various fields, including natural language processing (NLP). In NLP, sequence-to-sequence (Seq2Seq) models have shown remarkable performance in tasks such as machine translation, summarization, and question generation. However, pre-training Seq2Seq models is a challenging task due to the lack of a clear objective function. 

In this paper, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which utilizes a novel self-supervised objective function called future n-gram prediction. ProphetNet is optimized by predicting the next n tokens simultaneously based on previous context tokens at each time step, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. Additionally, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Related works in this field include MPNet, XLNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, BANG, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, and JANUS. These works have proposed various pre-training methods and architectures to improve the performance of Seq2Seq models in NLP tasks.

In this paper, we present a new approach to pre-training Seq2Seq models that achieves state-of-the-art results on several benchmarks. The rest of the paper is organized as follows: Section 2 provides an overview of related work, Section 3 describes the proposed ProphetNet model in detail, Section 4 presents the experimental setup and results, and Section 5 concludes the paper and discusses future work.