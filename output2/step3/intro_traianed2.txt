Machine learning has made significant strides in recent years, with pretraining models playing a crucial role in improving performance across various tasks. One of the key challenges in pretraining models is bridging the gap between Autoregressive (AR) and Non-autoregressive (NAR) generation. In this paper, we propose BANG, a large-scale pretraining model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pretraining a generative model.

To provide context for our proposed approach, we provide an overview of related works in the field. Prior works have explored various pretraining models, including Universal Conditional Masked Language Pre-training, ProphetNet, JANUS, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, MPNet, XLNet, and Attention Is All You Need. While these models have made significant contributions to the field, they have not fully addressed the challenge of bridging the gap between AR and NAR generation.

The proposed approach is motivated by the need to improve performance across various tasks, including question generation, summarization, and dialogue generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

Our experiments demonstrate that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong NAR baselines.

In summary, this paper proposes a novel approach to bridge the gap between AR and NAR generation, which has significant implications for improving performance across various tasks. The proposed approach is supported by extensive experiments, which demonstrate its effectiveness and potential impact on the field. The rest of the paper is organized as follows: Section 2 provides an overview of the proposed approach, Section 3 describes the experimental setup, Section 4 presents the results, and Section 5 concludes the paper.