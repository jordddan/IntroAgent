Machine translation has been a long-standing challenge in natural language processing, with the goal of enabling seamless communication across different languages. In recent years, pre-training models have shown significant improvements in machine translation performance. However, most pre-training models adopt an unidirectional decoder, which limits their performance in both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. 

To address this limitation, this paper proposes CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. Additionally, this paper introduces aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. 

The effectiveness of CeMAT is demonstrated through experiments on both low-resource and high-resource settings, achieving consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. This paper also provides a comprehensive overview of related works in the field, highlighting their strengths and weaknesses and how they relate to the proposed approach. 

The main contributions of this paper are: 1) the development of CeMAT, a pre-training model for machine translation that can provide unified initialization parameters for both AT and NAT tasks, 2) the introduction of aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders, and 3) the demonstration of the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. 

The rest of the paper is organized as follows. Section 2 provides an overview of related works in the field. Section 3 describes the proposed approach in detail. Section 4 presents the experimental setup and results. Finally, Section 5 concludes the paper and discusses future work.