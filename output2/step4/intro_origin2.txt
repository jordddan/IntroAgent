Machine learning has become an essential tool in various fields, including natural language processing (NLP). In NLP, machine translation is a crucial task that has received significant attention in recent years. Non-autoregressive (NAR) machine translation models have been proposed to accelerate inference by translating blocks of tokens in parallel. However, leading NAR models still lag behind their autoregressive (AR) counterparts, and only become competitive when trained with distillation. In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC), which addresses the problems of indistinguishability of tokens and mismatch between training and inference. The CMLMC achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets. The main contributions of this paper are: (1) proposing the CMLMC, which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation, (2) modifying the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens, (3) proposing a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence, and (4) achieving new state-of-the-art undistilled NAR results and approaching AR performance on multiple NMT benchmarks. We compare our work with several related works, including MPNet, XLNet, ProphetNet, Attention Is All You Need, Universal Conditional Masked Language Pre-training, JANUS, and BANG. The rest of the paper is organized as follows: Section 2 provides background and related work, Section 3 describes the proposed method, Section 4 presents the experimental setup and results, and Section 5 concludes the paper.