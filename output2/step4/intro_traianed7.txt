Machine learning has revolutionized the field of natural language processing (NLP) by enabling the development of models that can generate high-quality text. However, most existing models are either autoregressive (AR) or non-autoregressive (NAR), and each has its own limitations. AR models generate text sequentially, which can be slow and computationally expensive, while NAR models generate text in parallel, but often produce lower-quality output due to the lack of sequential dependencies. Bridging the gap between AR and NAR models is an important research problem that has not been fully addressed.

In this paper, we propose BANG, a new pretraining model designed to bridge the gap between AR and NAR generation. BANG is the first large-scale pretraining model designed for NAR and semi-NAR generation, and it achieves this by pretraining a generative model using an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Our experiments show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with strong AR pretrained models.

To the best of our knowledge, BANG is the first model that bridges the gap between AR and NAR generation, and it represents a significant advance in the field of NLP. Our proposed approach addresses the limitations of existing models and provides a novel and innovative solution to the problem of generating high-quality text. Our experimental results demonstrate the effectiveness of our approach and its superiority over existing state-of-the-art models.

Related work in this area includes MPNet, which combines the advantages of BERT and XLNet to avoid their limitations, and XLNet, which introduces a generalized autoregressive pretraining method that enables learning bidirectional contexts. Other related works include ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction, and JANUS, which proposes a joint autoregressive and non-autoregressive training method. However, none of these models address the problem of bridging the gap between AR and NAR generation, which is the main contribution of our work.

In this paper, we provide a comprehensive review of related work, highlighting the gaps in the literature that our proposed approach aims to address. We also outline the structure of the paper, which includes a brief overview of the experimental setup and results, and explain how the experimental results will be presented and analyzed. Overall, our proposed approach represents a significant advance in the field of NLP and has the potential to enable the development of more efficient and effective models for generating high-quality text.