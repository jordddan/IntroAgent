Machine translation has been a long-standing challenge in natural language processing, with recent advances in deep learning techniques leading to significant improvements in translation quality. However, the traditional autoregressive (AR) framework for machine translation, which translates one token at a time, can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed, which translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. 

In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC), which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. Our main contributions are four-fold: (1) we propose the CMLMC, which modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens; (2) we propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence; (3) we achieve new state-of-the-art undistilled NAR results and approach AR performance on multiple NMT benchmarks; and (4) we demonstrate the effectiveness of our approach through extensive experiments and comparisons with state-of-the-art models.

To the best of our knowledge, this is the first work to propose a correction-based approach for NAR machine translation. Our approach addresses the indistinguishability of tokens and the mismatch between training and inference, which are the main reasons behind the performance gap between NAR and AR models. Our approach is novel and innovative, and it advances the field of NAR machine translation. 

In the related work, we review recent advances in pre-training models, including MPNet, XLNet, ProphetNet, and CeMAT, which have significantly improved the performance of NMT. We also review recent approaches for joint AR and NAR training, including JANUS and BANG, which aim to bridge the gap between AR and NAR generation. However, none of these approaches address the correction of translation mistakes made in early decoding iterations, which is the main contribution of our work.

The rest of the paper is organized as follows. In Section 2, we provide a detailed description of our proposed approach. In Section 3, we present the experimental setup and results, followed by a discussion of the results in Section 4. Finally, we conclude the paper in Section 5 and discuss future directions for research.