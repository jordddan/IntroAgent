Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and robotics. In this paper, we focus on the problem of Non-autoregressive (NAR) and semi-NAR generation, which has been a challenging task due to the lack of dependency among predicted tokens. To address this issue, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation, which bridges the gap between Autoregressive (AR) and NAR via pretraining a generative model.

Our approach is based on an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. We demonstrate that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.

Our work builds upon several related works, including MPNet, XLNet, ProphetNet, Attention Is All You Need, Universal Conditional Masked Language Pre-training, and JANUS. These works have proposed various pretraining methods and architectures to improve the performance of machine learning models. However, none of them have addressed the specific problem of NAR and semi-NAR generation, which is the focus of our work.

In this paper, we provide a detailed description of our approach, including the model architecture, pretraining method, and finetuning strategies. We also present experimental results on question generation, summarization, and dialogue generation tasks, demonstrating the effectiveness of our approach. The rest of the paper is organized as follows: Section 2 provides an overview of related work, Section 3 describes our approach in detail, Section 4 presents experimental results, and Section 5 concludes the paper and discusses future work.