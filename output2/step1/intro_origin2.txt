Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and robotics. In recent years, there has been a growing interest in developing large-scale pretraining models to improve the performance of machine learning models. In this paper, we propose BANG, a novel pretraining model designed for Non-autoregressive (NAR) and semi-NAR generation, which bridges the gap between Autoregressive (AR) and NAR via pretraining a generative model. 

The main objective of this paper is to address the limitations of existing pretraining models and provide a more efficient and effective approach for NAR and semi-NAR generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

Our main contributions are fourfold. Firstly, BANG is proposed as the first large-scale pretraining model designed for NAR and semi-NAR generation. Secondly, BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. Thirdly, BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Finally, BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.

In this paper, we compare BANG with several related works, including JANUS, XLNet, Attention Is All You Need, ProphetNet, MPNet, and Universal Conditional Masked Language Pre-training. Our experiments show that BANG outperforms these models in terms of NAR and semi-NAR performance, and achieves comparable performance with strong AR pretrained models for AR finetuning. 

The rest of the paper is organized as follows. Section 2 provides a detailed description of the proposed BANG model. Section 3 presents the experimental setup and results. Section 4 discusses the related work, and Section 5 concludes the paper.