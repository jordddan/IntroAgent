Introduction:

Pre-training models have significantly improved the performance of machine translation tasks. However, existing pre-training models have limitations in terms of their architecture and training techniques. In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. 

Previous works have shown that pre-training models based on autoregressive language modeling, such as BERT, achieve better performance than pre-training approaches based on non-autoregressive language modeling. However, these models neglect the dependency between the masked positions and suffer from a pretrain-finetune discrepancy. To address these issues, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages.

To demonstrate the effectiveness of CeMAT, we conduct experiments on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our experiments show that CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. 

Our proposed model and techniques are inspired by related works such as ProphetNet, XLNet, Attention Is All You Need, MPNet, JANUS, and BANG. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. JANUS is a Joint Autoregressive and Non-autoregressive training method using an Auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. BANG bridges Autoregressive (AR) and Non-autoregressive (NAR) Generation by designing a novel model structure for large-scale pretraining. 

In summary, our proposed CeMAT model and techniques have shown significant improvements in machine translation performance on both low-resource and high-resource settings. Our experiments demonstrate the effectiveness of our proposed model and techniques, and we believe that they can be applied to other natural language processing tasks as well.