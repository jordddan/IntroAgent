Introduction:

Sequence generation tasks have been widely studied in natural language processing (NLP), and both autoregressive (AR) and non-autoregressive (NAR) models have been proposed to tackle this problem. While AR models achieve excellent performance, they suffer from slow decoding speed, which limits their practical applications. On the other hand, NAR models can generate sequences in parallel, but they often suffer from inferior performance compared to AR models. To address these issues, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

In this paper, we introduce JANUS, which leverages the advantages of both AR and NAR models to achieve better performance in sequence generation tasks. Specifically, JANUS proposes an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. By combining the strengths of both models, JANUS achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average. Moreover, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

To the best of our knowledge, JANUS is the first method that combines AR and NAR models in a joint training framework. Our approach is inspired by recent works on pre-training models, such as ProphetNet, XLNet, and BART, which have achieved state-of-the-art performance in various NLP tasks. ProphetNet introduces a novel self-supervised objective named future n-gram prediction, while XLNet maximizes the expected likelihood over all permutations of the factorization order. BART is a sequence-to-sequence model that combines both AR and NAR models in a unified framework. However, these methods do not explicitly combine AR and NAR models in a joint training framework, which is the main contribution of our work.

In summary, our main contributions in this paper are: (1) introducing JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance, (2) proposing an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, (3) demonstrating the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, and (4) exceeding the non-autoregressive pretraining model BANG on the same GLGE tasks and achieving comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.