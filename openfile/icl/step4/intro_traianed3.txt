Introduction:

Pre-training models have become a popular approach for improving the performance of natural language processing tasks. Among the pre-training methods, Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have been widely used. However, MLM neglects the dependency among predicted tokens, while PLM does not leverage the full position information of a sentence, leading to a position discrepancy. To address these limitations, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM.

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, allowing it to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. Specifically, MPNet leverages the dependency among predicted tokens through permuted language modeling, while taking auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy.

We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. Specifically, MPNet outperforms BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Related works have explored different pre-training methods. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. BERT adopts MLM for pre-training and is one of the most successful pre-training models. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between autoregressive and non-autoregressive models and proposes a new method that achieves state-of-the-art NAR performance when trained on raw data without distillation. JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously and effectively alleviates the problem of distribution discrepancy. BANG is a new pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training.

In summary, we propose MPNet, a new pre-training method that unifies the advantages of both MLM and PLM while addressing their limitations. MPNet outperforms previous state-of-the-art pre-trained methods on various downstream benchmark tasks. The related works have explored different pre-training methods, and our proposed method provides a new approach to address the limitations of MLM and PLM.