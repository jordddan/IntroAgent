Introduction:

Neural machine translation (NMT) has achieved remarkable success in recent years, with the Transformer architecture being the most widely used. The majority of proposed approaches are based on the autoregressive (AR) principle, where translation is done one token at a time conditioning on already translated tokens. However, AR inference can be time-consuming, especially for long sequences. To address this issue, recent works have explored non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite considerable progress, leading NAR models still require sequence-level knowledge distillation to achieve competitive accuracy. 

In this paper, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pretraining a generative model. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

Our proposed model achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning. For AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. The proposed model structure is novel and efficient, and it can be used for various NMT tasks. 

Related works have explored different pretraining methods for NMT models. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. CMLMC addresses the indistinguishability of tokens and mismatch between training and inference. CeMAT pre-trains a sequence-to-sequence model but with a bidirectional decoder to produce notable performance gains for both AR and NAR NMT. JANUS is a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. 

In summary, our proposed BANG model is a novel and efficient pretraining method for NMT models that bridges the gap between AR and NAR generation. The proposed model structure supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. The experimental results show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.