Introduction:

Pre-training large-scale sequence-to-sequence (Seq2Seq) models has become a popular approach to improve the performance of natural language generation (NLG) tasks. However, existing pre-training methods have limitations in capturing future context and preventing overfitting on strong local correlations. In this paper, we propose a new pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective called future n-gram prediction and a method to simultaneously predict the future n-gram at each time step during the training phase. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. 

To achieve this, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. This allows ProphetNet to capture future context and improve the performance of NLG tasks. 

We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. We also fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. 

Related works have explored different pre-training methods for Seq2Seq models. XLNet proposed a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. Attention Is All You Need proposed a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. MPNet proposed a novel pre-training method that leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigated possible reasons behind the performance gap between autoregressive and non-autoregressive approaches and proposed a new method called Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Universal Conditional Masked Language Pre-training proposed a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. JANUS proposed a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. BANG proposed a new pre-training model to bridge the gap between autoregressive and non-autoregressive generation. 

In summary, our proposed ProphetNet model introduces a novel self-supervised objective and a method to capture future context, which improves the performance of NLG tasks. We achieve new state-of-the-art results on CNN/DailyMail and Gigaword and outperform existing pre-training methods on several NLG tasks. The related works have explored different pre-training methods for Seq2Seq models, and our proposed method extends the two-stream self-attention to n-stream self-attention to capture future context.