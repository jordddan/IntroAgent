Introduction:

Pre-training models have become a popular approach for improving the performance of natural language processing tasks. Among the pre-training methods, Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have been widely used. However, MLM neglects the dependency among predicted tokens, while PLM does not leverage the full position information of a sentence, leading to a position discrepancy between pre-training and fine-tuning. To address these limitations, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM.

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, allowing the model to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. This approach enables MPNet to leverage the advantages of both MLM and PLM while addressing their limitations.

We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Related works:

ProphetNet [0] presents a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet [1] proposes a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. Attention Is All You Need [2] proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Universal Conditional Masked Language Pre-training [5] demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. JANUS [6] proposes a Joint Autoregressive and Non-autoregressive training method using an Auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. BANG [7] proposes a new pre-training model to bridge the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation by designing a novel model structure for large-scale pretraining.