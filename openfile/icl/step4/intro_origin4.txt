Introduction:

Machine translation has been a popular research topic in natural language processing (NLP) for decades. Recently, non-autoregressive (NAR) machine translation has attracted more attention due to its feature of parallel decoding, which greatly reduces the translation latency. However, NAR models still lag behind their autoregressive (AR) counterparts, and only become competitive when trained with distillation. In this paper, we propose a new pre-training model called Conditional Masked Language Model with Correction (CMLMC) that addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation.

Related works have shown that pre-training models based on autoregressive language modeling, such as BERT and XLNet, achieve better performance than those based on non-autoregressive models. However, these models neglect the dependency between the masked positions and suffer from a pretrain-finetune discrepancy. To overcome these limitations, we propose CMLMC, which modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Additionally, we propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence.

Our main contributions are: 1) Proposing the Conditional Masked Language Model with Correction (CMLMC) which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. 2) Modifying the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. 3) Proposing a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 4) Achieving new state-of-the-art undistilled NAR results and approaching autoregressive (AR) performance on multiple NMT benchmarks.

In addition to our proposed model, related works have explored various pre-training methods for NMT tasks, such as ProphetNet, XLNet, and MPNet. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages, which can produce notable performance gains for both Autoregressive and Non-autoregressive NMT.

Our proposed CMLMC model achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. Experimental results show that CMLMC outperforms previous state-of-the-art pre-trained methods, such as BERT, XLNet, and RoBERTa, under the same model setting. The code and the pre-trained models are available at: https://github.com/layer6ai-labs/CMLMC.