Introduction:

Machine translation has been a popular research topic in the field of natural language processing (NLP) for decades. Recently, pre-training models have been widely used to improve the performance of machine translation. In this paper, we propose a new pre-training model called CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. 

Previous works have shown that pre-training models based on autoregressive language modeling, such as BERT, achieve better performance than pre-training approaches based on non-autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. To address this issue, XLNet introduces permuted language modeling (PLM) for pre-training to capture the dependency among the predicted tokens. However, XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. 

To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. 

In this paper, we demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our experiments show that CeMAT outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. 

In summary, our contributions in this paper are threefold: (1) the development of CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them; (2) the introduction of aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders; and (3) the demonstration of the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings.