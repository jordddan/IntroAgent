Introduction:

Neural machine translation (NMT) models have achieved remarkable success in recent years, with the Transformer architecture being the most popular and effective one (Vaswani et al., 2017; Barrault et al., 2019; Huang et al., 2020). However, most of the proposed approaches are based on the autoregressive (AR) principle, which translates one token at a time conditioning on already translated tokens. This approach can be time-consuming, especially for long sequences, and requires a full forward pass through the decoder for each translated token. To address this issue, recent works have explored the non-autoregressive (NAR) approach, where subsets of tokens are translated in parallel (Gu et al., 2018; Ghazvininejad et al., 2019; Kasai et al., 2020). NAR models achieve significantly faster inference speed that no longer depends on sequence length. However, leading NAR models still require sequence-level knowledge distillation to achieve competitive accuracy, which is expensive and raises questions about the necessity and underlying problems solved by distillation (Zhou et al., 2020; Ding et al., 2021).

To overcome the limitations of both AR and NAR approaches, this paper proposes XLNet, a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. Unlike BERT, XLNet does not suffer from the pretrain-finetune discrepancy, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

To improve architectural designs for pretraining, this paper incorporates the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. Additionally, this paper proposes a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Empirical results show that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. This paper also compares XLNet with other related works, such as ProphetNet, Attention Is All You Need, MPNet, and JANUS, which have proposed different pretraining methods for NMT. However, XLNet outperforms these models on various tasks, often by a large margin.

In summary, this paper proposes XLNet, a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet outperforms BERT on a wide spectrum of problems, and the proposed architectural designs for pretraining improve performance, especially for tasks involving longer text sequences. The following sections will provide more details on the proposed method and experimental results.