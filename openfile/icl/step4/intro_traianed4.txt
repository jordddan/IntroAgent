Introduction:

Machine translation has been a popular research topic in the field of natural language processing (NLP) for decades. Recently, non-autoregressive (NAR) machine translation has attracted more attention due to its feature of parallel decoding, which greatly reduces the translation latency. However, leading NAR models still lag behind their autoregressive (AR) counterparts, and only become competitive when trained with distillation. In this paper, we propose a novel pre-training method, the Conditional Masked Language Model with Correction (CMLMC), that addresses the problems of NAR machine translation without the need for distillation.

The first contribution of this paper is the proposal of the Conditional Masked Language Model with Correction (CMLMC), which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. The CMLM has been widely used in NAR machine translation, but it neglects the dependency between the masked positions and suffers from a pretrain-finetune discrepancy. To overcome these limitations, we modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Additionally, we propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence.

The second contribution of this paper is achieving new state-of-the-art undistilled NAR results and approaching AR performance on multiple NMT benchmarks. Our proposed CMLMC model outperforms previous NAR models and approaches the performance of AR models on various NMT benchmarks. This demonstrates the effectiveness of our proposed method in improving NAR machine translation performance.

Related works have shown that pre-training models based on autoregressive language modeling, such as BERT, achieve better performance than pre-training approaches based on non-autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. To address this problem, XLNet proposes a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages, which can produce notable performance gains for both AR and NAR machine translation. JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously and effectively alleviates the problem of distribution discrepancy.

In summary, this paper proposes a novel pre-training method, the Conditional Masked Language Model with Correction (CMLMC), which addresses the problems of NAR machine translation without the need for distillation. Our proposed method achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. The effectiveness of our proposed method is demonstrated through extensive experiments and comparisons with related works.