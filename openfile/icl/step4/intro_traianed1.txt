Introduction:

Neural machine translation (NMT) models have achieved remarkable success in recent years, with the Transformer architecture being the most popular and effective one (Vaswani et al., 2017; Barrault et al., 2019; Huang et al., 2020). However, most of the proposed approaches are based on the autoregressive (AR) principle, which translates one token at a time conditioning on already translated tokens. This approach can be time-consuming, especially for long sequences, and requires a full forward pass through the decoder for each translated token. To address this issue, recent works have explored non-autoregressive (NAR) approaches that translate subsets of tokens in parallel (Gu et al., 2018; Ghazvininejad et al., 2019; Kasai et al., 2020). Although NAR models achieve faster inference speed, they still lag behind AR models in terms of accuracy, and only become competitive when trained with distillation (Ghazvininejad et al., 2019). 

To improve the performance of NAR models, this paper proposes XLNet, a generalized autoregressive pretraining method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. Unlike BERT, XLNet does not suffer from the pretrain-finetune discrepancy, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. 

To further improve the architectural designs for pretraining, this paper incorporates the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. Additionally, this paper proposes a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling. 

Empirical results show that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. These results demonstrate the effectiveness of XLNet in capturing bidirectional context and overcoming the limitations of BERT. 

In related works, ProphetNet (Fan et al., 2020) introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism to optimize n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time 