Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) models have been widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning, while non-autoregressive (NAR) models have shown great potential to reduce inference latency by introducing parallel decoding. However, AR models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction, while NAR models suffer from the multi-modality problem and lack meaningful global structure. To bridge the gap between AR and NAR models, this paper proposes BANG, the first large-scale pre-training model designed for NAR and semi-NAR generation. 

BANG is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. It supports NAR, semi-NAR, and AR fine-tuning to meet different requirements with the same pre-trained model structure. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models.

Previous works have proposed various pre-training models, such as BERT, XLNet, and ProphetNet. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) to address the problem of neglecting dependency among predicted tokens. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. However, these models do not specifically address the gap between AR and NAR models. 

In contrast, BANG is designed to bridge the gap between AR and NAR models by pre-training a generative model. BANG is the first large-scale pre-training model designed for NAR and semi-NAR generation, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models.

In conclusion, this paper proposes BANG, a new pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR models. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models. The proposed model structure and pre-training method can be applied to various natural language processing tasks, providing a new approach for improving the performance of AR and NAR models.