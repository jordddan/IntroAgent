The field of natural language processing has seen significant advancements in recent years, particularly in the area of pre-trained language models. Autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives. AR language modeling estimates the probability distribution of a text corpus with an autoregressive model, while AE-based pretraining aims to reconstruct the original data from corrupted input. However, AR language modeling is only trained to encode a unidirectional context, while downstream language understanding tasks often require bidirectional context information. On the other hand, AE-based pretraining suffers from the pretrain-finetune discrepancy, where the predicted tokens are masked in the input, and the model is not able to model the joint probability using the product rule as in AR language modeling.

To address these limitations, this paper introduces XLNet, a generalized autoregressive method that combines the best of both AR language modeling and AE while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. Unlike BERT, XLNet does not suffer from the pretrain-finetune discrepancy, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

Furthermore, this paper proposes a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling. The paper also improves architectural designs for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences.

Empirical results demonstrate that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Related works have explored permutation-based AR modeling, but previous models aim to improve density estimation by baking an "orderless" inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts. Other related works have proposed joint autoregressive and non-autoregressive training, attention-based models, and sequence-to-sequence models. However, XLNet's approach of combining the best of both AR language modeling and AE while avoiding their limitations is unique and effective.

In summary, this paper introduces XLNet, a generalized autoregressive method that combines the best of both AR language modeling and AE while avoiding their limitations. The paper proposes a reparameterization of the Transformer(-XL) network, improves architectural designs for pretraining, and demonstrates that XLNet consistently outperforms BERT on a wide spectrum of problems. The related works have explored various approaches, but XLNet's approach is unique and effective.