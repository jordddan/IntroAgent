Introduction:

Neural Machine Translation (NMT) has achieved remarkable progress in recent years, with Transformer-based models being the state-of-the-art (Vaswani et al., 2017; Barrault et al., 2019; Huang et al., 2020). However, the Autoregressive NMT (AT) approach, which generates translations one token at a time, suffers from slow inference speed, especially for long sequences. To address this issue, Non-autoregressive NMT (NAT) models have been proposed, which generate tokens in parallel and achieve faster inference speed. However, NAT models generally come with a decrease in accuracy compared to AT models. In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks.

Related Works:

Previous works have explored pre-training models for NMT, such as JANUS (Liang et al., 2021), Universal Conditional Masked Language Pre-training (Zhang et al., 2021), and BANG (Gong et al., 2021). JANUS proposes a joint autoregressive and non-autoregressive training method to enhance the model performance in both AR and NAR manner simultaneously. Universal Conditional Masked Language Pre-training introduces a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. BANG proposes a new pre-training model to bridge the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training. 

Contributions:

Our proposed CeMAT model has three main contributions. Firstly, CeMAT is pre-trained on both monolingual and bilingual corpora, which enables it to provide unified initialization parameters for both AT and NAT tasks. Secondly, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. Thirdly, we demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.

In conclusion, our proposed CeMAT model provides a promising approach for pre-training models for machine translation. The aligned code-switching & masking and dynamic dual-masking techniques enhance the model training under the setting of bidirectional decoders, and the model achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.