Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) models have been widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning, while non-autoregressive (NAR) models have shown great potential to reduce inference latency by introducing parallel decoding. However, AR models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction, while NAR models suffer from the multi-modality problem and lack meaningful global structure. To bridge the gap between AR and NAR models, this paper proposes BANG, the first large-scale pre-training model designed for NAR and semi-NAR generation.

BANG is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. It supports NAR, semi-NAR, and AR fine-tuning to meet different requirements with the same pre-trained model structure. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models.

Previous works have proposed various pre-training models, such as BERT, XLNet, and ProphetNet. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) to address the problem of neglecting dependency among predicted tokens. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. However, these models do not specifically address the gap between AR and NAR models.

To tackle this problem, BANG proposes a novel model structure for large-scale pre-training, which bridges AR and NAR models. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong NAR baselines.

In summary, this paper proposes BANG, a novel pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR models. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models. The proposed model structure and pre-training method can be applied to various natural language processing tasks, providing a promising direction for future research.