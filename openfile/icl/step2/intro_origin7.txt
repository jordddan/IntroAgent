INTRODUCTION

Pre-training methods have significantly improved the performance of neural machine translation (NMT) models. However, existing pre-training methods such as Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have their limitations. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations.

The proposed MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts. This approach allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. 

We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. The experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Related works have explored various pre-training methods for NMT models. For instance, JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously and effectively alleviates the problem of distribution discrepancy. Universal Conditional Masked Language Pre-training (CeMAT) is a pre-training method that pre-trains a sequence-to-sequence model but with a bidirectional decoder, which produces notable performance gains for both Autoregressive and Non-autoregressive NMT. BANG is a pre-training model that bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. ProphetNet is a sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. Finally, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems.

In summary, our proposed MPNet pre-training method unifies the advantages of MLM and PLM while addressing their limitations. The experimental results show that MPNet outperforms previous well-known models on various downstream benchmark tasks.