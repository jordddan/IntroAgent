Introduction:

Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT) in recent years. However, non-autoregressive (NAR) machine translation models have been lagging behind their autoregressive (AR) counterparts, and only become competitive when trained with distillation. In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC), which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation.

The first contribution of this paper is the proposal of CMLMC, which modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. This modification allows the model to capture bidirectional context information, which is essential for NMT tasks.

The second contribution of this paper is the introduction of a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. This correction loss is designed to prevent overfitting on strong local correlations and encourages the model to plan for future tokens.

The third contribution of this paper is achieving new state-of-the-art undistilled NAR results and approaching AR performance on multiple NMT benchmarks. This demonstrates the effectiveness of the proposed CMLMC model and the correction loss.

Related works have explored various pre-training methods for NMT, including autoregressive (AR) language modeling and autoencoding (AE). AR language modeling factorizes the likelihood into a forward or backward product, while AE-based pretraining aims to reconstruct the original data from corrupted input. Attention-based models have also been proposed, such as the Transformer, which is based solely on attention mechanisms and has shown superior performance in quality and parallelizability.

Other related works have proposed pre-training models that combine AR and NAR training, such as JANUS and Universal Conditional Masked Language Pre-training. JANUS is a joint AR and NAR training method that uses an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. Universal Conditional Masked Language Pre-training pre-trains a sequence-to-sequence model with a bidirectional decoder, which produces notable performance gains for both AR and NAR NMT.

ProphetNet is another pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT. BANG is a pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining.

In conclusion, this paper proposes a novel pre-training method, CMLMC, which addresses the shortcomings of CMLM in NAR machine translation. The proposed model achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. The related works have explored various pre-training methods for NMT, including AR language modeling, autoencoding, attention-based models, and models that combine AR and NAR training.