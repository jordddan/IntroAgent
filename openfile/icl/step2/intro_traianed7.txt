INTRODUCTION

Pre-training methods have significantly improved the performance of neural network models in various natural language processing tasks. However, existing pre-training methods such as Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have their limitations. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations.

In the first contribution, we introduce a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet.

In the second contribution, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to reduce the position discrepancy. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB.

In the third contribution, we evaluate the performance of MPNet on various downstream benchmark tasks and compare it with MLM, PLM, and previous well-known models such as BERT, XLNet, and RoBERTa. Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Related works have explored various pre-training methods for neural network models in natural language processing tasks. Attention Is All You Need proposed a new simple network architecture, the Transformer, based solely on attention mechanisms, which achieved superior performance in machine translation tasks. Universal Conditional Masked Language Pre-training proposed CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. XLNet proposed a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. ProphetNet presented a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigated possible reasons behind the performance gap between autoregressive and non-autoregressive models and proposed the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems.

In summary, our proposed MPNet pre-training method unifies the advantages of MLM and PLM while addressing their limitations. Our experiments show that MPNet outperforms previous well-known models on various downstream benchmark tasks. The related works have explored various pre-training methods for neural network models in natural language processing tasks, and our proposed method contributes to this field by providing a new approach that leverages the dependency among predicted tokens through permuted language modeling and auxiliary position information.