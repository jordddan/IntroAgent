Paragraph 1: Establishing the motivation for the research and explaining its importance and relevance to the AI community.

Sequence generation tasks, such as machine translation, have been widely studied in the field of artificial intelligence. Autoregressive (AR) models, which generate one token at a time conditioned on the previous tokens, have achieved state-of-the-art performance in these tasks. However, AR models suffer from slow decoding speed due to their sequential nature. Non-autoregressive (NAR) models, which generate tokens in parallel, have been proposed to address this issue. Although NAR models have faster decoding speed, they often suffer from inferior performance compared to AR models. Therefore, there is a need to develop a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

Paragraph 2: Clearly stating the problem the paper is addressing, the proposed solution, and the specific research questions or objectives.

In this paper, we propose JANUS, a method that combines AR and NAR models to improve performance in sequence generation tasks. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. The specific research questions addressed in this paper are: Can JANUS improve the performance of both AR and NAR models? Can JANUS achieve comparable performance to the state-of-the-art NAR model without distillation data? Can JANUS improve the performance of AR models?

Paragraph 3: Briefly mentioning key related work for context.

Previous works have attempted to combine AR and NAR models to improve performance in sequence generation tasks. BANG proposed a pre-training model that bridges the gap between AR and NAR models. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigated possible reasons behind the performance gap between AR and NAR models and proposed a method to address these problems. However, these methods still suffer from inferior performance compared to AR models. JANUS aims to address this issue by introducing an auxiliary distribution to bridge the discrepancy between AR and NAR models.

Paragraph 4: Explaining the main differences from the related work.

Compared to previous works, JANUS introduces an auxiliary distribution to bridge the discrepancy between AR and NAR models, which benefits AR and NAR to learn from each other. JANUS also demonstrates its effectiveness on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. JANUS also exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

In conclusion, this paper proposes JANUS, a method that combines the strengths of both AR and NAR models to improve performance in sequence generation tasks. JANUS introduces an auxiliary distribution to bridge the discrepancy between AR and NAR models, which benefits AR and NAR to learn from each other. JANUS demonstrates its effectiveness on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. JANUS also exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.