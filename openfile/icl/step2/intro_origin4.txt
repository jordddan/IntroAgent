Introduction:

Sequence-to-sequence (Seq2Seq) models have been widely used in natural language generation (NLG) tasks, such as machine translation, summarization, and question generation. Pre-training Seq2Seq models on large-scale corpora has been shown to significantly improve their performance. However, existing pre-training methods, such as BERT and XLNet, are based on autoregressive (AR) language modeling, which generates one token at a time and can be time-consuming, especially for long sequences. Non-autoregressive (NAR) models have been proposed to address this issue by generating tokens in parallel. However, NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. 

To address these issues, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction. Unlike traditional Seq2Seq models that optimize one-step-ahead prediction, ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. 

To achieve this, the paper develops a method to simultaneously predict the future n-gram at each time step during the training phase, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. This extends the two-stream self-attention proposed in XLNet to n-stream self-attention. 

ProphetNet is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. The paper also fine-tunes ProphetNet on several NLG tasks and achieves the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Related works have explored various pre-training methods for Seq2Seq models. BERT and XLNet are two widely used pre-training methods based on AR language modeling. JANUS proposes a joint AR and NAR training method to enhance the model performance in both AR and NAR manner simultaneously. CeMAT pre-trains a sequence-to-sequence model with a bidirectional decoder to produce notable performance gains for both AR and NAR NMT. BANG bridges the gap between AR and NAR by designing a novel model structure for large-scale pretraining. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes a new pre-training method that addresses these problems. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence, achieving better results on downstream tasks compared with previous state-of-the-art pre-trained methods. 

In summary, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction and a method to simultaneously predict the future n-gram at each time step during the training phase. The paper extends the two-stream self-attention proposed in XLNet to n-stream self-attention and achieves new state-of-the-art results on CNN/DailyMail and Gigaword. The related works have explored various pre-training methods for Seq2Seq models, but ProphetNet outperforms them on several NLG tasks.