Introduction:

Neural Machine Translation (NMT) has achieved remarkable progress in recent years, with Transformer-based models being the state-of-the-art (Vaswani et al., 2017; Barrault et al., 2019; Huang et al., 2020). However, the Autoregressive NMT (AT) approach, which generates translations one token at a time, suffers from slow inference speed, especially for long sequences. To address this issue, Non-autoregressive NMT (NAT) models have been proposed, which generate tokens in parallel and achieve faster inference speed. However, NAT models generally come with a decrease in accuracy compared to AT models. To balance the trade-off between speed and accuracy, Semi-NAR models have been proposed (Stern et al., 2019; Lee et al., 2018; Gu et al., 2019; Ghazvininejad et al., 2019). 

In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. CeMAT is designed to improve the performance of both AT and NAT models, and to provide a unified model for fine-tuning on both NMT tasks. 

To enhance the model training under the setting of bidirectional decoders, we introduce two techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques are designed to improve the model's ability to handle bidirectional decoding and to better capture the dependencies between source and target languages.

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Specifically, CeMAT achieves up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, CeMAT produces consistent performance gains, up to +5.3 BLEU. 

Related Works:

Previous works have proposed various pre-training methods for NMT. Attention Is All You Need (Vaswani et al., 2017) proposed a Transformer-based model that achieved state-of-the-art performance on machine translation tasks. Universal Conditional Masked Language Pre-training (Guo et al., 2020) proposed a pre-training model that can be fine-tuned on both AT and NAT tasks. BANG (Liang et al., 2021) proposed a pre-training model that bridges the gap between AR and NAR via pre-training a generative model. ProphetNet (Qi et al., 2020) introduced a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet (Yang et al., 2019) proposed a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION (Ding et al., 2021) proposed the Conditional Masked Language Model with Correction (CMLMC) that addresses the problems of indistinguishability of tokens and mismatch between training and inference. MPNet (Liu et al., 2021) proposed a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. 

In summary, while previous works have proposed various pre-training methods for NMT, CeMAT is the first pre-training model that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. CeMAT also introduces aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Empirical results show that CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.