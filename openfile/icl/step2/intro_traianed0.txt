Introduction:

Sequence generation tasks, such as machine translation, have been dominated by autoregressive (AR) models and non-autoregressive (NAR) models. AR models generate output tokens sequentially, while NAR models generate all output tokens in parallel. AR models achieve high performance but suffer from slow decoding speed, while NAR models have fast decoding speed but often sacrifice performance. To address these issues, this paper proposes JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. Specifically, P AUX is used to generate additional training samples to augment the original training data, which helps to alleviate the distribution discrepancy between AR and NAR models. This approach allows JANUS to achieve similar results to the state-of-the-art NAR model without distillation data and improve the AR model performance by more than 1.5 BLEU scores on average.

To demonstrate the effectiveness of JANUS, experiments are conducted on multiple NMT datasets and autoregressive pretraining models. The results show that JANUS achieves significant improvements over the AR model and comparable performance with the NAR model. Moreover, JANUS exceeds the NAR pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR model at least two times speedup based on the iterative inference mechanism.

Related works have shown that attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. The Transformer, a new neural network architecture for sequence modeling and transduction problems, relies entirely on an attention mechanism to draw global dependencies between input and output, without the need for recurrent networks. The proposed scaled dot-product attention, multi-head attention, and the parameter-free position representation are key components of the Transformer architecture. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.

Other related works have proposed pre-trained sequence-to-sequence models that have significantly improved Neural Machine Translation (NMT). CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. CeMAT achieves significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, CeMAT can also produce consistent performance gains, i.e., up to +5.3 BLEU. BANG is a new pre-training model that bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pre-trained models. ProphetNet is a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet is a novel pre-training method that leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy.

In summary, this paper proposes JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. The effectiveness of JANUS is demonstrated on multiple NMT datasets and autoregressive pretraining models. Related works have shown that attention mechanisms and pre-trained sequence-to-sequence models have significantly improved NMT performance.