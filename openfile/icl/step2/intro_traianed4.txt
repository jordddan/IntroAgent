Introduction:

Sequence-to-sequence (Seq2Seq) models have been widely used in natural language generation (NLG) tasks, such as machine translation, summarization, and question generation. However, traditional Seq2Seq models suffer from the problem of overfitting on strong local correlations and lack of planning for future tokens. To address these issues, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and the proposed n-stream self-attention mechanism.

The first contribution of this paper is introducing ProphetNet, which is pre-trained using a novel self-supervised objective future n-gram prediction. Unlike traditional Seq2Seq models that optimize one-step-ahead prediction, ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. This approach encourages the model to plan for future tokens and prevents overfitting on strong local correlations.

The second contribution of this paper is developing a method to simultaneously predict the future n-gram at each time step during the training phase. This method encourages the model to plan for future tokens and prevents overfitting on strong local correlations. The proposed method is achieved by extending the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

The third contribution of this paper is pre-training ProphetNet on two scale pre-trained datasets and achieving new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. This demonstrates the effectiveness of the proposed self-supervised objective and n-stream self-attention mechanism.

The fourth contribution of this paper is fine-tuning ProphetNet on several NLG tasks and achieving the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. This demonstrates the effectiveness of the proposed self-supervised objective and n-stream self-attention mechanism in downstream tasks.

In related works, previous studies have proposed various pre-training methods for Seq2Seq models, such as BERT, XLNet, and CeMAT. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) for pre-training to address the problem of neglecting dependency among predicted tokens. CeMAT proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. However, these methods do not consider future n-gram prediction and n-stream self-attention mechanism, which are the main contributions of this paper.

In conclusion, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction and the proposed n-stream self-attention mechanism. The proposed model achieves new state-of-the-art results on CNN/DailyMail and Gigaword, and the best performance on several NLG tasks. The proposed self-supervised objective and n-stream self-attention mechanism provide a new perspective for Seq2Seq pre-training and downstream NLG tasks.