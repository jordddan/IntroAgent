INTRODUCTION

Large-scale pre-trained language models have achieved remarkable success in various downstream tasks, including machine translation, summarization, and question generation. Autoregressive (AR) models have been widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning, but they can be time-consuming, especially for long sequences. To address this issue, recent works have explored non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. However, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper, we propose BANG, a new pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pre-training a generative model.

In recent years, pre-training models have been widely used to improve the performance of downstream tasks. However, most pre-training models are based on AR models, which can be time-consuming for long sequences. To address this issue, we propose BANG, the first large-scale pre-training model designed for NAR and semi-NAR generation. BANG is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. This approach bridges the gap between AR and NAR by pre-training a generative model that can support both AR and NAR generation.

To achieve this, we propose a novel model structure for large-scale pre-training that can simultaneously support AR, NAR, and semi-NAR generation to meet different requirements. BANG supports NAR, semi-NAR, and AR fine-tuning to meet different requirements with the same pre-trained model structure. We conduct extensive experiments and show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models.

Our proposed approach builds on related works in the field of pre-training models. Universal Conditional Masked Language Pre-training proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. JANUS proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. ProphetNet presents a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. These works have made significant contributions to the field of pre-training models, but they do not address the specific challenges of NAR and semi-NAR generation.

In conclusion, we propose BANG, a new pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pre-training a generative model. BANG is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. Our experiments show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models.