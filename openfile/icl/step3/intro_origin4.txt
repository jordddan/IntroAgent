Introduction:

Pre-training models have shown significant improvements in natural language generation tasks. However, most pre-training works are based on autoregressive (AR) language models, which can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) models have been proposed to reduce generation latency. In this paper, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. 

The main contributions of this paper are as follows. Firstly, we propose a new pre-training model called ProphetNet, which is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. Secondly, we develop a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. Thirdly, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. Fourthly, we pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Finally, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Previous works have proposed various pre-training methods based on Transformer and designed with AR language models. However, the latency of AR models is a well-known limitation for online real-time usage. To address this issue, NAR models have been proposed to reduce generation latency. Although NAR models can generate tokens in parallel, they generally come with a much lower inference latency, but a decrease in accuracy. In order to balance latency and accuracy, semi-NAR generation models have been proposed. However, most of the NAR and semi-NAR models focus on translation tasks rather than general natural language generation tasks. 

To bridge the gap between AR and NAR, we propose ProphetNet, a new large-scale pre-trained Seq2Seq model with a novel self-supervised objective future n-gram prediction. This model is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. We also develop a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. 

To achieve an efficient implementation for multiple arbitrary alternatives in a same output sequence, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. 

Finally, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. Our results demonstrate that ProphetNet is a promising model for natural language generation tasks, and our proposed method can effectively bridge the gap between AR and NAR models.