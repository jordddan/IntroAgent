Introduction:

Machine translation has been a popular research topic in natural language processing (NLP) for decades. Recently, transformer-based models have achieved significant improvements in machine translation tasks, especially the autoregressive (AR) generation model (Vaswani et al., 2017; So et al., 2019). However, the AR model translates one token at a time, which can be time-consuming, especially for long sequences. To accelerate inference, non-autoregressive (NAR) approaches have been explored, which translate blocks of tokens in parallel (Gu et al., 2018; Lee et al., 2018; Libovick√Ω and Helcl, 2018; Su et al., 2021). Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation (Guo et al., 2020; Zhu et al., 2020).

To address this issue, we propose JANUS, a Joint Autoregressive and Non-autoregressive training method using an Auxiliary loss to enhance the model performance in both AR and NAR manners simultaneously and effectively alleviate the problem of distribution discrepancy (Li et al., 2021). JANUS combines the strengths of both AR and NAR generation models while avoiding their weaknesses to improve performance. Specifically, we propose an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. 

In this paper, we demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Furthermore, we exceed the non-autoregressive pretraining model BANG on the same GLGE tasks and achieve comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Prior works have explored various pre-training models for machine translation tasks. For example, CeMAT is a pre-training model that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks (Liu et al., 2020). BANG is another pre-training model that bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models (Zhang et al., 2021). 

In conclusion, this paper proposes JANUS, a Joint Autoregressive and Non-autoregressive training method using an Auxiliary loss to enhance the model performance in both AR and NAR manners simultaneously. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.