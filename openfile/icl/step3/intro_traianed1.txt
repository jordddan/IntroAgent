Introduction:

Machine translation has been a popular research area in natural language processing (NLP) for decades. Recently, pre-training models have shown significant improvements in machine translation performance. However, most pre-training models are designed for either Autoregressive NMT (AT) or Non-autoregressive NMT (NAT) tasks, and there is a lack of a unified model that can provide initialization parameters for both tasks. In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks.

Related works have shown that pre-training models with a bidirectional decoder can produce notable performance gains for both AT and NAT tasks. However, the bidirectional decoder setting poses challenges for model training, such as the indistinguishability of tokens and the mismatch between training and inference. To address these challenges, we introduce two simple but effective techniques to enhance CeMAT's training under the setting of bidirectional decoders. The first technique is aligned code-switching & masking, which is applied based on a multilingual translation dictionary and word alignment between source and target sentences. The second technique is dynamic dual-masking, which is used to mask the contents of the source and target languages.

We conduct extensive experiments to demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our experiments show that CeMAT outperforms the state-of-the-art models on multiple datasets, including low-resource languages, and achieves up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, CeMAT produces consistent performance gains, achieving up to +5.3 BLEU. 

In summary, this paper proposes CeMAT, a pre-training model for machine translation that provides unified initialization parameters for both Autoregressive NMT and Non-autoregressive NMT tasks. We introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Our experiments demonstrate that CeMAT achieves significant improvements in machine translation performance on both low-resource and high-resource settings, outperforming state-of-the-art models on multiple datasets.