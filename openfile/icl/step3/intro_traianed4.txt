Introduction:

Pre-training models have shown significant improvements in natural language generation tasks. However, most pre-training works are based on autoregressive (AR) language models, which can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) models have been proposed to reduce generation latency. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper, we propose ProphetNet, a new large-scale pre-trained Seq2Seq model with a novel self-supervised objective future n-gram prediction, which bridges the gap between AR and NAR via pre-training a generative model.

ProphetNet is pre-trained using a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. This is achieved by optimizing n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations.

To extend the two-stream self-attention proposed in XLNet to n-stream self-attention, we introduce a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. This allows ProphetNet to predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations.

ProphetNet is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Fine-tuning ProphetNet on several NLG tasks, including CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks, achieves the best performance compared to the models using the same base scale pre-training dataset.

In conclusion, this paper introduces a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. The method to simultaneously predict the future n-gram at each time step during the training phase encourages the model to plan for future tokens and prevents overfitting on strong local correlations. The n-stream self-attention allows ProphetNet to predict the future n-gram at each time step during the training phase. Pre-training ProphetNet on two scale pre-trained datasets achieves new state-of-the-art results on CNN/DailyMail and Gigaword, and fine-tuning ProphetNet on several NLG tasks achieves the best performance compared to the models using the same base scale pre-training dataset.