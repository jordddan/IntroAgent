Introduction:

Machine learning has revolutionized the field of natural language processing (NLP) by providing state-of-the-art models for various NLP tasks. Pre-training models have been widely used to improve the performance of NLP models. However, existing pre-training methods such as Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have their limitations. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations.

Related works have shown that pre-training models with a bidirectional decoder can produce notable performance gains for both Autoregressive (AR) and Non-autoregressive (NAR) NLP tasks. CeMAT is a pre-training model that uses a bidirectional decoder and has shown significant improvements in both AR and NAR tasks. JANUS is another pre-training model that uses a joint AR and NAR training method to improve performance in both AR and NAR tasks. BANG is a pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training. ProphetNet is a pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism.

Our proposed MPNet pre-training method introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB, which outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

In summary, this paper proposes a new pre-training method called MPNet that unifies the advantages of both MLM and PLM while addressing their limitations. We introduce a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. Our experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa on various downstream benchmark tasks.