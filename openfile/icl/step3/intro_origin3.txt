INTRODUCTION

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) models have been widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning, but they can be time-consuming, especially for long sequences. To address this issue, recent works have explored non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. However, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper, we propose BANG, a new pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pre-training a generative model.

MOTIVATION

The motivation behind BANG is to improve the performance of NAR models and bridge the gap between AR and NAR models. AR models translate one token at a time, which can be time-consuming, especially for long sequences. NAR models translate blocks of tokens in parallel, but they still lag behind AR models in terms of performance. BANG aims to address these issues by pre-training a generative model that supports NAR, semi-NAR, and AR fine-tuning to meet different requirements with the same pre-trained model structure.

RELATED WORKS

Several related works have explored pre-training models for NAR and AR generation. Universal Conditional Masked Language Pre-training proposed a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. JANUS proposed a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. ProphetNet introduced a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet leveraged the dependency among predicted tokens through permuted language modeling and took auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. Attention Is All You Need proposed a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. XLNet proposed a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation.

CONTRIBUTIONS

The main contributions of this paper are:

1. BANG is proposed as the first large-scale pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pre-training a generative model.

2. BANG is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK].

3. BANG supports NAR, semi-NAR, and AR fine-tuning to meet different requirements with the same pre-trained model structure.

4. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with the comparison to strong AR pre-trained models.

In the following sections, we will describe the proposed BANG model in detail, including the pre-training process, the model structure, and the fine-tuning process. We will also present the experimental results and analysis to demonstrate the effectiveness of BANG.