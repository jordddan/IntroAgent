Introduction:

Pre-training language models have significantly improved the performance of neural machine translation (NMT) tasks. However, the traditional autoregressive (AR) framework translates one token at a time, which can be time-consuming, especially for long sequences. To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts and only become competitive when trained with distillation. 

To address this performance gap, this paper proposes the Conditional Masked Language Model with Correction (CMLMC), which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. CMLMC modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Additionally, CMLMC proposes a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 

The proposed CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. To investigate possible reasons behind the performance gap between AR and NAR models, Universal Conditional Masked Language Pre-training and JANUS have been proposed. Universal Conditional Masked Language Pre-training demonstrates that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both AR and NAR NMT. JANUS proposes a joint AR and NAR training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. 

BANG and ProphetNet are two other pre-training models that have been proposed to bridge the gap between AR and NAR generation. BANG introduces a novel model structure for large-scale pre-training that bridges AR and NAR generation by designing a model structure that can simultaneously support AR, NAR, and semi-NAR generation to meet different requirements. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. 

Finally, MPNet is proposed, which inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. MPNet achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. 

In summary, this paper proposes CMLMC, which addresses the shortcomings of CMLM in NAR machine translation, and achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. Additionally, this paper investigates possible reasons behind the performance gap between AR and NAR models and proposes several pre-training models that bridge the gap between AR and NAR generation. Finally, MPNet is proposed, which achieves better results on these tasks compared with previous state-of-the-art pre-trained methods.