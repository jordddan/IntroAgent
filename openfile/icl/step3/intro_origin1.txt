Introduction:

Machine translation has been a long-standing challenge in natural language processing (NLP). Recently, pre-training models have shown significant improvements in machine translation performance. However, most pre-training models are designed for either Autoregressive NMT (AT) or Non-autoregressive NMT (NAT) tasks, and there is a lack of a unified model that can provide initialization parameters for both tasks. In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. 

To enhance the model training under the setting of bidirectional decoders, we introduce two techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques are designed to improve the model's ability to handle bidirectional decoding and to better capture the dependencies between source and target languages.

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our experiments show that CeMAT outperforms the state-of-the-art models on several machine translation benchmarks, including WMT14 English-German and WMT14 English-French. 

Related works have explored various pre-training methods for machine translation. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION proposed a Conditional Masked Language Model with Correction (CMLMC) that achieves state-of-the-art NAR performance when trained on raw data without distillation. Universal Conditional Masked Language Pre-training demonstrated that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both AT and NAT tasks. JANUS proposed a Joint Autoregressive and Non-autoregressive training method using an Auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. BANG proposed a new pre-training model to bridge the gap between AR and NAR generation. ProphetNet introduced a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. 

In summary, our contribution is the development of CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. We introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. Our experiments show that CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.