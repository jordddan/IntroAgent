Introduction:

Sequence modeling and transduction problems have been widely studied in the field of machine learning. Recurrent neural networks (RNNs) have been the dominant approach for these tasks, but they suffer from slow training and limited parallelization. In this paper, we introduce the Transformer, a new neural network architecture that relies entirely on an attention mechanism to draw global dependencies between input and output, without the need for recurrent networks.

The Transformer architecture allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. This is a significant improvement over previous approaches that relied on RNNs.

The key components of the Transformer architecture are scaled dot-product attention, multi-head attention, and the parameter-free position representation. Scaled dot-product attention allows the model to attend to all positions in the input sequence simultaneously, while multi-head attention enables the model to jointly attend to information from different representation subspaces at different positions. The parameter-free position representation allows the model to encode the position information of the input sequence without requiring any additional parameters.

To implement and evaluate the Transformer, we developed tensor2tensor, a library for training deep learning models in a distributed and efficient manner. Tensor2tensor was used to pre-train the Transformer on a large corpus and fine-tune it on translation tasks.

In summary, the main contributions of this paper are: (1) the introduction of the Transformer, a new neural network architecture for sequence modeling and transduction problems that relies entirely on an attention mechanism to draw global dependencies between input and output, (2) the demonstration that the Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs, (3) the proposal of scaled dot-product attention, multi-head attention, and the parameter-free position representation, which are key components of the Transformer architecture, and (4) the development of tensor2tensor, a library for training deep learning models in a distributed and efficient manner, which was used to implement and evaluate the Transformer. 

Related works:

Prior works have explored various approaches to sequence modeling and transduction problems. Autoregressive (AR) machine translation models have achieved significant performance improvements, but they translate one token at a time, which can be time-consuming, especially for long sequences. Recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel, but leading NAR models still lag behind their AR counterparts. To address this problem, Conditional Masked Language Model with Correction (CMLMC) was proposed, which achieved state-of-the-art NAR performance when trained on raw data without distillation.

Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, CeMAT can also produce consistent performance gains, i.e., up to +5.3 BLEU.

XLNet is a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.

In summary, prior works have explored various approaches to sequence modeling and transduction problems, including AR and NAR models for machine translation and pre-trained sequence-to-sequence models. XLNet is a generalized autoregressive pretraining method that outperforms BERT on various tasks.