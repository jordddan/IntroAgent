Introduction:

Transformer-based autoregressive (AR) and non-autoregressive (NAR) models have been widely used in sequence generation tasks. While AR models achieve high-quality results, they suffer from slow inference due to their sequential nature. On the other hand, NAR models can generate sequences in parallel, but they often lack accuracy and diversity in their output. In this paper, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

To achieve this, we introduce an auxiliary distribution P AUX that bridges the discrepancy between P AR and P NAR, allowing AR and NAR models to learn from each other. Specifically, we propose a method that regularizes the model predictions by minimizing the distribution distance between the output generated by the two manners. This approach benefits AR and NAR to learn from each other with the following advantages: (1) P AUX possesses rich context information compared with AR and NAR, (2) there is an autoregressive dependency between the predicted tokens, and (3) the random mask mechanism applied to this distribution enables the model to learn various token distributions.

We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models. Our experiments show that JANUS achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average. Furthermore, we exceed the non-autoregressive pretraining model BANG on the same GLGE tasks and achieve comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Related works have explored various approaches to improve the performance of NAR models. For example, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION proposed the Conditional Masked Language Model with Correction (CMLMC) that addresses the problems of indistinguishability of tokens and mismatch between training and inference. Universal Conditional Masked Language Pre-training introduced CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. BANG: Bridging Autoregressive and Non-autoregressive Generation proposed a new pre-training model to bridge the gap between AR and NAR generation. 

In conclusion, our proposed JANUS method combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance. The auxiliary distribution P AUX bridges the discrepancy between P AR and P NAR, allowing AR and NAR models to learn from each other. Our experiments demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving state-of-the-art performance without distillation data.