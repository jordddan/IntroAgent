Introduction:

Pre-training language models have revolutionized natural language processing (NLP) tasks in recent years. Among them, BERT has been one of the most successful models, which adopts masked language modeling (MLM) for pre-training. However, BERT neglects the dependency among predicted tokens, which limits its performance. To address this issue, XLNet is proposed in this paper, which introduces permuted language modeling (PLM) for pre-training to capture the dependency among predicted tokens. 

XLNet is a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach eliminates the independence assumption made in BERT and provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens.

In addition to introducing XLNet, this paper also improves architectural designs for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. Furthermore, a reparameterization of the Transformer(-XL) network is proposed to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Empirical results show that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. This paper also compares XLNet with other related works, such as ProphetNet, MPNet, and BANG, which have proposed different approaches to improve pre-training models for NLP tasks. 

In summary, this paper proposes XLNet, a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. The proposed method improves the performance of pre-training models for NLP tasks and outperforms BERT on a wide range of problems. The following sections will provide more details on the proposed method and experimental results.