Introduction:

Sequence generation tasks have been widely studied in natural language processing, and both autoregressive (AR) and non-autoregressive (NAR) models have been proposed to tackle these tasks. While AR models achieve high-quality results, they suffer from slow decoding speed due to their sequential nature. On the other hand, NAR models can generate sequences in parallel, but they often sacrifice accuracy. In this paper, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. Specifically, JANUS maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach enables JANUS to achieve similar results to the state-of-the-art NAR model without distillation data and improve the AR model performance by more than 1.5 BLEU scores on average.

To demonstrate the effectiveness of JANUS, we conduct experiments on multiple NMT datasets and autoregressive pretraining models. The results show that JANUS outperforms the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism. These results demonstrate the potential of JANUS to improve the performance of sequence generation tasks.

Related works have explored different pretraining objectives and architectures for sequence generation tasks. XLNet proposed a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. BERT, on the other hand, adopts masked language modeling for pretraining and achieves better performance than pretraining approaches based on autoregressive language modeling. BANG proposed a new pretraining model to bridge the gap between autoregressive and non-autoregressive generation. Attention Is All You Need proposed a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. ProphetNet introduced a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet leveraged the dependency among predicted tokens through permuted language modeling and took auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigated possible reasons behind the performance gap between non-autoregressive and autoregressive models and proposed the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Universal Conditional Masked Language Pre-training demonstrated that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT.

In summary, JANUS proposes a novel method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance. The auxiliary distribution P AUX bridges the discrepancy between P AR and P NAR, and the experiments demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models. The related works have explored different pretraining objectives and architectures for sequence generation tasks, and JANUS contributes to this field by proposing a new method that achieves state-of-the-art performance.