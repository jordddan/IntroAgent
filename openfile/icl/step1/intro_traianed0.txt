Introduction:

Sequence generation tasks have been widely studied in natural language processing, and both autoregressive (AR) and non-autoregressive (NAR) models have been proposed to address the challenges of these tasks. While AR models achieve high-quality results, they suffer from slow decoding speed due to their sequential nature. On the other hand, NAR models generate tokens in parallel, but their performance is often inferior to AR models due to the difficulty of capturing bidirectional context information. In this paper, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. Specifically, JANUS maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach enables JANUS to overcome the limitations of both AR and NAR models and achieve improved performance.

To evaluate the effectiveness of JANUS, we conduct experiments on multiple neural machine translation (NMT) datasets and autoregressive pretraining models. The results show that JANUS achieves similar performance to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average. Moreover, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Related works in this field include XLNet, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. BERT, a pretraining approach based on autoencoding, has achieved better performance than pretraining approaches based on autoregressive language modeling. However, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. BANG is a pretraining model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining. ProphetNet is a sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet is a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations by leveraging the dependency among predicted tokens through permuted language modeling and taking auxiliary position information as input.

In conclusion, JANUS is a novel method that combines the strengths of both AR and NAR models to improve performance in sequence generation tasks. The proposed auxiliary distribution P AUX bridges the discrepancy between P AR and P NAR, enabling AR and NAR to learn from each other. The experimental results demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average.