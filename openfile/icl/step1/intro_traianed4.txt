Paragraph 1: 
The field of natural language processing has seen significant advancements in recent years, with the development of large-scale pre-trained models that have achieved state-of-the-art results on various tasks. In this paper, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which is designed to improve the performance of sequence-to-sequence tasks. ProphetNet is pre-trained with a novel self-supervised objective future n-gram prediction, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. 

Paragraph 2: 
To achieve this, we develop a method to simultaneously predict the future n-gram at each time step during the training phase. This method encourages the model to plan for future tokens and prevents overfitting on strong local correlations. We extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. 

Paragraph 3: 
We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. We fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. 

Paragraph 4: 
Related works in the field have explored various pre-training objectives and architectures to improve the performance of sequence-to-sequence tasks. XLNet proposed a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. BERT adopts masked language modeling (MLM) for pre-training, while IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION proposed the Conditional Masked Language Model with Correction (CMLMC) that addresses the problems of indistinguishability of tokens and mismatch between training and inference. 

Paragraph 5: 
In addition, related works have explored bridging the gap between autoregressive and non-autoregressive generation, such as JANUS, BANG, and Universal Conditional Masked Language Pre-training. JANUS proposed a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. Universal Conditional Masked Language Pre-training demonstrated that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT.