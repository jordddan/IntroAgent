Paragraph 1: 
Sequence-to-sequence (Seq2Seq) models have become a popular approach for various natural language generation (NLG) tasks, such as machine translation, summarization, and question generation. Pre-training large-scale Seq2Seq models has been shown to be effective in improving the performance of downstream NLG tasks. In this paper, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which achieves state-of-the-art results on several NLG tasks. 

Paragraph 2: 
The main contribution of this paper is the introduction of ProphetNet, a Seq2Seq model that is pre-trained with a novel self-supervised objective called future n-gram prediction. Unlike traditional Seq2Seq models that optimize one-step-ahead prediction, ProphetNet is optimized by n-step ahead prediction, which predicts the next n tokens simultaneously based on previous context tokens at each time step. This approach explicitly encourages the model to plan for future tokens and prevents overfitting on strong local correlations. 

Paragraph 3: 
To achieve the n-step ahead prediction, we develop a method to simultaneously predict the future n-gram at each time step during the training phase. This method encourages the model to plan for future tokens and prevents overfitting on strong local correlations. We extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. 

Paragraph 4: 
We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Fine-tuning ProphetNet on several NLG tasks, we achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. 

Paragraph 5: 
Related works have explored various pre-training objectives and architectures for Seq2Seq models. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) for pre-training. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates the performance gap between autoregressive and non-autoregressive models and proposes a new model that addresses the problems. Universal Conditional Masked Language Pre-training demonstrates that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both autoregressive and non-autoregressive NMT. JANUS: Joint Autoregressive and Non-autoregressive Training proposes a joint training method using an auxiliary loss to enhance the model performance in both autoregressive and non-autoregressive manner simultaneously. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.