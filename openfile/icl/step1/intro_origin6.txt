Pre-training language models have significantly improved the performance of natural language processing (NLP) tasks in recent years. Among them, autoregressive (AR) models and non-autoregressive (NAR) models have been widely used in sequence generation tasks. AR models can achieve excellent performance, but suffer from slow decoding speed, while NAR models bring fast decoding speed for inference, but often sacrifice the quality of generated sequences. 

To address the limitations of NAR models, this paper proposes a new pre-training method called Conditional Masked Language Model with Correction (CMLMC). CMLMC is an extension of the Conditional Masked Language Model (CMLM) that addresses the shortcomings of CMLM in NAR machine translation. Specifically, the decoder structure of CMLM is modified by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Moreover, a novel correction loss is proposed to teach the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 

To evaluate the effectiveness of CMLMC, experiments are conducted on multiple NMT benchmarks. The results show that CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance. 

Related works have proposed various pre-training methods for NLP tasks. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) to capture the dependency among predicted tokens. However, XLNet suffers from position discrepancy between pre-training and fine-tuning. MPNet is a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. It leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. 

In addition, recent works have proposed bridging AR and NAR generation by designing a novel model structure for large-scale pre-training. JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously. BANG bridges AR and NAR generation by introducing a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and achieves new state-of-the-art results on multiple datasets. 

In summary, this paper proposes a new pre-training method CMLMC that addresses the limitations of NAR models in machine translation. The proposed method achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. The related works have proposed various pre-training methods for NLP tasks, and some of them have bridged the gap between AR and NAR generation.