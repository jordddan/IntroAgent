Introduction:

Pre-training language models have significantly improved the performance of neural machine translation (NMT) in recent years. However, the autoregressive (AR) framework of NMT models, which translates one token at a time, can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed, which translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts and only become competitive when trained with distillation. 

In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC), which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. Specifically, we modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 

Our proposed CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. To further improve the performance of NAR models, recent work has been exploring bridging the gap between AR and NAR generation. JANUS [0] proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. BANG [2] bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. 

In addition, recent pre-training models have shown that the bidirectional decoder can produce notable performance gains for both AR and NAR NMT. CeMAT [7] is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for AR NMT. For NAR NMT, CeMAT can also produce consistent performance gains, i.e., up to +5.3 BLEU. 

In terms of pre-training methods, BERT [1] adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. XLNet [4] introduces permuted language modeling (PLM) for pre-training to capture the dependency among the predicted tokens. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations. 

In summary, our proposed CMLMC addresses the shortcomings of CMLM in NAR machine translation and achieves new state-of-the-art results. Recent work has explored bridging the gap between AR and NAR generation, and pre-training models with a bidirectional decoder have shown notable performance gains for both AR and NAR NMT. Our proposed MPNet unifies the advantages of MLM and PLM while addressing their limitations in pre-training.