Introduction:

Pre-training language models have become a popular approach in natural language processing (NLP) tasks, with BERT being one of the most successful models. However, BERT has limitations, such as neglecting the dependency among predicted tokens. To address this issue, XLNet was introduced, which combines the best of both autoregressive language modeling and autoencoding. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. 

XLNet overcomes the limitations of BERT thanks to its autoregressive formulation, which provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. Moreover, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to. 

To further improve pretraining, this paper proposes a new pre-training method called MPNet, which inherits the advantages of BERT and XLNet while avoiding their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling, and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. 

In addition to introducing XLNet and MPNet, this paper also improves architectural designs for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. Furthermore, this paper proposes a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Empirical results show that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Similarly, MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. 

Related works have also explored bridging the gap between autoregressive and non-autoregressive generation, such as JANUS and BANG. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, which has achieved significant performance improvements. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between autoregressive and non-autoregressive models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Universal Conditional Masked Language Pre-training demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. 

In summary, this paper introduces XLNet and MPNet, which improve upon the limitations of BERT and other pre-training methods. The proposed methods have achieved state-of-the-art results on a wide range of NLP tasks. The related works have also explored bridging the gap between autoregressive and non-autoregressive generation, which is an important direction for future research.