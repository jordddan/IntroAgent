Introduction:

Pre-training language models have become a popular approach in natural language processing (NLP) tasks, with BERT being one of the most successful models. However, BERT has limitations, such as neglecting the dependency among predicted tokens. To address this issue, XLNet was introduced, which combines the best of both autoregressive language modeling and autoencoding. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. 

XLNet overcomes the limitations of BERT thanks to its autoregressive formulation, which provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. Moreover, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to. 

To further improve pretraining, this paper proposes a new pre-training method called MPNet, which inherits the advantages of BERT and XLNet while avoiding their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling, and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. 

In addition to introducing XLNet and MPNet, this paper also improves architectural designs for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. Furthermore, this paper proposes a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Empirical results show that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Similarly, MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. 

Related works have also explored bridging the gap between autoregressive and non-autoregressive generation, such as JANUS and BANG, and proposed new sequence-to-sequence pre-training models like ProphetNet. Attention Is All You Need proposed a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigated possible reasons behind the performance gap between autoregressive and non-autoregressive models and proposed the Conditional Masked Language Model with Correction (CMLMC) to address these problems. Universal Conditional Masked Language Pre-training demonstrated that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. 

In summary, this paper introduces XLNet and MPNet, two pre-training methods that overcome the limitations of previous models and improve performance on a wide range of NLP tasks. The proposed methods incorporate ideas from related works and improve architectural designs for pretraining. Empirical results demonstrate the effectiveness of the proposed methods and their superiority over previous state-of-the-art pre-trained methods.