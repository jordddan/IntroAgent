Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) language modeling, which estimates the probability distribution of the text corpus, is widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning. However, AR-based models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction. This issue stems from the fact that local correlations such as bigram combination are usually stronger than long-term dependencies, and teacher forcing, where the model focuses on one-step-ahead prediction for each time step, has no explicit bias toward future token planning and modeling. As a result, the model may learn a bias for language modeling, which leads to overfitting on strong local correlations and underfitting on global coherence and long-term dependency.

To address this issue, we propose BANG, a new pretraining model that bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) generation. BANG is the first large-scale pretraining model designed for NAR and semi-NAR generation, which pretrains a generative model to support predicting tokens with arbitrary previous golden tokens or [MASK]. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

To achieve this, we design a novel model structure for large-scale pretraining that bridges AR and NAR generation. The pretrained BANG model can simultaneously support AR, NAR, and semi-NAR generation to meet different requirements. We conduct experiments on question generation (SQuAD 1.1), summarization (XSum), and dialogue generation (PersonaChat) to show that BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong NAR baselines.

Our work is related to several previous works in the field of natural language processing. XLNet proposed a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. ProphetNet introduced a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. CMLMC proposed the Conditional Masked Language Model with Correction that addresses the problems of indistinguishability of tokens and mismatch between training and inference. CeMAT proposed a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages.

In summary, our proposed BANG model bridges the gap between AR and NAR generation and supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Our experiments show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.