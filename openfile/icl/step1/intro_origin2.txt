Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) language modeling, which estimates the probability distribution of the text corpus, is widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning. However, AR-based models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction. On the other hand, non-autoregressive (NAR) models show great potential to reduce the inference latency by introducing parallel decoding. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. To bridge the gap between AR and NAR, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation.

In this paper, we present the main contributions of BANG. Firstly, BANG is proposed as a generative model that combines the strengths of both AR and NAR models while avoiding their weaknesses. Secondly, BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. Thirdly, BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Finally, BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.

Related works have shown that AR and NAR models have their own advantages and disadvantages. XLNet proposed a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. ProphetNet introduced a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigated possible reasons behind the performance gap between AR and NAR models and proposed the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Universal Conditional Masked Language Pre-training demonstrated that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT.

In conclusion, BANG is a novel pretraining model that bridges the gap between AR and NAR models. It supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Experimental results show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.