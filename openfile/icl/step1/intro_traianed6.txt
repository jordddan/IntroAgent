Introduction:

Pre-training language models have significantly improved the performance of natural language processing (NLP) tasks in recent years. Among them, autoregressive (AR) models and non-autoregressive (NAR) models have been widely used in sequence generation tasks. AR models can achieve excellent performance, but suffer from slow decoding speed, while NAR models bring fast decoding speed for inference but often suffer from inferior performance compared to AR models. In this paper, we propose a new pre-training method called Conditional Masked Language Model with Correction (CMLMC) that addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation.

Previous works have shown that pre-training models based on masked language modeling (MLM) like BERT can achieve better performance than pre-training approaches based on autoregressive language modeling. However, MLM neglects the dependency among the masked tokens and suffers from a pretrain-finetune discrepancy. To improve MLM, XLNet introduces permuted language modeling (PLM) for pre-training to capture the dependency among the predicted tokens. However, PLM has its own limitation: each token can only see its preceding tokens in a permuted sequence but does not know the position information of the full sentence during the autoregressive pre-training, which brings discrepancy between pre-training and fine-tuning. 

To address these issues, we propose CMLMC, which modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Moreover, we propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 

We pre-train CMLMC on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Experimental results show that CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. Our proposed method outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

In summary, our contributions in this paper are: 1) proposing CMLMC, which addresses the shortcomings of CMLM in NAR machine translation, 2) modifying the decoder structure of CMLM and introducing a novel correction loss, 3) achieving new state-of-the-art undistilled NAR results and approaching AR performance on multiple NMT benchmarks. Our proposed method shows great potential for improving the performance of NAR models in sequence generation tasks.