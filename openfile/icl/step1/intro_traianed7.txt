INTRODUCTION

Neural Machine Translation (NMT) has achieved remarkable progress in recent years, with the Transformer architecture being one of the most successful models. However, the autoregressive (AR) approach used in most NMT models has a significant drawback in that it translates one token at a time, which can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed, which translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper, we propose a new pre-training model called CeMAT, which can provide unified initialization parameters for both AR and NAR tasks, and demonstrate its effectiveness in improving machine translation performance on both low-resource and high-resource settings.

The main contributions of this paper are threefold. First, we introduce CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AR and NAR tasks. Second, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. Third, we demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AR and NAR tasks with data of varied sizes.

To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. Our work is related to several recent studies that have proposed novel pre-training methods for sequence-to-sequence models. JANUS (Liang et al., 2021) is a joint autoregressive and non-autoregressive training method that uses an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. BANG (Zhang et al., 2021) is a pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION (Wang et al., 2021) investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Universal Conditional Masked Language Pre-training (Wang et al., 2021) demonstrates that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both AR and NAR NMT.

In conclusion, our proposed CeMAT model and the aligned code-switching & masking and dynamic dual-masking techniques have shown significant improvements in machine translation performance on both low-resource and high-resource settings. Our work contributes to the development of pre-training models for sequence-to-sequence models and provides a unified model for fine-tuning on both AR and NAR tasks.