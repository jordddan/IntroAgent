Introduction:

Neural machine translation (NMT) has made significant progress in recent years, with the introduction of pre-trained sequence-to-sequence models. However, most pre-trained models adopt an unidirectional decoder, which limits their performance in both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. 

To enhance the model training under the setting of bidirectional decoders, we introduce two simple but effective techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques are designed to address the limitations of bidirectional decoders and improve the performance of the model.

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our experiments show that CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks.

Related Works:

Previous works have explored various pre-training models for machine translation. BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. However, XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). 

ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. It achieves new state-of-the-art results on all datasets compared to the models using the same scale pre-training corpus. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It achieves superior quality while being more parallelizable and requiring significantly less time to train. 

BANG proposes a new pre-training model to bridge the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. It improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pre-trained models. JANUS proposes Joint Autoregressive and Non-autoregressive training using an Auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. 

In summary, our proposed CeMAT model is unique in its bidirectional encoder and decoder architecture, and the introduction of aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Our experiments demonstrate that CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.