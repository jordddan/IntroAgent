Introduction:

Pre-training language models have become a popular approach for improving the accuracy of natural language processing tasks. One of the most successful models is BERT, which adopts masked language modeling (MLM) for pre-training. However, BERT neglects the dependency among predicted tokens, which limits its performance. To address this issue, XLNet was introduced, which combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. 

XLNet overcomes the limitations of BERT thanks to its autoregressive formulation, which provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences.

To remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling, a reparameterization of the Transformer(-XL) network was proposed. This reparameterization improves the performance of the model and makes it more effective for pre-training.

Empirical results show that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. 

In recent years, there have been several attempts to bridge the gap between autoregressive and non-autoregressive generation. JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously and effectively alleviates the problem of distribution discrepancy. BANG is a new pre-training model that bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. The pre-trained BANG model can simultaneously support AR, NAR, and semi-NAR generation to meet different requirements. 

ProphetNet is a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevents overfitting on strong local correlations. Empirical results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.

MPNet is another pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. Experimental results show that MPNet outperforms MLM and PLM by a large margin and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting.

In summary, this paper introduces XLNet, a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet overcomes the limitations of BERT and consistently outperforms it on a wide spectrum of problems. Additionally, this paper discusses several attempts to bridge the gap between autoregressive and non-autoregressive generation, including JANUS, BANG, ProphetNet, and MPNet. These methods have shown promising results and have the potential to improve the performance of pre-trained language models.