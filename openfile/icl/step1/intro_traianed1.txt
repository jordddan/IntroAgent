Introduction:

Pre-training language models have revolutionized natural language processing (NLP) tasks in recent years. Among them, BERT has achieved remarkable success by adopting masked language modeling (MLM) for pre-training. However, BERT neglects the dependency among predicted tokens, which limits its performance. To address this issue, XLNet was proposed, which introduces permuted language modeling (PLM) for pre-training to capture the dependency among predicted tokens. However, XLNet does not leverage the full position information of a sentence, which leads to position discrepancy between pre-training and fine-tuning. 

To overcome the limitations of BERT and XLNet, this paper proposes XLNet, a generalized autoregressive pre-training method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. 

XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. To improve architectural designs for pretraining, XLNet incorporates the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. 

To remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling, this paper proposes a reparameterization of the Transformer(-XL) network. Empirical results demonstrate that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. 

Related works have explored bridging the gap between autoregressive and non-autoregressive generation, such as JANUS, which proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Universal Conditional Masked Language Pre-training demonstrates that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. 

In summary, this paper proposes XLNet, a generalized autoregressive pre-training method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet outperforms BERT on a wide spectrum of problems, and the proposed reparameterization of the Transformer(-XL) network removes the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling. The related works have explored bridging the gap between autoregressive and non-autoregressive generation, which is a promising direction for future research.