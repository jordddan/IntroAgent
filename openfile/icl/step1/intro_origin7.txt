INTRODUCTION

Neural Machine Translation (NMT) has achieved remarkable progress in recent years, with the Transformer architecture being one of the most successful models (Vaswani et al., 2017; Barrault et al., 2019; Huang et al., 2020). However, the autoregressive (AR) principle used in most NMT models has a significant drawback: it translates one token at a time, which can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed, which translate blocks of tokens in parallel (Gu et al., 2018; Ghazvininejad et al., 2019; Kasai et al., 2020). Despite the progress made in NAR models, they still lag behind their AR counterparts, and only become competitive when trained with distillation (Ghazvininejad et al., 2019). In this paper, we propose a new pre-training model, CeMAT, that can provide unified initialization parameters for both AR and NAR tasks, and demonstrate its effectiveness in improving machine translation performance on both low-resource and high-resource settings.

The main contributions of this paper are threefold. First, we introduce CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AR and NAR tasks. Second, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. Third, we demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AR and NAR tasks with data of varied sizes.

Related works have explored various pre-training models for NMT. JANUS (Liang et al., 2020) proposed a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. XLNet (Yang et al., 2019) introduced a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. BANG (Wang et al., 2021) proposed a new pre-training model to bridge the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training. ProphetNet (Qi et al., 2020) presented a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named fu