Introduction:

Neural machine translation (NMT) has achieved remarkable progress in recent years, with the Transformer architecture being one of the most successful models (Vaswani et al., 2017). However, the autoregressive (AR) principle used in most NMT models has a significant drawback: it translates one token at a time, which can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed, which translate blocks of tokens in parallel (Gu et al., 2018; Ghazvininejad et al., 2019; Kasai et al., 2020). Despite the progress made in NAR models, they still lag behind their AR counterparts, and only become competitive when trained with distillation (Ghazvininejad et al., 2019). 

In this paper, we propose a new pre-training model for machine translation called CeMAT. CeMAT consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. It is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AR and NAR tasks. CeMAT is designed to address the performance gap between AR and NAR models by leveraging the advantages of both approaches.

To enhance the model training under the setting of bidirectional decoders, we introduce two techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques are simple but effective, and they significantly improve the performance of CeMAT.

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AR and NAR tasks with data of varied sizes. Our experiments show that CeMAT outperforms state-of-the-art models on multiple datasets, including low-resource languages, with up to +14.4 BLEU on low-resource and +7.9 BLEU on average for AR NMT. For NAR NMT, CeMAT also produces consistent performance gains, with up to +5.3 BLEU. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks.

Related works have explored various approaches to improve NMT models, including joint autoregressive and non-autoregressive training (Liang et al., 2021), generalized autoregressive pre-training (Yang et al., 2019), bridging autoregressive and non-autoregressive generation (Zhang et al., 2021), and attention-based models (Vaswani et al., 2017). However, our work is unique in proposing a pre-training model that leverages bidirectional decoders and introducing aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training. 

In summary, our contributions include the development of CeMAT, a pre-training model for machine translation that leverages bidirectional decoders and introduces aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training. We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings, achieving consistent improvements over strong competitors in both AR and NAR tasks with data of varied sizes.