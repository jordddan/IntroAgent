Introduction:

Pre-training models have become a popular approach for improving the performance of sequence-to-sequence tasks. Among the pre-training objectives, Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have been the most successful. However, MLM neglects the dependency among predicted tokens, while PLM does not leverage the full position information of a sentence, leading to a position discrepancy between pre-training and fine-tuning. To address these limitations, this paper proposes a new pre-training method called Masked and Permuted Language Modeling (MPNet), which unifies the advantages of both MLM and PLM while addressing their limitations.

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows the model to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. The model is pre-trained on a large-scale text corpus and fine-tuned on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB, which outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Related works in the field have explored various pre-training objectives and architectures. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. BANG is a new pre-training model that bridges the gap between autoregressive and non-autoregressive generation by designing a novel model structure for large-scale pre-training. ProphetNet is a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. CMLMC is a conditional masked language model that addresses the problems of indistinguishability of tokens and mismatch between training and inference in non-autoregressive machine translation models. CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages, which demonstrates notable performance gains for both autoregressive and non-autoregressive NMT.

In summary, this paper proposes a new pre-training method called MPNet that unifies the advantages of both MLM and PLM while addressing their limitations. The model is pre-trained on a large-scale text corpus and fine-tuned on various downstream benchmark tasks, outperforming previous well-known models. The related works in the field have explored various pre-training objectives and architectures, each with their own strengths and limitations.