INTRODUCTION

Pre-training models have become a popular approach for improving the performance of sequence-to-sequence models in various natural language processing tasks. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations. 

The first contribution of this paper is the proposal of MPNet, which splits the tokens in a sequence into non-predicted and predicted parts, allowing the model to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. This approach unifies the advantages of MLM and PLM while addressing their limitations, resulting in a more effective pre-training method.

The second contribution of this paper is the introduction of a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. This approach unifies the advantages of MLM and PLM while addressing their limitations, resulting in a more effective pre-training method.

The third contribution of this paper is the pre-training of MPNet on a large-scale text corpus and fine-tuning it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. The experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Related works in this field include JANUS, which proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. XLNet introduces a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. BANG bridges AR and NAR generation by designing a novel model structure for large-scale pre-training. ProphetNet presents a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Universal Conditional Masked Language Pre-training proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. 

In summary, this paper proposes a new pre-training method called MPNet that unifies the advantages of both MLM and PLM while addressing their limitations. The experimental results show that MPNet outperforms previous well-known models BERT, XLNet, and RoBERTa by a large margin on various downstream benchmark tasks. The proposed approach has the potential to improve the performance of sequence-to-sequence models in various natural language processing tasks.