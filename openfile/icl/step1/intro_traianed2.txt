Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) language modeling, which estimates the probability distribution of the text corpus, is widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning. However, AR-based models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction. On the other hand, non-autoregressive (NAR) models show great potential to reduce the inference latency by introducing parallel decoding. In this paper, we propose BANG, a new large-scale pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pretraining a generative model.

Previous works have explored different pre-training methods for language models, such as masked language modeling (MLM) and permuted language modeling (PLM). However, these methods neglect the dependency between predicted tokens and suffer from pretrain-finetune discrepancy. To address these issues, we propose BANG, which is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

To achieve the best performance, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. During training, the i-th predicting stream attends to the main stream's hidden states to predict the next i-th future token, which guarantees every n continuous tokens in the target sequence are trained to predict at one time step. Since the main stream parameters are shared with every predicting stream, we can disable the n-stream self-attention during inference. Only the next first token is predicted for each time step, which is the same as the original Transformer Seq2Seq model.

We evaluate BANG on various natural language generation tasks, including question generation, summarization, and dialogue generation. Experimental results show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning. For AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. 

In summary, our proposed BANG model bridges the gap between AR and NAR via pretraining a generative model. It supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. Experimental results show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.