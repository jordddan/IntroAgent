Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) language modeling, which estimates the probability distribution of the text corpus, is widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning. However, AR-based models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction. This issue is due to the strong local correlations and the lack of explicit bias toward future token planning and modeling. Non-autoregressive (NAR) models, on the other hand, can capture long-term dependencies and bring fast decoding speed for inference. 

In this paper, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pretraining a generative model. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

Our proposed model achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning. For AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. Our approach and BART-JANUS can achieve significant improvement on multiple generation tasks, including machine translation and GLGE benchmarks. 

Related works in the field include XLNet, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. ProphetNet is another sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. 

In conclusion, our proposed BANG model bridges the gap between AR and NAR models and achieves significant performance improvements on various natural language processing tasks. The related works in the field provide valuable insights into the development of large-scale pre-trained language models and their applications in sequence modeling and sequence-to-sequence learning.