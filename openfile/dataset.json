{"title": "JANUS: Joint Autoregressive and Non-autoregressive Training with Xiaobo Liang Abstract Acknowledgement References", "abstract": "Transformer-based autoregressive and non- autoregressive models have played an essen- tial role in sequence generation tasks. The autoregressive model can obtain excellent performance, while the non-autoregressive model brings fast decoding speed for infer- ence. In this paper, we propose JANUS , a J oint A utoregressive and N on-autoregressive training method using a U xiliary los S to en- hance the model performance in both AR and NAR manner simultaneously and effectively al- leviate the problem of distribution discrepancy. Further, we pre-train BART with JANUS on a large corpus with minimal cost (16 GPU days) and make the BART-JANUS capable of non- autoregressive generation, demonstrating that our approach can transfer the AR knowledge to NAR. Empirically, we show our approach and BART-JANUS can achieve significant improve- ment on multiple generation tasks, including machine translation and GLGE benchmarks. Our code is available at Github 1 . 1", "intro_words": 1267, "introduction": "The transformer-based autoregressive ( Vaswani et al. , 2017 ; So et al. , 2019 ) (AR) generation model has achieved high-quality results in various natural language generation tasks. Meanwhile, the non- autoregressive (NAR) ( Gu et al. , 2018 ; Lee et al. , 2018 ; Libovick ` y and Helcl , 2018 ; Su et al. , 2021 ) generation methods show great potential to reduce the inference latency by introducing parallel decod- ing. Especially, iterative generative paradigms like CMLM ( Wang et al. , 2019a ; Ghazvininejad et al. , 2019 ) can dynamically adapt the trade-off between performance and latency. Recent works ( Qi et al. , 2021 ; Guo et al. , 2020 ; Tian et al. , 2020 ) have suc- cessfully combined these two mechanisms by joint training. However, these approaches only consider the relevance of model parameters, ignoring the cor- relations between the two manners, which require 1 https://github.com/dropreg/JANUS Figure 1: The example of JANUS. We exhibit some cases that show the distribution discrepancy due to the difference in context with orange background. efforts for improvement. Therefore, in this paper, we attempt to leverage the merits of both AR and NAR mechanisms while avoiding their weaknesses to improve their performance. The principal difference between AR and NAR is that they use distinct attention mechanisms. In particular, the AR uses unidirectional atten- tion ( Vaswani et al. , 2017 ) to simplify the sentence probability distributions by introducing Markov Hypothesis, which outputs the next token only depending on the previous context. This pattern makes the output distribution of each token accu- rate and unambiguous but also lacks diversity in inference. The NAR introduces bi-directional at- tention ( Devlin et al. , 2019 ) that can capture bi- directional context by considering the whole sen- tence information. This pattern needs to predict multiple tokens simultaneously, which causes the token distribution to be ambiguous, stemming from the multi-modality problem ( Zhou et al. , 2019 ). Inspired by knowledge distillation ( Hinton et al. , 2015 ) and deep mutual learning ( Zhang et al. , 2018b ), we try to regularize the model predictions by minimizing the distribution distance between the output generated by the two manners. How- ever, the conditional output probability of each target position is inconsistent between AR and NAR. We present an example in Figure 1 to il- 8050 lustrate such a discrepancy, where the AR out- put probability P AR ( y 3 | X, y 1 , y 2 ) for token y 3  de- pends on the context information { X, y 1 , y 2 } , and the NAR output P NAR ( y 3 | X, y 1 , m 2 , y 4 , m 5 , y 6 ) relies on more context { X, y 1 , m 2 , y 4 , m 5 , y 6 } . The P AR ( y 3 ) lack the token y 4 , y 6  as condition compare to P NAR ( y 3 ) , in contrast, the P NAR ( y 3 ) misses the context y 2 . Such distinct token distri- butions bring difficulty in directly regularizing the prediction distances D (AR | NAR) . To tackle this issue, we introduce an auxil- iary distribution P AUX to bridge the discrepancy between P AR and P NAR with the help of mini- mizing the distribution distance D (AR | AUX) and D (NAR | AUX) rather than the direct D (AR | NAR) , as shown in Figure 1 . This approach benefits AR and NAR to learn from each other with the following advantages: ( 1 ) The P AUX possesses rich context information compared with AR and NAR. For example, comparing to P AR and P NAR , P AUX ( y 3 | X, y 1 , y 2 , y 4 , m 5 , y 6 ) contains all to- ken y 2 , y 4 , y 6  as condition. ( 2 ) There is an autore- gressive dependency between the predicted tokens, which guarantees the accuracy of output distribu- tion and without ambiguity for each token. ( 3 ) The random mask mechanism applied to this distribu- tion, similar to NAR, enables the model to learn various token distributions. For effective implemen- tation, we draw on the experience of two-stream self-attention ( Yang et al. , 2019b ) and position com- pensation ( Song et al. , 2020 ). We build the P AUX by altering the attention matrix of NAR, which merely modifies the training procedure without af- fecting the inference. We first launched experiments on multiple NMT datasets to verify whether JANUS can help improve performance in two manners. Then, we explored our method on existing autoregressive pretraining models, in which we pre-train our BART-JANUS initialized by BART-base on a large corpus and fine-tuned to adapt various downstream tasks. Here we use the GLGE datasets as our benchmark to val- idate the model performance. Meanwhile, we also conducted comparative ablation studies to illustrate the effectiveness of our proposed method. Exper- imental results show that our method can achieve similar results to the state-of-the-art NAR model without distillation data. It simultaneously im- proves the AR model performance by more than 1 . 5 BLEU scores on average. Furthermore, our model exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks. Perhaps surpris- ingly, it can achieve comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing JANUS, a method that combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) generation models while avoiding their weaknesses to improve performance.\n2. Proposing an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other.\n3. Demonstrating the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average.\n4. Exceeding the non-autoregressive pretraining model BANG on the same GLGE tasks and achieving comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism."}
{"title": "XLNet: Generalized Autoregressive Pretraining", "abstract": "based pretraining like BERT achieves better performance than pretraining ap- proaches based on autoregressive language modeling. However, relying on corrupt- ing the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-\ufb01netune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking. 1 . 1", "intro_words": 1151, "introduction": "processing [ 7 , 22 , 27 , 28 , 10 ]. Typically, these methods \ufb01rst pretrain neural networks on large-scale unlabeled text corpora, and then \ufb01netune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives. AR language modeling seeks to estimate the probability distribution of a text corpus with an au- toregressive model [ 7 , 27 , 28 ]. Speci\ufb01cally, given a text sequence x = ( x 1 , \u00b7 \u00b7 \u00b7 , x T ) , AR language modeling factorizes the likelihood into a forward product p ( x ) =  \ufffd T t =1  p ( x t | x <t ) or a backward one p ( x ) =  \ufffd 1 t = T  p ( x t | x >t ) . A parametric model (e.g. a neural network) is trained to model each conditional distribution. Since an AR language model is only trained to encode a uni-directional con- text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information. This results in a gap between AR language modeling and effective pretraining. In comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT [ 10 ], which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK] , and the model is trained to recover the original tokens from the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize \u2217 Equal contribution. Order determined by swapping the one in [9]. 1 Pretrained models and code are available at https://github.com/zihangdai/xlnet 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1906.08237v2  [cs.CL]  2 Jan 2020 bidirectional contexts for reconstruction. As an immediate bene\ufb01t, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the arti\ufb01cial symbols like [MASK] used by BERT during pretraining are absent from real data at \ufb01netuning time, resulting in a pretrain-\ufb01netune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimpli\ufb01ed as high-order, long-range dependency is prevalent in natural language [9]. XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations. \u2022 Firstly, instead of using a \ufb01xed forward or backward factorization order as in conventional AR mod- els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order . Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context. \u2022 XLNet does not suffer from the pretrain-\ufb01netune discrepancy that BERT is subject to. Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining. \u2022 recurrence mechanism and relative encoding scheme of Transformer-XL [ 9 ] into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence. \u2022 Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity. Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [ 10 ] on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classi\ufb01cation tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Related Work The idea of permutation-based AR modeling has been explored in [ 32 , 12 ], but there are several key differences. Firstly, previous models aim to improve density estimation by baking an \u201corderless\u201d inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that \u201corderless\u201d does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution. 11 ], which only considers a \ufb01xed order though. 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing XLNet, a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context.\n\n2. XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.\n\n3. Improving architectural designs for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences.\n\n4. Proposing a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.\n\n5. Empirically demonstrating that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task."}
{"title": "BANG: Bridging Autoregressive and Non-autoregressive Generation with", "abstract": "In this paper, we propose BANG, a new pre-\nto Bridge the gap between\ntraining model\nAutoregressive (AR) and Non-autoregressive\n(NAR) Generation. AR and NAR generation can\nbe uniformly regarded as to what extent previ-\nous tokens can be attended, and BANG bridges\nAR and NAR generation by designing a novel\nmodel structure for large-scale pretraining. The\npretrained BANG model can simultaneously sup-\nport AR, NAR and semi-NAR generation to meet\ndifferent requirements. Experiments on question\ngeneration (SQuAD 1.1), summarization (XSum)\nand dialogue generation (PersonaChat) show that\nBANG improves NAR and semi-NAR perfor-\nmance signi\ufb01cantly as well as attaining compara-\nble performance with strong AR pretrained mod-\nels. Compared with the semi-NAR strong base-\nlines, BANG achieves absolute improvements of\n14.01 and 5.24 in the overall scores of SQuAD\n1.1 and XSum, respectively. In addition, BANG\nachieves absolute improvements of 10.73, 6.39\nand 5.90 in the overall scores of SQuAD, XSUM\nand PersonaChat respectively compared with the\nstrong NAR baselines.\n", "intro_words": 1275, "introduction": "\n1. Introduction\n\nVarious pretraining methods (Song et al., 2019; Lewis et al.,\n2019; Qi et al., 2020; Raffel et al., 2020; Zhang et al., 2019a)\nhave been successfully applied in natural language gener-\nation. Most of the pretraining works are based on Trans-\nformer and designed with autoregressive (AR) language\nmodel. Transformer based pretraining models show consis-\n\n*Equal contribution 1University of Science and Technology of\nChina, Hefei, China 2During Internship at MSRA 3Microsoft Re-\nsearch Asia, Beijing, China 4Microsoft, Redmond, USA 5Sichuan\nUniversity, Chengdu, China. Correspondence to: Yeyun Gong\n<yegong@microsoft.com>.\n\ntent improvements with larger model size and larger pretrain-\ning corpus. Although the autoregressive generation method\nachieves high-quality results in many tasks, its latency is a\nwell-known limitation for online real-time usage.\n\nNon-autoregressive (NAR) models (Gu et al., 2017; Lee\net al., 2018; Ghazvininejad et al., 2019; Raffel et al., 2020;\nZhang et al., 2019a) are proposed to reduce generation la-\ntency. Different from AR models which generate tokens\nsequentially, NAR models generate tokens in parallel. Com-\npared to AR models, NAR models generally come with a\nmuch lower inference latency, but a decrease in accuracy. In\norder to balance latency and accuracy, semi-NAR generation\nmodels (Stern et al., 2019; Lee et al., 2018; Gu et al., 2019;\nGhazvininejad et al., 2019) are proposed. However, most of\nthe NAR and semi-NAR models focus on translation tasks\nrather than general natural langauge generation tasks, which\nare proved to signi\ufb01cantly bene\ufb01t from pretraining (Qi et al.,\n2020; Lewis et al., 2019). Some works (Guo et al., 2020b;\nSu et al., 2021) initialize their NAR models with pretrained\nnatural language understanding model BERT (Devlin et al.,\n2018) for better performance. To the best of our knowledge,\nthis paper proposes the \ufb01rst large-scale pretraining model\ndesigned for NAR and semi-NAR generation.\n\nIn this paper, we propose a new model named BANG 1 to\nbridge the gap between AR and NAR via pretraining a gen-\nerative model. Speci\ufb01cally, we consider pretraining model\nusing AR, semi-NAR and NAR objectives with different\nattention mechanisms, which decide what extent previous\ntokens can be attended to. Precisely, BANG is pretrained to\npredict each token with arbitrary length of previous golden\ntokens replaced with special token [MASK]s. For example,\nwith complete previous golden tokens, BANG predicts the\nnext token in the AR manner. With all previous tokens re-\nplaced by [MASK], BANG predicts the next token in the\nNAR manner.\n\nFor AR models, the training strategy of teacher-forcing is\ncommonly used, which uses the golden tokens as previous\ncontext to predict the next token. For NAR models, [MASK]\ninitialization (Ghazvininejad et al., 2019) or other methods\n\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\n\n1https://github.com/microsoft/BANG\n\n \n \n \n \n \n \n\fBANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining\n\nlike encoder copy (Gu et al., 2017) and posterior distribu-\ntion approximation (Shu et al., 2020) are applied. In BANG\npretraining, we consider the previous context of golden and\n[MASK] tokens, with arbitrary golden tokens\u2019 length and\n[MASK] tokens\u2019 length. To achieve an ef\ufb01cient implemen-\ntation for multiple arbitrary alternatives in a same output\nsequence, we propose a new structure named cross-stream\nvisible n-stream self-attention, which can be used to train\nBANG with different golden and [MASK] tokens\u2019 combina-\ntions. For usage on downstream tasks, the single pretrained\nBANG model can be directly \ufb01netuned for either vanilla AR\nmodels or vanilla NAR models. Additionally, BANG can\nalso be \ufb01netuned for hybrid semi-NAR models, which sup-\nport predicting tokens with arbitrary previous golden tokens\nor [MASK]. Concretely, for semi-NAR generation, BANG\npredicts the \ufb01rst several tokens one by one as a high-quality\nsub-sequence hint, then produces all the remaining tokens\nsimultaneously.\n\nOur main contributions are: 1) BANG bridges the gap\nbetween AR and NAR by considering arbitrary previous\n[MASK] length during large-scale pretraining. 2) BANG is\npretrained using an ef\ufb01cient cross-stream visible n-stream\ndecoder to realize parallelization. Given multiple arbitrary\nnumber of previous tokens replaced with [MASK], every\ntoken is trained to predict simultaneously at each time step.\n3) BANG supports NAR, semi-NAR and AR \ufb01netuning to\nmeet different requirements with the same pretrained model\nstructure. 4) We pretrain BANG with 16GB English lan-\nguage corpora of Wikipedia and BookCorpus, and \ufb01netune\nit on 3 popular natural language generation tasks in AR,\nsemi-NAR and NAR manners, respectively. For NAR and\nsemi-NAR \ufb01netuning, BANG achieves signi\ufb01cant perfor-\nmance improvements on all the tasks. For AR \ufb01netuning\nwith the comparison to strong AR pretrained models, BANG\ncan attain comparable performance.\n", "contribution": "Main contributions of the paper are:\n\n1. BANG is proposed as the first large-scale pretraining model designed for Non-autoregressive (NAR) and semi-NAR generation, which bridges the gap between Autoregressive (AR) and NAR via pretraining a generative model.\n\n2. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK].\n\n3. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.\n\n4. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models."}
{"title": "Attention Is All You Need Abstract References", "abstract": "convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signi\ufb01cantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. 1", "intro_words": 537, "introduction": "12 ] and gated recurrent [ 7 ] neural networks in particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29 , 2 , 5 ]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13]. \u2217 Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and ef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. \u2020 Work performed while at Google Brain. \u2021 Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t \u2212 1 and the input for position t . This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signi\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [ 18 ] and conditional computation [ 26 ], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2 , 16 ]. In all but a few cases [ 22 ], however, such attention mechanisms are used in conjunction with a recurrent network. relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2", "contribution": "The main contributions of this paper are:\n\n1. The introduction of the Transformer, a new neural network architecture for sequence modeling and transduction problems that relies entirely on an attention mechanism to draw global dependencies between input and output, without the need for recurrent networks.\n\n2. The demonstration that the Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n\n3. The proposal of scaled dot-product attention, multi-head attention, and the parameter-free position representation, which are key components of the Transformer architecture.\n\n4. The development of tensor2tensor, a library for training deep learning models in a distributed and efficient manner, which was used to implement and evaluate the Transformer."}
{"title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence", "abstract": "This paper presents a new sequence-to- sequence pre-training model called Prophet- Net, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mech- anism. Instead of optimizing one-step- ahead prediction in the traditional sequence- to-sequence model, the ProphetNet is opti- mized by n -step ahead prediction that pre- dicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encour- ages the model to plan for the future tokens and prevent over\ufb01tting on strong local cor- relations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we con- duct experiments on CNN/DailyMail, Giga- word, and SQuAD 1.1 benchmarks for abstrac- tive summarization and question generation tasks. Experimental results show that Prophet- Net achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus. 1", "intro_words": 1232, "introduction": "Large-scale pre-trained language models ( Devlin et al. , 2018 ; Radford et al. , 2019 ; Yang et al. , 2019 ) and sequence-to-sequence models ( Lewis et al. , 2019 ; Song et al. , 2019 ; Raffel et al. , 2019 ) have achieved remarkable success in downstream tasks. Autoregressive (AR) language modeling, which estimates the probability distribution of the text corpus, is widely used for sequence model- ing and sequence-to-sequence (Seq2Seq) learn- ing ( Sutskever et al. , 2014 ). Recently, it also be- comes one of the successful self-supervised objec- tives for large-scale pre-training as used in GPT- Asia. \u2217 Work is done during internship at Microsoft Research \u2020 Equal contribution 2 ( Radford et al. , 2019 ). Speci\ufb01cally, given a text sequence x = ( x 1 , . . . , x T ) , AR language modeling factorizes the likelihood into a product p ( x ) =  \ufffd T t =1  p ( x t | x <t ) . In this manner, language models (LMs) and Seq2Seq models are usually trained by teacher forcing. The models are opti- mized to predict the next token given all previous context tokens at each time step. However, as discussed in previous works ( Pas- canu et al. , 2013 ; Gulcehre et al. , 2017 ; Serdyuk et al. , 2018 ), AR-based models may prefer to fo- cus on the latest tokens rather than capture long- term dependencies for the next token prediction. The reasons are as follows: (a) Local correlations such as bigram combination are usually stronger than long-term dependencies. (b) Teacher forcing, where the model focus on one-step-ahead predic- tion for each time step, has no explicit bias toward future token planning and modeling. As a result, the model may learn a bias for language modeling; that is, the local token combinations\u2019 modeling is over\ufb01tting, but the global coherence and long-term dependency are under\ufb01tting ( Krueger et al. , 2016 ; Merity et al. , 2017 ; Serdyuk et al. , 2018 ). During inference, the generations tend to maintain local coherence but lack meaningful global structure ( Li et al. , 2017 ; Serdyuk et al. , 2018 ), especially when we use greedy decoding instead of beam search. In this paper, we present a new large-scale pre- trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction . In addition to the traditional language model (LM) or Seq2Seq model that optimizes one- step-ahead prediction, the ProphetNet also learns n - step ahead predictionThis future n-gram prediction is served as extra guidance that explicitly encour- ages the model to plan for future tokens and pre- vents over\ufb01tting on strong local correlations. The hidden states of ProphetNet are forced to contain useful information for the next token and further arXiv:2001.04063v3  [cs.CL]  21 Oct 2020 help predict multiple future tokens. There are two goals when designing ProphetNet: (a) the model should be able to simultaneously predict the future n-gram at each time step in an ef\ufb01cient way during the training phase, and (b) the model can be easily converted to predict the next token only as original Seq2Seq model for inference or \ufb01ne-tuning phase. To achieve that, we extend the two-stream self-attention proposed in XLNet ( Yang et al. , 2019 ) to n-stream self-attention . Prophet- Net contains a main stream self-attention, which is the same as the self-attention in the original Trans- former. Besides, we introduce n extra self-attention predicting streams for future n-gram prediction, respectively. During training, the i -th predicting stream attends to the main stream\u2019s hidden states to predict the next i -th future token, which guarantees every n continuous tokens in the target sequence are trained to predict at one time step. Since the main stream parameters are shared with every pre- dicting stream, we can disable the n-stream self- attention during inference. Only the next \ufb01rst token is predicted for each time step, which is same as the original Transformer Seq2Seq model. For experiments, we use the proposed future n- gram prediction with the mask based auto-encoder denoising task ( Song et al. , 2019 ; Lewis et al. , 2019 ) which has been proved to be effective for Seq2Seq pre-training as compared in Raffel et al. ( 2019 ) for ProphetNet pre-training. We use two scale pre-trained datasets to pre-train ProphetNet, respectively: the base scale (16GB) dataset as used in BERT ( Devlin et al. , 2018 ), and the large scale (160GB) similar to BART ( Lewis et al. , 2019 ). The pre-trained ProphetNet is further \ufb01ne-tuned on several NLG tasks. Experimental results show that ProphetNet has achieved the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the mod- els using the same base scale pre-training dataset. For the large scale dataset pre-training experiment, ProphetNet achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 ( Raffel et al. , 2019 ) and PEGASUS ( Zhang et al. , 2019 ). 2", "contribution": "The main contributions of this paper are:\n\n1. Introducing a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction.\n2. Developing a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations.\n3. Extending the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.\n4. Pre-training ProphetNet on two scale pre-trained datasets and achieving new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS.\n5. Fine-tuning ProphetNet on several NLG tasks and achieving the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset."}
{"title": "MPNet: Masked and Permuted Pre-training for Abstract Acknowledgements and Disclosure of Funding Broader Impact References", "abstract": "BERT adopts masked language modeling (MLM) for pre-training and is one of\nthe most successful pre-training models. Since BERT neglects dependency among\npredicted tokens, XLNet introduces permuted language modeling (PLM) for pre-\ntraining to address this problem. However, XLNet does not leverage the full\nposition information of a sentence and thus suffers from position discrepancy\nbetween pre-training and \ufb01ne-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids\ntheir limitations. MPNet leverages the dependency among predicted tokens through\npermuted language modeling (vs. MLM in BERT), and takes auxiliary position\ninformation as input to make the model see a full sentence and thus reducing the\nposition discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale\ndataset (over 160GB text corpora) and \ufb01ne-tune on a variety of down-streaming\ntasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms\nMLM and PLM by a large margin, and achieves better results on these tasks\ncompared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet,\nRoBERTa) under the same model setting. The code and the pre-trained models are\navailable at: https://github.com/microsoft/MPNet.\n", "intro_words": 622, "introduction": "\nPre-training language models [1, 2, 3, 4, 5, 6, 7, 8] have greatly boosted the accuracy of NLP tasks\nin the past years. One of the most successful models is BERT [2], which mainly adopts masked\nlanguage modeling (MLM) for pre-training1. MLM leverages bidirectional context of masked tokens\nef\ufb01ciently, but ignores the dependency among the masked (and to be predicted) tokens [5].\n\nTo improve BERT, XLNet [5] introduces permuted language modeling (PLM) for pre-training to\ncapture the dependency among the predicted tokens. However, PLM has its own limitation: Each\ntoken can only see its preceding tokens in a permuted sequence but does not know the position\ninformation of the full sentence (e.g., the position information of future tokens in the permuted\nsentence) during the autoregressive pre-training, which brings discrepancy between pre-training and\n\ufb01ne-tuning. Note that the position information of all the tokens in a sentence is available to BERT\nwhile predicting a masked token.\n\nIn this paper, we \ufb01nd that MLM and PLM can be uni\ufb01ed in one view, which splits the tokens\nin a sequence into non-predicted and predicted parts. Under this uni\ufb01ed view, we propose a new\npre-training method, masked and permuted language modeling (MPNet for short), which addresses\nthe issues in both MLM and PLM while inherits their advantages: 1) It takes the dependency among\nthe predicted tokens into consideration through permuted language modeling and thus avoids the\n\n1We do not consider next sentence prediction here since previous works [5, 7, 9] have achieved good results\n\nwithout next sentence prediction.\n\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\n\fissue of BERT; 2) It takes position information of all tokens as input to make the model see the\nposition information of all the tokens and thus alleviates the position discrepancy of XLNet.\n\nWe pre-train MPNet on a large-scale text corpora (over 160GB data) following the practice in [5, 7],\nand \ufb01ne-tune on a variety of down-streaming benchmark tasks, including GLUE, SQuAD, RACE\nand IMDB. Experimental results show that MPNet outperforms MLM and PLM by a large margin,\nwhich demonstrates that 1) the effectiveness of modeling the dependency among the predicted tokens\n(MPNet vs. MLM), and 2) the importance of the position information of the full sentence (MPNet vs.\nPLM). Moreover, MPNet outperforms previous well-known models BERT, XLNet and RoBERTa by\n4.8, 3.4 and 1.5 points respectively on GLUE dev sets under the same model setting, indicating the\ngreat potential of MPNet for language understanding.", "contribution": "The main contributions of this paper are:\n\n1. Proposing a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations.\n\n2. Introducing a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet.\n\n3. Pre-training MPNet on a large-scale text corpus and fine-tuning it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB, which outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting."}
{"title": "IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION", "abstract": "Transformer-based autoregressive (AR) machine translation models have achieved\nsigni\ufb01cant performance improvements, nearing human-level accuracy on some\nlanguages. The AR framework translates one token at a time which can be time\nconsuming, especially for long sequences. To accelerate inference, recent work has\nbeen exploring non-autoregressive (NAR) approaches that translate blocks of tokens\nin parallel. Despite signi\ufb01cant progress, leading NAR models still lag behind their\nAR counterparts, and only become competitive when trained with distillation. In\nthis paper we investigate possible reasons behind this performance gap, namely, the\nindistinguishability of tokens, and mismatch between training and inference. We\nthen propose the Conditional Masked Language Model with Correction (CMLMC)\nthat addresses these problems. Empirically, we show that CMLMC achieves state-\nof-the-art NAR performance when trained on raw data without distillation, and\napproaches AR performance on multiple datasets. Code for this work is available\nhere: https://github.com/layer6ai-labs/CMLMC.\n", "intro_words": 992, "introduction": "\nINTRODUCTION\n\nNeural machine translation (NMT) models based on the Transformer architecture have achieved\nleading performance (Vaswani et al., 2017; Barrault et al., 2019; Huang et al., 2020). Majority\nof the proposed approaches are based on the autoregressive (AR) principle, where translation is\ndone one token at a time conditioning on already translated tokens. AR inference scales linearly\nwith the number of tokens and full forward pass through the decoder is required for each translated\ntoken. This can be prohibitively expensive for long sequences, particularly as leading models are\nbecoming increasingly larger in size. To mitigate this problem, recent works have explored the\nnon-autoregressive (NAR) approach where subsets of tokens are translated in parallel (Gu et al., 2018;\nGhazvininejad et al., 2019; Kasai et al., 2020). NAR models achieve signi\ufb01cantly faster inference\nspeed that no longer depends on sequence length. However, despite considerable progress, leading\nNAR models still require sequence-level knowledge distillation (Kim & Rush, 2016) to achieve\ncompetitive accuracy. In practice, a large AR Transformer model trained on the raw data is used\nas the teacher for distillation (Ghazvininejad et al., 2019). This process is expensive, as every new\nlanguage pair requires training a new teacher. It is also non-standard, and raises questions to the\nnecessity and the underlying problems solved by distillation (Zhou et al., 2020; Ding et al., 2021).\n\nIn this work we focus on one of the leading NAR approaches, the Conditional Masked Language\nModel (CMLM) (Ghazvininejad et al., 2019). CMLM achieved leading NAR performance on\nmultiple NMT datasets - especially when combined with semi-autoregressive training (Ghazvininejad\net al., 2020b) - but only when the model is trained on distilled data. Without distillation, CMLM\nperformance drops signi\ufb01cantly below AR benchmarks. The need for distillation indicates that\nCMLM alone is unable to fully leverage the information available in the raw training data (Ding et al.,\n2021). Here, we identify two shortcomings of CMLM that, when addressed, signi\ufb01cantly improve\nNAR translation quality and narrow the gap between raw and distilled performance.\n\nFirst, input token representations in CMLM can become nearly indistinguishable, especially for\nadjacent positions. In AR models this problem is avoided by a combination of causal masked attention,\nsequential inference, and learned positional encodings (PEs). However, unmasked attention and\nsimultaneous translation of token blocks in CMLM loses most of the information that distinguishes\ntokens. This problem is particularly severe during the \ufb01rst inference step, where the input is fully\nmasked. The model thus only relies on learned PEs to distinguish tokens, which is not suf\ufb01cient.\nPoor token separation can cause signi\ufb01cant translation errors, including the identi\ufb01ed phenomenon of\ntoken repetition stemming from the related multi-modality problem (Zhou et al., 2020).\n\n1\n\n\fPublished as a conference paper at ICLR 2022\n\nSecond, there is a misalignment between CMLM\u2019s training and inference procedures. During training\nCMLM is optimized with a masked loss analogous to language model training in popular models\nsuch as BERT (Devlin et al., 2019). However, CMLM inference always starts with a fully masked\nsentence and translates all tokens simultaneously. Iterative re\ufb01nement is then applied where subsets\nof low con\ufb01dence tokens are masked and re-translated at each iteration. During training the model\nrarely sees a fully masked sentence, and is not trained to self-correct from the initial fully masked\ntranslation that can contain signi\ufb01cant errors. The misalignment between the two procedures can\ncause a disconnect, where optimization of the training loss does not transfer to improvements in\ntranslation quality.\n\nIn this work we propose the Conditional Masked Language Model with Correction (CMLMC). Our\nmodel builds on the CMLM architecture and addresses the aforementioned problems. We modify the\ndecoder structure by exposing the positional encodings and incorporating causal attention layers to\ndifferentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to\ncorrect translation mistakes made in early decoding iterations from the fully masked sentence. With\nthese improvements, CMLMC achieves new state-of-the-art undistilled NAR results and approaches\nAR performance on multiple NMT benchmarks.\n", "contribution": "The main contributions of this paper are:\n\n1. Proposing the Conditional Masked Language Model with Correction (CMLMC) which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in non-autoregressive (NAR) machine translation.\n2. Modifying the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens.\n3. Proposing a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence.\n4. Achieving new state-of-the-art undistilled NAR results and approaching autoregressive (AR) performance on multiple NMT benchmarks."}
{"title": "Universal Conditional Masked Language Pre-training", "abstract": "Pre-trained sequence-to-sequence models have signi\ufb01cantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence- to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non- autoregressive NMT. Speci\ufb01cally, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. 1 We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking . We conduct extensive experi- ments and show that our CeMAT can achieve signi\ufb01cant performance improvement for all scenarios from low- to extremely high- resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the best of our knowledge, this is the \ufb01rst work to pre-train a uni\ufb01ed model for \ufb01ne-tuning on both NMT tasks. 1", "intro_words": 1762, "introduction": "adopted in NLP tasks ( Devlin et al. , 2019 ; Radford and Narasimhan , 2018 ). For example, XLM ( Con- neau and Lample , 2019 ) demonstrated that cross- lingual pre-training is effective in improving neu- ral machine translation (NMT), especially on low- resource languages. These methods all directly pre- train a bidirectional encoder or an unidirectional decoder. The encoder and decoder in NMT models 1 Code, data, and pre-trained models are avail- able at https://github.com/huawei-noah/ Pretrained-Language-Model/tree/master/ CeMAT Approach Enc. Dec. Mono. Para. mBERT ( Devlin et al. , 2019 ) \u2022 \u2022 XLM ( Conneau and Lample , 2019 ) \u2022 \u2022 \u2022 MASS ( Song et al. , 2019 ) \u2022 \u2192 \u2022 mBART ( Liu et al. , 2020 ) \u2022 \u2192 \u2022 mRASP ( Lin et al. , 2020 ) \u2022 \u2192 \u2022 CeMAT (Ours) \u2022 \u21d0\u21d2 \u2022 \u2022 Table 1: Comparison and summary of existing pre- trained models for machine translation. Enc: encoder; Dec: decoder; Mono: monolingual; Para: bilingual. \u201c \u2022 \u201d denotes the corresponding model is pre-trained or the corresponding data is used. \u201c \u2192 \u201d denotes the de- coder of model is unidirectional, \u201c \u21d0\u21d2 \u201d denotes the de- coder is bidirectional. are then independently initialized with them and \ufb01ne-tuned ( Guo et al. , 2020 ; Zhu et al. , 2020 ). Re- cently, pre-training standard sequence-to-sequence (Seq2Seq) models has shown signi\ufb01cant improve- ments and become a popular paradigm for NMT tasks ( Song et al. , 2019 ; Liu et al. , 2020 ; Lin et al. , 2020 ). However, some experimental results from XLM ( , 2019 ) have shown that the decoder module initialized by the pre-trained bidi- rectional masked language model (MLM) ( et al. , 2019 ), rather than the unidirectional causal language model (CLM, , 2018 ), would achieve better results on Autore- gressive NMT (AT). Especially, compared to ran- dom initialization, initialized by GPT ( Radford and , 2018 ) might result in performance degradation sometimes. We conjecture that when \ufb01ne-tuning on generation tasks (e.g., NMT), the representation capability of the pre-trained models may be more needed than the generation capability. Therefore, during pre-training, we should focus on training the representation capability not only for the encoder, but also for the decoder more explic- itly. mul- tilingual Conditional masked language prE- arXiv:2203.09210v3  [cs.CL]  2 Jun 2022 [en] Cat sat on the mat [en] We dance on the grass [de] Wir tanzen auf dem gras Mono. Para. Original [en] Cat sat on the mat [en] [mask] sat on the [mask] [en] We danse [mask] the grass [de] Wir [mask] auf dem  [mask] Mono. Para. Masked [en] Kedi sat on the [mask] Encoder Self-Attention Feed Forward Bidirectional Decoder Cross-Attention Self-Attention Feed Forward Encoder Decoder [en] who are you [de] Wer bist du Wer bist du </s> Autoregressive NMT Encoder Bidirectional Decoder [en] who are you [de] [mask] bist [mask] Wer du Non-autoregressive NMT Cat mat on tanzen gras mat Pre-training Fine-tuning Aligned code-switching & masking dynamic dual-masking Mono. Para. Figure 1: The framework for CeMAT, which consists of an encoder and a bidirectional decoder . \u201c Mono \u201d denotes monolingual, \u201c Para \u201d denotes bilingual. During the pre-training (left), the original monolingual and bilingual inputs in many languages are augmented (the words are replaced with new words with same semantics or \u201c[mask]\u201d, please see Figure 2 for more details) and fed into the model. Finally, we predict all the \u201c[mask]\u201d words on the source side and target side respectively. For \ufb01ne-tuning (right), CeMAT provides uni\ufb01ed initial parameter sets for AT and NAT. training model for MAchine Translation , which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridg- ing them. Speci\ufb01cally, the model is jointly trained by MLM on the encoder and Conditional MLM (CMLM) on the decoder with large-scale monolin- gual and bilingual texts in many languages. Table 1 compares our model with prior works. Bene\ufb01ting from the structure, CeMAT can provide uni\ufb01ed initialization parameters not only for AT task, but also for Non-autoregressive NMT (NAT) directly. NAT has been attracting more and more attention because of its feature of parallel decoding, which helps to greatly reduce the translation latency. the model, the masking operations are applied in two steps. First, some source words that have been aligned with target words are randomly selected and then substituted by new words of similar mean- ings in other languages, and their corresponding tar- get words are masked. We call this method aligned code-switching & masking . Then, the remaining words in both source and target languages will be masked by dynamic dual-masking . NAT tasks show signi\ufb01cant gains over prior works. Speci\ufb01cally, under low-resource conditions ( < 1M bitext pairs), our system gains up to +14.4 BLEU points over baselines. Even for extremely high- resource settings ( > 25M), CeMAT still achieves signi\ufb01cant improvements. In addition, experiments on the WMT16 Romanian \u2192 strate that our system can be further improved (+2.1 BLEU) by the Back-Translation (BT; nrich et al. , 2016a ). The main contributions of our work can be sum- marized as follows: \u2022 coder, a bidirectional decoder. The model is pre-trained on both monolingual and bilingual corpora and then used for initializing down- stream AT and NAT tasks. To the best of our knowledge, this is the \ufb01rst work to pre-train a uni\ufb01ed model suitable for both AT and NAT. \u2022 enhance the model training under the setting of bidirectional decoders. Based on a multi- lingual translation dictionary and word align- ment between source and target sentences, aligned code-switching & masking is \ufb01rstly applied. Then, dynamic dual-masking is used. \u2022 and NAT tasks with data of varied sizes. Con- sistent improvements over strong competitors demonstrate the effectiveness of CeMAT. [en] We dance on the grass [de] Wir tanzen auf dem gras Spanish : danza German : tanzen French  : danse \u2026 dance 2. \ud835\udc39\ud835\udc39 \ud835\udc5a\ud835\udc5a (\ud835\udc65\ud835\udc65 \ud835\udc5a\ud835\udc5a \ud835\udc56\ud835\udc56 ) [en] We danse on the grass [de] Wir [mask] auf dem gras CSR CSM [en] We danse [mask] the grass [de] Wir [mask] auf dem  [mask] DM DM 1.Aligned Figure 2: The details of our two-step masking. We \ufb01rst obtain the aligned pair set \u039b = {(\u201cdance\u201d,\u201ctanzen\u201d),...} (marked with \ufffd\ufffd\ufffd ) from the original inputs by looking up the cross-lingual dictionary (denote as 1.Aligned ), and then randomly select a subset (marked as \u201cdance\u201d \ufffd\ufffd\ufffd \u201ctanzen\u201d with red color) from it, in the lower left of the \ufb01gure. For each element in the subset, we select a new word by F m ( x i m ) , and perform CSR to replace the source fragment (\u201cdanse\u201d marked as red color) and CSM for target (\u201c[mask]\u201d marked as red color) respectively. Finally, we do the DM process to mask the contents of the source and target respectively (\u201c[mask]\u201d marked as light-blue color). 2", "contribution": "The main contributions of this paper are:\n\n1. The development of CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.\n\n2. The introduction of aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages.\n\n3. The demonstration of the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes."}
