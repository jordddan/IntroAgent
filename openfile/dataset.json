{"introduction": "\nIntroduction\n\nPre-training language models [1, 2, 3, 4, 5, 6, 7, 8] have greatly boosted the accuracy of NLP tasks\nin the past years. One of the most successful models is BERT [2], which mainly adopts masked\nlanguage modeling (MLM) for pre-training1. MLM leverages bidirectional context of masked tokens\nef\ufb01ciently, but ignores the dependency among the masked (and to be predicted) tokens [5].\n\nTo improve BERT, XLNet [5] introduces permuted language modeling (PLM) for pre-training to\ncapture the dependency among the predicted tokens. However, PLM has its own limitation: Each\ntoken can only see its preceding tokens in a permuted sequence but does not know the position\ninformation of the full sentence (e.g., the position information of future tokens in the permuted\nsentence) during the autoregressive pre-training, which brings discrepancy between pre-training and\n\ufb01ne-tuning. Note that the position information of all the tokens in a sentence is available to BERT\nwhile predicting a masked token.\n\nIn this paper, we \ufb01nd that MLM and PLM can be uni\ufb01ed in one view, which splits the tokens\nin a sequence into non-predicted and predicted parts. Under this uni\ufb01ed view, we propose a new\npre-training method, masked and permuted language modeling (MPNet for short), which addresses\nthe issues in both MLM and PLM while inherits their advantages: 1) It takes the dependency among\nthe predicted tokens into consideration through permuted language modeling and thus avoids the\n\n1We do not consider next sentence prediction here since previous works [5, 7, 9] have achieved good results\n\nwithout next sentence prediction.\n\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\n\fissue of BERT; 2) It takes position information of all tokens as input to make the model see the\nposition information of all the tokens and thus alleviates the position discrepancy of XLNet.\n\nWe pre-train MPNet on a large-scale text corpora (over 160GB data) following the practice in [5, 7],\nand \ufb01ne-tune on a variety of down-streaming benchmark tasks, including GLUE, SQuAD, RACE\nand IMDB. Experimental results show that MPNet outperforms MLM and PLM by a large margin,\nwhich demonstrates that 1) the effectiveness of modeling the dependency among the predicted tokens\n(MPNet vs. MLM), and 2) the importance of the position information of the full sentence (MPNet vs.\nPLM). Moreover, MPNet outperforms previous well-known models BERT, XLNet and RoBERTa by\n4.8, 3.4 and 1.5 points respectively on GLUE dev sets under the same model setting, indicating the\ngreat potential of MPNet for language understanding.\n", "contribution": "The main contribution of this paper is the proposal of a new pre-training method called Masked and Permuted Language Modeling (MPNet), which unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations. MPNet takes into consideration the dependency among predicted tokens through permuted language modeling and also takes the position information of all tokens as input to alleviate the position discrepancy of XLNet. The authors pre-trained MPNet on a large-scale text corpus and fine-tuned it on various benchmark tasks, showing that it outperforms MLM and PLM by a large margin and also outperforms previous well-known models such as BERT, XLNet, and RoBERTa on the GLUE dev sets."}
{"introduction": "\nIntroduction\n\nThe Earth is a complex system. Variabilities of the Earth system, ranging from regular events like\ntemperature \ufb02uctuation to extreme events like drought, hail storm, and El Ni\u00f1o/Southern Oscillation\n(ENSO), impact our daily life. Among all the consequences, Earth system variabilities can in\ufb02uence\ncrop yields, delay airlines, cause \ufb02oods and forest \ufb01res. Precise and timely forecasting of these vari-\nabilities can help people take necessary precautions to avoid crisis, or better utilize natural resources\nsuch as wind and solar energy. Thus, improving forecasting models for Earth variabilities (e.g.,\nweather and climate) has a huge socioeconomic impact. Despite its importance, the operational\nweather and climate forecasting systems have not fundamentally changed for almost 50 years [34].\nThese operational models, including the state-of-the-art High Resolution Ensemble Forecast (HREF)\n\n\u21e4Work done while being an intern at Amazon Web Services. \u2020Contact person.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fFigure 1: Example Vertically Integrated Liquid (VIL) observation sequence from the Storm EVent\nImageRy (SEVIR) dataset. The observation intensity is mapped to pixel value of the range 0-255.\nThe larger value indicates the higher precipitation intensity.\n\nrainfall nowcasting model used in National Oceanic and Atmospheric Administration (NOAA) [32],\nrely on meticulous numerical simulation of physical models. Such simulation-based systems in-\nevitably fall short in the ability to incorporate signals from newly emerging geophysical observation\nsystems [12], or take advantage of the Petabytes-scale Earth observation data [43].\n\nAs an appealing alternative, deep learning (DL) is offering a new approach for Earth system forecast-\ning [34]. Instead of explicitly incorporating physical rules, DL-based forecasting models are trained\non the Earth observation data [36]. By learning from a large amount of observations, DL models\nare able to \ufb01gure out the system\u2019s intrinsic physical rules and generate predictions that outperform\nsimulation-based models [9]. Such technique has demonstrated success in several applications,\nincluding precipitation nowcasting [32, 6] and ENSO forecasting [15]. Because the Earth system is\nchaotic [21], high-dimensional, and spatiotemporal, designing appropriate DL architecture for model-\ning the system is particularly challenging. Previous works relied on the combination of Recurrent\nNeural Networks (RNN) and Convolutional Neural Networks (CNN) [36, 37, 43, 13, 45]. These two\narchitectures impose temporal and spatial inductive biases that help capturing spatiotemporal patterns.\nHowever, as a chaotic system, variabilities of the Earth system, such as rainfall and ENSO, are highly\nsensitive to the system\u2019s initial conditions and can respond abruptly to internal changes. It is unclear\nwhether the inductive biases in RNN and CNN models still hold for such complex systems.\n\nOn the other hand, recent years have witnessed major breakthroughs in DL brought by the wide\nadoption of Transformer. The model was originally proposed for natural language processing [42, 7],\nand later has been extended to computer vision [8, 22], multimodal text-image generation [31],\ngraph learning [52], etc. Transformer relies on the attention mechanism to capture data correlations\nand is powerful at modeling complex and long-range dependencies, both of which appear in Earth\nsystems (See Fig. 1 for an example of Earth observation data). Despite being suitable for the problem,\nTransformer sees limited adoption for Earth system forecasting. Naively applying the Transformer\narchitecture is infeasible because the O(N 2) attention mechanism is too computationally expensive\nfor the high-dimensional Earth observation data. How to design a space-time Transformer that is\ngood at predicting the future of the Earth systems is largely an open problem to the community.\n\nIn this paper, we propose Earthformer, a space-time Transformer for Earth system forecasting. To\nbetter explore the design of space-time attention, we propose Cuboid Attention, which is a generic\nbuilding block for ef\ufb01cient space-time attention. The idea is to decompose the input tensor to\nnon-overlapping cuboids and apply cuboid-level self-attention in parallel. Since we limit the O(N 2)\nself-attention inside the local cuboids, the overall complexity is greatly reduced. Different types\nof correlations can be captured via different cuboid decompositions. By stacking multiple cuboid\nattention layers with different hyperparameters, we are able to subsume several previously proposed\nvideo Transformers [19, 23, 4] as special cases, and also come up with new attention patterns that\nwere not studied before. A limitation of this design is the lack of a mechanism for the local cuboids to\ncommunicate with each other. Thus, we introduce a collection of global vectors that attend to all the\nlocal cuboids, thereby gathering the overall status of the system. By attending to the global vectors,\nthe local cuboids can grasp the general dynamics of the system and share information with each other.\n\nTo verify the effectiveness of cuboid attention and \ufb01gure out the best design under the Earth system\nforecasting scenario, we conducted extensive experiments on two synthetic datasets: the MovingM-\nNIST [36] dataset and a newly proposed N -body MNIST dataset. Digits in the N -body MNIST\nfollow the chaotic 3-body motion pattern [25], which makes the dataset not only more challenging\nthan MovingMNIST but also more relevant to Earth system forecasting. The synthetic experiments\nreveal the following \ufb01ndings: 1) stacking cuboid attention layers with the Axial attention pattern is\nboth ef\ufb01cient and effective, achieving the best overall performance, 2) adding global vectors provides\nconsistent performance gain without increasing the computational cost, 3) adding hierarchy in the\nencoder-decoder architecture can improve performance. Based on these \ufb01ndings, we \ufb01gured out\nthe optimal design for Earthformer and made comparisons with other baselines on the SEVIR [43]", "contribution": "The main contribution of this paper is the proposal of Earthformer, a space-time Transformer for Earth system forecasting. The authors introduce Cuboid Attention, a building block for efficient space-time attention, which decomposes the input tensor to non-overlapping cuboids and applies cuboid-level self-attention in parallel. They also introduce a collection of global vectors that attend to all the local cuboids, thereby gathering the overall status of the system. The authors conducted extensive experiments on two synthetic datasets and found that stacking cuboid attention layers with the Axial attention pattern is both efficient and effective, adding global vectors provides consistent performance gain without increasing the computational cost, and adding hierarchy in the encoder-decoder architecture can improve performance. They also made comparisons with other baselines on the SEVIR dataset."}
{"introduction": "\nIntroduction\n\nUnsupervised representation learning has been highly successful in the domain of natural language\nprocessing [7, 22, 27, 28, 10]. Typically, these methods \ufb01rst pretrain neural networks on large-scale\nunlabeled text corpora, and then \ufb01netune the models or representations on downstream tasks. Under\nthis shared high-level idea, different unsupervised pretraining objectives have been explored in\nliterature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been\nthe two most successful pretraining objectives.\n\nAR language modeling seeks to estimate the probability distribution of a text corpus with an au-\ntoregressive model [7, 27, 28]. Speci\ufb01cally, given a text sequence x = (x1, \u00b7 \u00b7 \u00b7 , xT ), AR language\nmodeling factorizes the likelihood into a forward product p(x) = (cid:81)T\nt=1 p(xt | x<t) or a backward\none p(x) = (cid:81)1\nt=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each\nconditional distribution. Since an AR language model is only trained to encode a uni-directional con-\ntext (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the\ncontrary, downstream language understanding tasks often require bidirectional context information.\nThis results in a gap between AR language modeling and effective pretraining.\n\nIn comparison, AE based pretraining does not perform explicit density estimation but instead aims to\nreconstruct the original data from corrupted input. A notable example is BERT [10], which has been\nthe state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens\nare replaced by a special symbol [MASK], and the model is trained to recover the original tokens from\nthe corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize\n\n\u2217Equal contribution. Order determined by swapping the one in [9].\n1Pretrained models and code are available at https://github.com/zihangdai/xlnet\n\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\n\n \n \n \n \n \n \n\fbidirectional contexts for reconstruction. As an immediate bene\ufb01t, this closes the aforementioned\nbidirectional information gap in AR language modeling, leading to improved performance. However,\nthe arti\ufb01cial symbols like [MASK] used by BERT during pretraining are absent from real data at\n\ufb01netuning time, resulting in a pretrain-\ufb01netune discrepancy. Moreover, since the predicted tokens are\nmasked in the input, BERT is not able to model the joint probability using the product rule as in AR\nlanguage modeling. In other words, BERT assumes the predicted tokens are independent of each\nother given the unmasked tokens, which is oversimpli\ufb01ed as high-order, long-range dependency is\nprevalent in natural language [9].\n\nFaced with the pros and cons of existing language pretraining objectives, in this work, we propose\nXLNet, a generalized autoregressive method that leverages the best of both AR language modeling\nand AE while avoiding their limitations.\n\n\u2022 Firstly, instead of using a \ufb01xed forward or backward factorization order as in conventional AR mod-\nels, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations\nof the factorization order. Thanks to the permutation operation, the context for each position can\nconsist of tokens from both left and right. In expectation, each position learns to utilize contextual\ninformation from all positions, i.e., capturing bidirectional context.\n\n\u2022 Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,\nXLNet does not suffer from the pretrain-\ufb01netune discrepancy that BERT is subject to. Meanwhile,\nthe autoregressive objective also provides a natural way to use the product rule for factorizing the\njoint probability of the predicted tokens, eliminating the independence assumption made in BERT.\n\nIn addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.\n\n\u2022 Inspired by the latest advancements in AR language modeling, XLNet integrates the segment\nrecurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which\nempirically improves the performance especially for tasks involving a longer text sequence.\n\n\u2022 Naively applying a Transformer(-XL) architecture to permutation-based language modeling does\nnot work because the factorization order is arbitrary and the target is ambiguous. As a solution, we\npropose to reparameterize the Transformer(-XL) network to remove the ambiguity.\n\nEmpirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a\nwide spectrum of problems including GLUE language understanding tasks, reading comprehension\ntasks like SQuAD and RACE, text classi\ufb01cation tasks such as Yelp and IMDB, and the ClueWeb09-B\ndocument ranking task.\n\nRelated Work The idea of permutation-based AR modeling has been explored in [32, 12], but there\nare several key differences. Firstly, previous models aim to improve density estimation by baking\nan \u201corderless\u201d inductive bias into the model while XLNet is motivated by enabling AR language\nmodels to learn bidirectional contexts. Technically, to construct a valid target-aware prediction\ndistribution, XLNet incorporates the target position into the hidden state via two-stream attention\nwhile previous permutation-based AR models relied on implicit position awareness inherent to their\nMLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that\n\u201corderless\u201d does not mean that the input sequence can be randomly permuted but that the model\nallows for different factorization orders of the distribution.\n\nAnother related idea is to perform autoregressive denoising in the context of text generation [11],\nwhich only considers a \ufb01xed order though.\n", "contribution": "The main contribution of this paper is the proposal of XLNet, a generalized autoregressive method for unsupervised representation learning in natural language processing. XLNet leverages the best of both autoregressive (AR) language modeling and autoencoding (AE) while avoiding their limitations. Specifically, XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing for bidirectional context. Additionally, XLNet does not rely on data corruption and provides a natural way to use the product rule for factorizing the joint probability of predicted tokens. The paper also proposes improvements to architectural designs for pretraining and shows that XLNet outperforms BERT on a wide range of language understanding and text classification tasks."}
{"introduction": "Introduction\n\nMany problems in computer science amount to finding the best sequence of objects consistent with\nsome precedence constraints. An intuitive example comes from routing problems, where we would\nlike to find the shortest route between cities but we have requirements (i.e. for example to pick up\nand subsequently deliver a package) on the order in which the cities should be visited [1]. Another\ncase is found in compiler pipelines, wherein the \"cities\" become operations to be executed and the\nconstraints come from the data dependencies between these operations, such as when the result of\nan operation is an operand in a subsequent one. In this case, the metric to be optimized can be the\nrun time of the compiled program, or the memory required to execute the program [2]. Common\nacross this class of problems is their formulation in term of finding the optimal topological order\n\n\u2217Equal contribution\n\u2020Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\n\u2021Work completed during employment at Qualcomm Technologies, Inc.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fof the Directed Acyclic Graph (DAG) that encodes the precedence constraints, which induces a\nCombinatorial Optimization [3] (CO) problem which is in general computationally hard [4].\n\nAlready from the two examples above, one can immediately grasp the relevance of such problems\nfor industrial Operations Research, which has prompted various actors to invest in the development\nof efficient CO solvers; these solvers usually encapsulate heuristic methods whose design typically\nrequires extensive use of domain-specific and problem-specific knowledge, across decades of de-\nvelopment. In recent years, considerable interest has emerged in the possibility of replacing such\nhandcrafted heuristics with ones learned by deep neural nets [5] (machine learning for combinatorial\noptimization, MLCO). As a matter of fact, both of our two examples of DAG-based CO problems\nhave indirectly been object of study in the Machine Learning literature. References [6, 7, 8, 9] take\ninto consideration Routing Problems, especially the Traveling Salesperson Problem (TSP) which, on\naccount of its richness, complexity and long history of mathematical study [10], has attained the status\nof a standard benchmark for MLCO [8]. Conversely, less attention has been devoted to operations\nsequencing likely due to the proprietary and sensitive nature of compiler workflows, which hampers\nthe definition of public benchmarks. References [11, 12] both consider the task of optimizing the\nrun time of a neural network\u2019s forward pass by optimizing the ordering and device assignment of\nits required operations. However, in this last case the sequencing stage is only one part of a larger\ncompiler pipeline, and as a result of this both the performance metrics and the datasets employed\ncannot be made available for reproduction by third parties. This makes it both hard to assess the\nresults therein, and to draw general conclusions and guidelines for the advancement of MLCO, which\nstill suffers from a lack of commonly accepted and standard datasets and benchmarks.\n\nIn this work, we address the problem of finding optimal topological orders in a DAG using deep\nlearning, focusing on the compiler task of optimizing the peak local memory usage during execution.\nWe make the following contributions:\n\n\u2022 We present a neural framework to optimize sequences on directed acyclic graphs. Mindful of\nthe need for scalability, we consider a non-auto-regressive (NAR) scheme for parametrizing the\nprobability distribution of topological orders. This allows our method to attain an extremely\nfavorable performance vs. run time tradeoff: it always outperforms fast baselines, and is only\nmatched or outperformed by those requiring a much longer (in one case 4000x more) run time.\n\u2022 We address the problem of how to perform meaningful message-passing on DAGs, a graph type\nwhich has received comparatively less attention in the literature on Graph Neural Networks. We\nintroduce Topoformer, a flexible, attention-based architecture wherein messages can be passed\nbetween each and every pair of nodes, with a different set of learnable parameters depending on\nthe topological relation between the nodes.\n\n\u2022 To test our method, we introduce an algorithm for the generation of synthetic, layered, Neural\nNet-like computation graphs, allowing any researcher to generate a dataset of as many as desired\ngraphs of any desired size. These graphs are a more faithful model of real NN workflows, and\nallow us to prove our method on a much larger and varied dataset, than previous efforts [11]. To\nour knowledge, this is the first public algorithm of this kind. Nevertheless, we also test our method\non proprietary graphs to illustrate its relevance to realistic compiler workflows.\n", "contribution": "The main contribution of this paper is the development of a neural framework called Topoformer that optimizes sequences on directed acyclic graphs (DAGs) using deep learning. The framework addresses the problem of finding optimal topological orders in a DAG, focusing on the compiler task of optimizing the peak local memory usage during execution. The authors introduce a non-auto-regressive (NAR) scheme for parametrizing the probability distribution of topological orders, which allows the method to attain an extremely favorable performance vs. run time tradeoff. They also address the problem of how to perform meaningful message-passing on DAGs and introduce Topoformer, a flexible, attention-based architecture for passing messages between nodes. To test their method, the authors introduce an algorithm for the generation of synthetic, layered, Neural Net-like computation graphs, allowing any researcher to generate a dataset of as many as desired graphs of any desired size. The authors also test their method on proprietary graphs to illustrate its relevance to realistic compiler workflows."}
