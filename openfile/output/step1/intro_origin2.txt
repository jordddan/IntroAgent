Recent advances in natural language processing have led to the development of large-scale pretraining models that have achieved state-of-the-art performance on various language tasks. However, most of these models are autoregressive, which means they generate tokens sequentially, leading to slow inference times. Non-autoregressive models, on the other hand, generate tokens in parallel, resulting in faster inference times but lower quality outputs. 

To bridge the gap between autoregressive and non-autoregressive models, we propose BANG, the first large-scale pretraining model designed for non-autoregressive and semi-non-autoregressive generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. It also supports non-autoregressive, semi-non-autoregressive, and autoregressive finetuning to meet different requirements with the same pretrained model structure. 

Our proposed model achieves significant performance improvements on all tasks for non-autoregressive and semi-non-autoregressive finetuning, and for autoregressive finetuning, it can attain comparable performance with strong autoregressive pretrained models. Compared to the strong semi-non-autoregressive baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared to the strong non-autoregressive baselines. 

Our work builds upon previous research in the field of natural language processing, including JANUS, XLNet, Attention Is All You Need, ProphetNet, MPNet, and CMLMC. JANUS proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both autoregressive and non-autoregressive manners. XLNet introduces a generalized autoregressive pretraining method that enables learning bidirectional contexts and overcomes the limitations of BERT. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, which achieves superior quality while being more parallelizable and requiring significantly less time to train. ProphetNet presents a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. CMLMC proposes the Conditional Masked Language Model with Correction that addresses the problems of indistinguishability of tokens and mismatch between training and inference. 

Compared to these previous works, our proposed BANG model is the first large-scale pretraining model designed for non-autoregressive and semi-non-autoregressive generation. It bridges the gap between autoregressive and non-autoregressive models by designing a novel model structure for large-scale pretraining. The pretrained BANG model can simultaneously support autoregressive, non-autoregressive, and semi-non-autoregressive generation to meet different requirements. Our experiments show that BANG improves non-autoregressive and semi-non-autoregressive performance significantly as well as attaining comparable performance with strong autoregressive pretrained models.