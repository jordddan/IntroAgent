Recent advancements in machine translation have led to the development of both autoregressive (AR) and non-autoregressive (NAR) models. While AR models achieve excellent performance, they suffer from slow decoding speed due to their sequential nature. On the other hand, NAR models offer faster decoding but often lag behind AR models in terms of translation quality. In this paper, we propose a novel approach, the Conditional Masked Language Model with Correction (CMLMC), that bridges the gap between AR and NAR models by addressing the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation.

Our proposed CMLMC model modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Additionally, we introduce a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. Our experiments show that CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks.

To provide context for our work, we review related works in the field of machine translation. JANUS is a joint AR and NAR training method that enhances model performance in both AR and NAR manners simultaneously. XLNet is a generalized autoregressive pretraining method that overcomes the limitations of BERT by enabling learning bidirectional contexts. BANG is a pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, which achieves superior quality while being more parallelizable and requiring significantly less time to train. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. Finally, Universal Conditional Masked Language Pre-training is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages.

Our proposed CMLMC model differs from these related works by addressing the shortcomings of CMLM in NAR machine translation and achieving new state-of-the-art undistilled NAR results. In summary, our work contributes to the advancement of machine translation by proposing a novel approach that bridges the gap between AR and NAR models and achieves superior performance on multiple NMT benchmarks.