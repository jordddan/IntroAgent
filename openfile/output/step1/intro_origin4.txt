Recent advances in natural language processing have been driven by the development of large-scale pre-trained models. However, traditional pre-training methods based on autoregressive language modeling have limitations in capturing bidirectional contexts and suffer from pretrain-finetune discrepancy. To address these issues, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and the proposed n-stream self-attention mechanism. 

The main contributions of this paper are: (1) introducing ProphetNet, a new large-scale pre-trained Seq2Seq model with a novel self-supervised objective future n-gram prediction, (2) developing a method to simultaneously predict the future n-gram at each time step during the training phase, (3) extending the two-stream self-attention proposed in XLNet to n-stream self-attention, (4) pre-training ProphetNet on two scale pre-trained datasets and achieving new state-of-the-art results on CNN/DailyMail and Gigaword, and (5) fine-tuning ProphetNet on several NLG tasks and achieving the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

To provide context, we briefly mention some related works. JANUS proposes a joint autoregressive and non-autoregressive training method to enhance the model performance in both AR and NAR manner simultaneously. XLNet introduces a generalized autoregressive pretraining method that enables learning bidirectional contexts and overcomes the limitations of BERT. BANG bridges autoregressive and non-autoregressive generation by designing a novel model structure for large-scale pretraining. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between NAR and AR models and proposes the Conditional Masked Language Model with Correction. Universal Conditional Masked Language Pre-training pre-trains a sequence-to-sequence model but with a bidirectional decoder to produce notable performance gains for both Autoregressive and Non-autoregressive NMT.

Compared to these related works, ProphetNet introduces a novel self-supervised objective future n-gram prediction and the proposed n-stream self-attention mechanism, which achieves new state-of-the-art results on several NLG tasks. The proposed method to simultaneously predict the future n-gram at each time step during the training phase encourages the model to plan for future tokens and prevents overfitting on strong local correlations. The n-stream self-attention mechanism extends the two-stream self-attention proposed in XLNet and includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. The experimental results demonstrate the effectiveness of ProphetNet in various NLG tasks and its potential for future research in natural language processing.