The field of natural language processing has seen significant advancements in recent years, with pre-training methods such as BERT and XLNet achieving state-of-the-art results on various downstream tasks. However, these methods have their limitations, such as neglecting the dependency among predicted tokens and suffering from position discrepancy between pre-training and fine-tuning. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that addresses these limitations while unifying the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM).

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, allowing the model to consider the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-trained MPNet on a large-scale text corpus and fine-tuned it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Our proposed method builds upon previous works such as BERT and XLNet, which have achieved significant success in pre-training methods. However, MPNet introduces a novel approach that addresses the limitations of these methods while unifying their advantages. Other related works, such as JANUS, BANG, Attention Is All You Need, ProphetNet, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, and Universal Conditional Masked Language Pre-training, have also contributed to the field of natural language processing. However, our proposed method differs from these works in its approach to addressing the limitations of previous pre-training methods and achieving state-of-the-art results on various downstream tasks.

In summary, our proposed pre-training method, MPNet, addresses the limitations of previous pre-training methods while unifying their advantages. Our experiments show that MPNet outperforms previous state-of-the-art models on various downstream tasks. This work contributes to the field of natural language processing and has significant implications for future research in this area.