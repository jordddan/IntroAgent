Machine translation has been a long-standing challenge in the field of natural language processing. Despite significant progress in recent years, there is still a need for more effective and efficient approaches to improve the quality of machine translation. In this paper, we propose a novel pre-training model for machine translation, called CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.

Our proposed approach builds upon existing literature in the field of machine translation. We conduct a comprehensive review of related work, highlighting the gaps in the literature and comparing with existing literature. We compare our approach with several state-of-the-art models, including JANUS, XLNet, BANG, Attention Is All You Need, ProphetNet, MPNet, and CMLMC. We show that our approach outperforms these models in terms of translation quality and efficiency.

The main contributions of this paper are: 1) the development of CeMAT, a pre-training model for machine translation that can provide unified initialization parameters for both AT and NAT tasks, 2) the introduction of aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders, and 3) the demonstration of the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. We provide a detailed description of our proposed approach, including the technical details and experimental setup, and how it addresses the research questions or objectives. We also provide a clear roadmap of the paper and an overview of the paper's structure, including the organization of the remaining sections.