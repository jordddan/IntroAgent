Machine learning has revolutionized the field of artificial intelligence, enabling computers to learn from data and make predictions or decisions without being explicitly programmed. One of the most important applications of machine learning is natural language processing, which involves teaching computers to understand and generate human language. In recent years, pretraining methods based on transformer models have achieved state-of-the-art performance on a wide range of natural language processing tasks. However, these methods still suffer from limitations such as the pretrain-finetune discrepancy and the independence assumption made in the training process.

In this paper, we propose XLNet, a generalized autoregressive pretraining method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

We also improve architectural designs for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. We propose a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Empirically, we demonstrate that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. We also conduct a comprehensive review of related work, highlighting the gaps in the literature and comparing with existing literature. The proposed method is explained in detail, including the technical details and experimental setup, and how it addresses the research questions or objectives. Finally, we provide a clear roadmap of the paper and an overview of the paper's structure, including the organization of the remaining sections.