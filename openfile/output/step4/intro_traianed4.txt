Machine translation has been a long-standing challenge in the field of artificial intelligence. While autoregressive (AR) models have achieved excellent performance, non-autoregressive (NAR) models have been proposed to address the time-consuming nature of AR models. However, NAR models still lag behind AR models in terms of performance, and only become competitive when trained with distillation. To address this issue, this paper proposes a novel method called JANUS, which combines the strengths of both AR and NAR models while avoiding their weaknesses. 

In the related work, several pre-training models have been proposed to improve the performance of NMT. Universal Conditional Masked Language Pre-training (CeMAT) is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses the problems of indistinguishability of tokens and mismatch between training and inference. BANG is a new pre-training model that bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

The proposed JANUS method introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. JANUS is demonstrated to be effective on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Furthermore, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism. 

In summary, this paper proposes a novel method called JANUS that combines the strengths of both AR and NAR models while avoiding their weaknesses. The proposed method is demonstrated to be effective on multiple NMT datasets and pretraining models, achieving significant improvements in performance. The rest of the paper is organized as follows: Section 2 provides a detailed description of the proposed method. Section 3 presents the experimental setup and results. Section 4 analyzes the experimental results. Finally, Section 5 concludes the paper and discusses future work.