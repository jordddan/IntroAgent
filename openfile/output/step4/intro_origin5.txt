Recent advances in natural language processing have led to the development of large-scale pretraining models that have significantly improved the performance of various NLP tasks. However, most of these models are autoregressive, which means they generate tokens sequentially, leading to slow inference times. Non-autoregressive models, on the other hand, generate tokens in parallel, which makes them faster but less accurate. Bridging the gap between these two approaches is a challenging task that requires a novel model architecture.

In this paper, we propose BANG, the first large-scale pretraining model designed for Non-autoregressive (NAR) and semi-NAR generation. BANG bridges the gap between Autoregressive (AR) and NAR via pretraining a generative model. Our model is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

Our proposed model achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. We compare our work with several related works, including Universal Conditional Masked Language Pre-training, ProphetNet, XLNet, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, JANUS, MPNet, and Attention Is All You Need. Our work differs from these related works in terms of the model architecture and the ability to support both AR and NAR generation.

In summary, our work proposes a novel model architecture that bridges the gap between AR and NAR generation, achieving significant performance improvements on various NLP tasks. Our work has important implications for the AI community, as it provides a new approach to developing large-scale pretraining models that can support both AR and NAR generation.