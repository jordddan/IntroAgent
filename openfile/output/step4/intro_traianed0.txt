Machine translation has been a popular research topic in the field of artificial intelligence for decades. Despite significant progress, there are still challenges in achieving high-quality translations, especially for low-resource languages. In this paper, we propose a novel pre-training model, CeMAT, that can improve machine translation performance on both low-resource and high-resource settings.

To provide a comprehensive overview of related work, we highlight the gaps in the existing literature that our research aims to fill. We discuss several recent works, including Universal Conditional Masked Language Pre-training, ProphetNet, XLNet, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, JANUS, BANG, MPNet, and Attention Is All You Need. While these works have made significant contributions to the field, there is still a need for a pre-training model that can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

The main contribution of this paper is the development of CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. We also introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders.

We provide a clear motivation for the proposed research, explaining why the proposed model is necessary and how it addresses the gaps in the existing literature. We explain the proposed model and techniques in detail, including the technical details and experimental results to support the effectiveness of the proposed model. We also provide a clear comparison with previous work and explain the methodology used, highlighting the advantages of the proposed method over existing methods.

In conclusion, this paper proposes a novel pre-training model, CeMAT, that can improve machine translation performance on both low-resource and high-resource settings. We provide a comprehensive overview of related work, highlight the gaps in the existing literature, and explain the proposed model and techniques in detail. We also provide experimental results to support the effectiveness of the proposed model and compare it with previous work. The rest of the paper is organized as follows: Section 2 describes the related work in more detail, Section 3 explains the proposed model and techniques, Section 4 presents the experimental results, and Section 5 concludes the paper.