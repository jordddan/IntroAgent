The field of natural language processing has seen significant advancements in recent years, with pre-training methods such as BERT and XLNet achieving state-of-the-art results on various downstream tasks. However, these methods have their limitations, such as neglecting the dependency among predicted tokens and suffering from position discrepancy between pre-training and fine-tuning. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that addresses these limitations while unifying the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM).

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, allowing the model to consider the dependency among predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-trained MPNet on a large-scale text corpus and fine-tuned it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

To provide context, we briefly mention related works such as ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction, and XLNet, which enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. We also discuss CMLMC, which addresses the problems of indistinguishability of tokens and mismatch between training and inference, and JANUS, which proposes a joint autoregressive and non-autoregressive training method. Additionally, we introduce BANG, which bridges the gap between autoregressive and non-autoregressive generation, and Attention Is All You Need, which proposes a new simple network architecture based solely on attention mechanisms.

In summary, our work proposes a novel pre-training method that unifies the advantages of MLM and PLM while addressing their limitations. Our experiments demonstrate that MPNet outperforms previous state-of-the-art models on various downstream tasks.