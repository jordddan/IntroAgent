Machine translation has been a long-standing challenge in the field of artificial intelligence. While autoregressive (AR) models have achieved impressive results, they suffer from slow inference times due to their sequential nature. Non-autoregressive (NAR) models have been proposed to address this issue, but they still lag behind AR models in terms of performance. In this paper, we propose a novel approach, the Conditional Masked Language Model with Correction (CMLMC), which achieves state-of-the-art NAR performance without the need for distillation.

Our proposed CMLMC model builds upon the Conditional Masked Language Model (CMLM) by modifying the decoder structure to incorporate causal attention layers and expose positional encodings. Additionally, we introduce a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. Our approach achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks.

To provide context for our work, we review related works in the field of NMT. Universal Conditional Masked Language Pre-training proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet proposes a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the CMLMC model. JANUS proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. BANG proposes a new pre-training model to bridge the gap between AR and NAR generation. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

Our proposed CMLMC model differs from these related works by addressing the shortcomings of the CMLM model in NAR machine translation and introducing a novel correction loss to improve translation accuracy. We believe that our approach represents a significant step forward in the field of NMT and has the potential to improve the efficiency and accuracy of machine translation systems.