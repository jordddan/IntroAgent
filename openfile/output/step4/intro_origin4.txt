Recent advances in natural language processing have led to the development of powerful sequence-to-sequence models for tasks such as machine translation and text generation. Autoregressive (AR) models, which generate output tokens one at a time conditioned on previous tokens, have achieved state-of-the-art performance on many benchmarks. However, AR models suffer from slow inference due to their sequential nature. Non-autoregressive (NAR) models, which generate output tokens in parallel, have been proposed to address this issue. While NAR models are faster, they often lag behind AR models in terms of performance.

To address the limitations of both AR and NAR models, we propose JANUS, a joint AR and NAR training method that combines the strengths of both models while avoiding their weaknesses. JANUS introduces an auxiliary distribution to bridge the discrepancy between AR and NAR models, allowing them to learn from each other. Our approach achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average.

Our work builds upon several related works in the field of pre-training and sequence generation. Universal Conditional Masked Language Pre-training (CeMAT) proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes a Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. BANG proposes a new pre-training model to bridge the gap between AR and NAR generation. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

Compared to these related works, JANUS introduces a novel approach to combine AR and NAR models, achieving significant improvements in performance without the need for distillation data. Our approach also demonstrates the ability to transfer AR knowledge to NAR, making it a promising direction for future research. In the following sections, we provide a detailed description of our proposed method and experimental results on multiple generation tasks, including machine translation and GLGE benchmarks.