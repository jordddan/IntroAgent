Machine translation has been a long-standing challenge in the field of natural language processing. While autoregressive (AR) models have achieved impressive performance, they suffer from slow decoding speed due to their sequential nature. Non-autoregressive (NAR) models, on the other hand, can generate translations in parallel, but their performance lags behind AR models, especially when trained without distillation data. In this paper, we propose a novel method called JANUS that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Furthermore, we exceed the non-autoregressive pretraining model BANG on the same GLGE tasks and achieve comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Our work builds upon and extends the existing literature on pre-training models for sequence-to-sequence tasks. Prior works have proposed various pre-training methods, such as ProphetNet, MPNet, CeMAT, and XLNet, to improve the performance of NMT models. However, these methods either focus on AR or NAR models, neglect the dependency among predicted tokens, or suffer from pretrain-finetune discrepancy. In contrast, JANUS combines the strengths of both AR and NAR models and bridges the gap between them, achieving state-of-the-art performance on multiple NMT datasets.

In summary, our main contributions are: (1) introducing JANUS, a method that combines the strengths of both AR and NAR generation models while avoiding their weaknesses to improve performance, (2) proposing an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, (3) demonstrating the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, and (4) exceeding the non-autoregressive pretraining model BANG on the same GLGE tasks and achieving comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism. Our work has the potential to significantly improve the performance of NMT models and accelerate the development of more efficient and accurate machine translation systems.