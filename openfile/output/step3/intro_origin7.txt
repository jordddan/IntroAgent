Machine translation has been a long-standing challenge in the field of artificial intelligence. While autoregressive (AR) models have achieved impressive results, they suffer from slow decoding speed due to their sequential nature. Non-autoregressive (NAR) models, on the other hand, can translate blocks of tokens in parallel, but they often lag behind their AR counterparts in terms of performance. Recent work has explored the use of pre-training to improve NAR models, but these approaches still require distillation to achieve competitive results.

In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC), which addresses the limitations of existing NAR models without the need for distillation. Our model builds upon the Conditional Masked Language Model (CMLM) by modifying the decoder structure to expose positional encodings and incorporate causal attention layers. We also introduce a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence.

Our proposed method achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. To the best of our knowledge, this is the first work to achieve such results without the need for distillation. We believe that our approach has the potential to significantly improve the efficiency and effectiveness of NAR machine translation.

Related works in this area include ProphetNet, MPNet, CeMAT, JANUS, BANG, and XLNet. ProphetNet introduces a novel self-supervised objective for pre-training, while MPNet leverages the dependency among predicted tokens through permuted language modeling. CeMAT pre-trains a unified model for fine-tuning on both AR and NAR NMT tasks, while JANUS proposes a joint AR and NAR training method. BANG bridges the gap between AR and NAR generation, and XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. Our proposed CMLMC method differs from these works by addressing the indistinguishability of tokens and the mismatch between training and inference in NAR models without the need for distillation.

In summary, our proposed CMLMC method represents a significant step forward in the development of efficient and effective NAR machine translation models. By addressing the limitations of existing approaches, we believe that our work has the potential to significantly improve the state-of-the-art in this area and contribute to the broader field of artificial intelligence.