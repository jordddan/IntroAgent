Machine learning has made significant progress in recent years, particularly in the field of natural language processing (NLP). However, generating high-quality text remains a challenging task, especially when it comes to non-autoregressive (NAR) and semi-NAR generation. Bridging the gap between autoregressive (AR) and NAR generation is a crucial research question that has yet to be fully addressed. In this paper, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pretraining a generative model.

Our proposed model, BANG, is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. We demonstrate that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.

We provide a comprehensive overview of related work, highlighting the main differences from our work and explaining how our work builds upon and extends the existing literature. We compare our work with other pretraining models such as ProphetNet, MPNet, CeMAT, XLNet, and JANUS, and show that BANG outperforms them in terms of performance and efficiency.

Our proposed model, BANG, is a significant contribution to the field of NLP, as it bridges the gap between AR and NAR generation and supports NAR, semi-NAR, and AR finetuning. We provide a structured approach and technical details of the proposed model, highlighting its advantages over existing models. We conclude the introduction by summarizing the significance of the research and its potential impact on the field of machine learning.