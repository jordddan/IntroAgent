Recent advances in natural language processing have led to the development of large-scale pretraining models that have achieved state-of-the-art performance on various language tasks. However, most of these models are autoregressive, which means that they generate tokens sequentially, leading to slow inference times. Non-autoregressive models have been proposed to address this issue, but they often suffer from lower performance compared to their autoregressive counterparts. 

In this paper, we propose BANG, a large-scale pretraining model designed for non-autoregressive and semi-non-autoregressive generation. BANG bridges the gap between autoregressive and non-autoregressive models by pretraining a generative model using an efficient cross-stream visible n-stream decoder that supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG also supports non-autoregressive, semi-non-autoregressive, and autoregressive fine-tuning to meet different requirements with the same pretrained model structure. 

Our proposed model achieves significant performance improvements on all tasks for non-autoregressive and semi-non-autoregressive fine-tuning, and for autoregressive fine-tuning, it can attain comparable performance with strong autoregressive pretrained models. Compared with the semi-non-autoregressive strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong non-autoregressive baselines. 

Our work is motivated by the need for faster inference times without sacrificing performance. The proposed model is relevant to the AI community as it provides a solution to the problem of slow inference times in autoregressive models. Our research questions include how to bridge the gap between autoregressive and non-autoregressive models and how to design a large-scale pretraining model that supports non-autoregressive and semi-non-autoregressive generation. 

Related works include ProphetNet, MPNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, JANUS, XLNet, and Improving Non-Autoregressive Translation Models Without Distillation. While these works have made significant contributions to the field of natural language processing, our proposed model differs in its ability to support non-autoregressive and semi-non-autoregressive generation while achieving comparable performance with strong autoregressive pretrained models. 

In summary, our proposed model, BANG, bridges the gap between autoregressive and non-autoregressive models and supports non-autoregressive and semi-non-autoregressive generation while achieving comparable performance with strong autoregressive pretrained models. This work is relevant to the AI community as it provides a solution to the problem of slow inference times in autoregressive models.