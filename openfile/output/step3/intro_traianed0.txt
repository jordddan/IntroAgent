Machine learning has revolutionized the field of natural language processing (NLP) by enabling the development of powerful models for various NLP tasks. One of the key challenges in NLP is to develop models that can generate coherent and fluent text. In this paper, we propose a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction. The main contributions of this paper are: (1) developing a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations; (2) extending the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction; (3) pre-training ProphetNet on two scale pre-trained datasets and achieving new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS; (4) fine-tuning ProphetNet on several NLG tasks and achieving the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Our work builds upon and extends the existing literature on pre-trained Seq2Seq models. Previous models have focused on optimizing one-step-ahead prediction, while ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We also extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. Our experiments show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.

Our proposed model has several unique features and technical contributions. First, we introduce a novel self-supervised objective future n-gram prediction, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. Second, we develop a method to simultaneously predict the future n-gram at each time step during the training phase, which further improves the model's ability to generate coherent and fluent text. Third, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which improves the model's ability to capture long-range dependencies and generate more accurate predictions.

In conclusion, our work presents a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction and extends the two-stream self-attention proposed in XLNet to n-stream self-attention. Our experiments show that ProphetNet achieves new state-of-the-art results on several NLG tasks, demonstrating the effectiveness of our proposed model. We believe that our work will have a significant impact on the field of machine learning and NLP, and will inspire further research in this area.