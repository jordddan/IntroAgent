Recent advances in natural language processing have led to significant improvements in various tasks such as machine translation, summarization, and question generation. Pre-trained sequence-to-sequence models have played a crucial role in these advancements. However, traditional sequence-to-sequence models suffer from overfitting on strong local correlations and lack the ability to plan for future tokens. 

To address these issues, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet. ProphetNet introduces a novel self-supervised objective future n-gram prediction and a method to simultaneously predict the future n-gram at each time step during the training phase. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. Additionally, ProphetNet extends the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

The proposed model is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, fine-tuning ProphetNet on several NLG tasks results in the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

To provide context, we briefly mention some related works. BANG bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation, while CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms. JANUS is a joint autoregressive and non-autoregressive training method, while XLNet is a generalized autoregressive pretraining method. MPNet is a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. Finally, CMLMC is a conditional masked language model with correction that achieves state-of-the-art NAR performance when trained on raw data without distillation.

In summary, this paper proposes a new pre-trained Seq2Seq model called ProphetNet that addresses the issues of overfitting and lack of future token planning. The proposed model achieves state-of-the-art results on various NLG tasks and outperforms existing pre-trained models.