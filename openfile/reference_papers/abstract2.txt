In this paper, we propose BANG, a new pre-
to Bridge the gap between
training model
Autoregressive (AR) and Non-autoregressive
(NAR) Generation. AR and NAR generation can
be uniformly regarded as to what extent previ-
ous tokens can be attended, and BANG bridges
AR and NAR generation by designing a novel
model structure for large-scale pretraining. The
pretrained BANG model can simultaneously sup-
port AR, NAR and semi-NAR generation to meet
different requirements. Experiments on question
generation (SQuAD 1.1), summarization (XSum)
and dialogue generation (PersonaChat) show that
BANG improves NAR and semi-NAR perfor-
mance signiÔ¨Åcantly as well as attaining compara-
ble performance with strong AR pretrained mod-
els. Compared with the semi-NAR strong base-
lines, BANG achieves absolute improvements of
14.01 and 5.24 in the overall scores of SQuAD
1.1 and XSum, respectively. In addition, BANG
achieves absolute improvements of 10.73, 6.39
and 5.90 in the overall scores of SQuAD, XSUM
and PersonaChat respectively compared with the
strong NAR baselines.
