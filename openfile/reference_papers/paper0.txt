8050
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8050 - 8060
December 7-11, 2022 Â©2022 Association for Computational Linguistics

JANUS:JointAutoregressiveandNon-autoregressiveTrainingwithAuxiliaryLossforSequenceGenerationXiaoboLiang1JuntaoLi1LijunWu2MinZhang11SoochowUniversity,2MicrosoftResearchxbliang3@stu.suda.edu.cn,{ljt,minzhang}@suda.edu.cnlijuwu@microsoft.comAbstractTransformer-basedautoregressiveandnon-autoregressivemodelshaveplayedanessen-tialroleinsequencegenerationtasks.Theautoregressivemodelcanobtainexcellentperformance,whilethenon-autoregressivemodelbringsfastdecodingspeedforinfer-ence.Inthispaper,weproposeJANUS,aJointAutoregressiveandNon-autoregressivetrainingmethodusingaUxiliarylosStoen-hancethemodelperformanceinbothARandNARmannersimultaneouslyandeffectivelyal-leviatetheproblemofdistributiondiscrepancy.Further,wepre-trainBARTwithJANUSonalargecorpuswithminimalcost(16GPUdays)andmaketheBART-JANUScapableofnon-autoregressivegeneration,demonstratingthatourapproachcantransfertheARknowledgetoNAR.Empirically,weshowourapproachandBART-JANUScanachievesignificantimprove-mentonmultiplegenerationtasks,includingmachinetranslationandGLGEbenchmarks.OurcodeisavailableatGithub1.1IntroductionThetransformer-basedautoregressive(Vaswanietal.,2017;Soetal.,2019)(AR)generationmodelhasachievedhigh-qualityresultsinvariousnaturallanguagegenerationtasks.Meanwhile,thenon-autoregressive(NAR)(Guetal.,2018;Leeetal.,2018;Libovick`yandHelcl,2018;Suetal.,2021)generationmethodsshowgreatpotentialtoreducetheinferencelatencybyintroducingparalleldecod-ing.Especially,iterativegenerativeparadigmslikeCMLM(Wangetal.,2019a;Ghazvininejadetal.,2019)candynamicallyadaptthetrade-offbetweenperformanceandlatency.Recentworks(Qietal.,2021;Guoetal.,2020;Tianetal.,2020)havesuc-cessfullycombinedthesetwomechanismsbyjointtraining.However,theseapproachesonlyconsidertherelevanceofmodelparameters,ignoringthecor-relationsbetweenthetwomanners,whichrequire1https://github.com/dropreg/JANUSFigure1:TheexampleofJANUS.Weexhibitsomecasesthatshowthedistributiondiscrepancyduetothedifferenceincontextwithorangebackground.effortsforimprovement.Therefore,inthispaper,weattempttoleveragethemeritsofbothARandNARmechanismswhileavoidingtheirweaknessestoimprovetheirperformance.TheprincipaldifferencebetweenARandNARisthattheyusedistinctattentionmechanisms.Inparticular,theARusesunidirectionalatten-tion(Vaswanietal.,2017)tosimplifythesentenceprobabilitydistributionsbyintroducingMarkovHypothesis,whichoutputsthenexttokenonlydependingonthepreviouscontext.Thispatternmakestheoutputdistributionofeachtokenaccu-rateandunambiguousbutalsolacksdiversityininference.TheNARintroducesbi-directionalat-tention(Devlinetal.,2019)thatcancapturebi-directionalcontextbyconsideringthewholesen-tenceinformation.Thispatternneedstopredictmultipletokenssimultaneously,whichcausesthetokendistributiontobeambiguous,stemmingfromthemulti-modalityproblem(Zhouetal.,2019).Inspiredbyknowledgedistillation(Hintonetal.,2015)anddeepmutuallearning(Zhangetal.,2018b),wetrytoregularizethemodelpredictionsbyminimizingthedistributiondistancebetweentheoutputgeneratedbythetwomanners.How-ever,theconditionaloutputprobabilityofeachtargetpositionisinconsistentbetweenARandNAR.WepresentanexampleinFigure1toil-8051

lustratesuchadiscrepancy,wheretheARout-putprobabilityPAR(y3|X,y1,y2)fortokeny3de-pendsonthecontextinformation{X,y1,y2},andtheNARoutputPNAR(y3|X,y1,m2,y4,m5,y6)reliesonmorecontext{X,y1,m2,y4,m5,y6}.ThePAR(y3)lackthetokeny4,y6asconditioncomparetoPNAR(y3),incontrast,thePNAR(y3)missesthecontexty2.Suchdistincttokendistri-butionsbringdifficultyindirectlyregularizingthepredictiondistancesD(AR|NAR).Totacklethisissue,weintroduceanauxil-iarydistributionPAUXtobridgethediscrepancybetweenPARandPNARwiththehelpofmini-mizingthedistributiondistanceD(AR|AUX)andD(NAR|AUX)ratherthanthedirectD(AR|NAR),asshowninFigure1.ThisapproachbenefitsARandNARtolearnfromeachotherwiththefollowingadvantages:(1)ThePAUXpossessesrichcontextinformationcomparedwithARandNAR.Forexample,comparingtoPARandPNAR,PAUX(y3|X,y1,y2,y4,m5,y6)containsallto-keny2,y4,y6ascondition.(2)Thereisanautore-gressivedependencybetweenthepredictedtokens,whichguaranteestheaccuracyofoutputdistribu-tionandwithoutambiguityforeachtoken.(3)Therandommaskmechanismappliedtothisdistribu-tion,similartoNAR,enablesthemodeltolearnvarioustokendistributions.Foreffectiveimplemen-tation,wedrawontheexperienceoftwo-streamself-attention(Yangetal.,2019b)andpositioncom-pensation(Songetal.,2020).WebuildthePAUXbyalteringtheattentionmatrixofNAR,whichmerelymodifiesthetrainingprocedurewithoutaf-fectingtheinference.WefirstlaunchedexperimentsonmultipleNMTdatasetstoverifywhetherJANUScanhelpimproveperformanceintwomanners.Then,weexploredourmethodonexistingautoregressivepretrainingmodels,inwhichwepre-trainourBART-JANUSinitializedbyBART-baseonalargecorpusandfine-tunedtoadaptvariousdownstreamtasks.HereweusetheGLGEdatasetsasourbenchmarktoval-idatethemodelperformance.Meanwhile,wealsoconductedcomparativeablationstudiestoillustratetheeffectivenessofourproposedmethod.Exper-imentalresultsshowthatourmethodcanachievesimilarresultstothestate-of-the-artNARmodelwithoutdistillationdata.Itsimultaneouslyim-provestheARmodelperformancebymorethan1.5BLEUscoresonaverage.Furthermore,ourmodelexceedsthenon-autoregressivepretrainingmodelBANGonthesameGLGEtasks.Perhapssurpris-ingly,itcanachievecomparableperformancewiththeARmanneratleasttwotimesspeedupbasedontheiterativeinferencemechanism.2RelatedWorkHowtotakeadvantageofbothARandNARparadigmsforsequencegenerationhasbeendis-cussedheatedly.Guetal.(2018)andZhouetal.(2019)introducedknowledgedistillation(Hintonetal.,2015)intothetrainingprocedureofNAR,whichcanreducethedatacomplexityandallowthemodeltolearnthevariationsfromdata.Someapproaches(Guoetal.,2020;SunandYang,2020;Haoetal.,2021)trytotransferARknowledgetoNARbyapplyingsomespecificlearningstrat-egy,whichcangraduallyguidethemodeltraining.Zhouetal.(2020)proposedthattheNARmodelcaneffectivelyimprovetheARperformancebyutilizingtheNARoutputwithslightspeeddegrada-tion.Severalotherworks(Tianetal.,2020;Wangetal.,2022)havealsoproposedtoimprovetheNARperformancebycombiningmultipledecod-ingparadigmsintoaunifiedmodel.ThemostrelatedworkisBANG(Qietal.,2021),whichproposedanovelcross-streamvisiblen-streamself-attentionstructuretobridgethegapbetweenARandNARgeneration.Specifically,itsupportsdifferentdecodingmannersusingAR,semi-NAR,andNARlossfunctionsastrainingob-jectives.Unlikeit,ourapproachnotonlyfocusesonjointtrainingandparametersharingbutalsoconsiderstheinnerrelationshipofoutputdistri-butions.Comparedwithpreviouswork,wecanintegratethemeritsoftwodifferentwayswhilecircumventingtheirdrawbacksbyintroducinganauxiliaryloss.Someworks(Zhangetal.,2018b;Shenetal.,2020;Liangetal.,2021)haveexploredthatdistributionregularizationcanmakethemodelmorerobust.Nevertheless,thepurposeofourworkismoreinclinedtoconstraintwodistincttrainingoptimizationobjectivesratherthanthesame.Be-sides,thepretrainingmodels(Raffeletal.,2020;Fedusetal.,2022;Brownetal.,2020)canachieverobustgeneralizationbutsufferfromhugemodelsize.Themodelwithmassiveparametersispronetoleadtheunacceptableinferencelatencyforgen-erationtasks.Ourapproachattemptstoleverageanexistingautoregressivemodel(Lewisetal.,2020)tomakeitobtaintheabilitiesofnon-autoregressiveratherthanpre-trainaNARmodelfromscratch.8052

Figure2:TheoverviewofJANUS.Theleftsideisthemodelarchitecture,inwhichweusethetwo-streamattentiontotrainthemodel.Thequerystreamsrequiredifferentkeysandvaluesforeachinput.Therightsideistheattentionmatrixofthedecoderself-attentionlayer,andthewhitedotsrepresentthemaskedsimilarityscore.Thesymbolyisthewordembedding,andpisthepositionembeddingfordecodermodules.3ApproachInthissection,weelaborateonthedetailsofourJANUSmethod,whichsupportsARandCMLM-basedNARgenerationandmutuallyenhancestheirperformance.AsshowninFigure2,weintroducethetwo-streamself-attentionmechanismtounifythesetwomannersintooursinglemodel.3.1ModelArchitectureWefirstintroduceanauxiliarydistribution(AUX)asabridgebetweenARandNARmodels.Consid-eringthatthepredictedtokensinCMLMareinde-pendentofeachother,ignoringtheirdependency,weintegrateAUXwiththeNARgenerationpro-cedureusingthetwo-streamself-attentionmech-anism,whichisproposedbyXLNet(Yangetal.,2019b)toovercomethelimitationsofMLMbyintroducingautoregressiveinformation.Morecon-cretely,weuseasanalternativeofXLNet,i.e.,MP-Net(Songetal.,2020),totacklethepositiondis-crepancyproblembypositioncompensation,whichinputsauxiliarypositioninformationtomakethemodelseethewholesentence.Unlikeitsorig-inalfunctioninaddressingpositiondiscrepancybetweenpre-trainingandfine-tuning,weintroduceMPNettoalleviatethedistributiondiscrepancybe-tweenARandNARpatterns.Inparticular,giventhetrainingpairs(X,Y),thetransformerencodertakesthesourcesentenceXasinput.Thedecoderacceptsthemultipleinputsequencestobuildtheattentionmatrixfordiffer-entstreams.ForAR,weinputanoriginalsen-tence{b,y1,...,y5}2andafullymaskedsentence{m1,...,m6}intothedecoder.ForNARandAUX,wefirstsamplethemaskedtargetYMASKfromtheoriginalsentenceY,e.g.YMASK={y1,m2,m3,y4,m5,y6}.Then,weexpandthemaskedse-quencewiththeground-truthtokenusingpositioncompensationandreorderthepositionofthewholesentencetoobtainYMASK={y1,y4,y6,m2,m3,m5,y2,y3,y5}.Finally,wetakethereorderedsen-tenceYMASKandmaskedsentence{m2,m3,m5}asdecoderinputs.Thedecodercanoutputthedis-tributionsaccordingtothecorrespondinggenera-tionparadigmbymanipulatingtheattentionmatrixofeachdecoderself-attentionlayer.ARPatternTheARdecodersuffersfromthepositionshiftofthepredictedtokenduetosharingparameterswiththeNARdecoder.Thatis,thedecoderisconfusedaboutwhethertooutputthecurrentorthenexttoken.So,weusetheMASKtokeninthequerystreaminsteadofthecontentstreamtogeneratetheoutputdistribution.Specifi-cally,webuildthelowertriangularmatrixforboththequeryandcontentstream.Thecontentstreamisresponsibleforencodingthecontextinformation,andthequerystreamisforprediction.Thecontentstreamtakesthehiddenstatesof{b,y1,...,y5}asinputtoensureeachpositioncanseethepreviousposition.Thequerystreampredictseachmaskedtokenaccordingtothepreviouscontentstream,e.g.,2WeappendaspecialtokenbinfrontofsentenceY.8053

m2generatesy2accordingto{b,y1}.NARPatternWeonlyusethecontentstreamfortheNARmannerandleveragethebi-directionalattentiontogeneratetheoutputdistribution.Thebi-directionalattentionoperationonlydependsonre-orderedtargetsequences{y1,y4,y6,m2,m3,m5},whichisthefrontpartofYMASK.Itisworthnot-ingthatthebidirectionalself-attentionoperationispositionindependent,sosentencereorderingdoesnotaffectthetrainingprocedure.AUXPatternWeemployaspecificattentionmasktobuildtheauxiliarydistribution.Specif-ically,thecontentstreamtaketheYMASKasin-put,andsplitthetokensinasequenceintoorig-inalandcompensationparts.Forexample,theprevioussentence{y1,y4,y6,m2,m3,m5}isorig-inalparts,and{y2,y3,y5}isthecompensationpart.Thecontentstreamensuresthateachposi-tionincompensationpartcanseetheinputtokeninsteadoftheMASK,e.g.y3basedoncontext{y1,y4,y6,y2,y3,m5}insteadoforiginalparts.Unlikeit,thequerystreampredictsthenexttokenofmaskedsequenceinanautoregressivemanner,e.g.m3basedoncontext{y1,y4,y6,y2,m3,m5},andm5basedoncontext{y1,y4,y6,y2,y3,m5}.Inthisway,wecangeneratevariousoutputdis-tributionsbyswitchingtheattentionmatrix,whichisasample-awareoperationbecauseeachsentenceandmaskedspanshaveanarbitrarylength.3.2TrainingandInferenceARThetrainingobjectiveofARgenerationman-neristominimizethefollowingcross-entropyloss:LAR=âXNilogPAR(yi|X,Y<i),(1)whereY<ireferstothetokensbeforethei-thtimestep,Nisthetargetlength,andtheoutputproba-bilityP(yi)equalsP(mi)generatedbythequerystream.Westillusetheoutputprobabilityofquerystreamduringinferencetopredicttheresults.NARWeusespanmaskingasatokensamplingmethod,whichmaskscontiguousrandomspansratherthantokens.Weselecteachspanbysam-plingfromaPoissondistribution(Î»=2)inspiredby(Joshietal.,2020;Lewisetal.,2020),andeachtokeninthespanisreplacedwithMASKtoken.Thetraininglossfunctionistominimizethesumofnegativelog-likelihoodformaskedsequence:LNAR=âXMilogPNAR(yi|X,YMASK),(2)Figure3:DifferentSpanmaskingisappliedtoBARTencoderanddecoderinputtoavoiddecoderdegradation.whereMisthemaskedsequencelength.Duringinference,thedecoderrefinesthelow-probabilitytokensstartingwithafullymaskedsentencebyiterativemaskingandprediction.AUXThetraininglossfunctionistominimizethesumofnegativelog-likelihoodformaskedto-kengeneratedbyquerystream:LAUX=âXMilogPAUX(yi|X,Y<iâªYMask),(3)wheretheâªoperationindicatedremovingduplicatemasktokensandkeepingoriginaltokensincontext.Then,weusetheauxiliarydistributionPAUXasabridgetoregularizethetwodistributionsPARandPNARbyminimizingthetoken-levelbidirectionalKullback-Leiblerdivergence:LKL=DKL(PAR||PAUX)+DKL(PAUX||PAR)(4)+DKL(PNAR||PAUX)+DKL(PAUX||PNAR).ThefinallossweusedinJANUSisacombinationofcross-entropyandKLlosses:L=LAR+LNAR+LAUX+LKL.(5)3.3PretrainingandFinetuningWerunourapproachonthesecondphaseofpre-trainingbasedonanexistingencoder-decoderpre-trainedmodellikeBART.Specifically,weusespanmaskingfordecoderinputwhenswitchingtotheNARmannerandreplaceeachtokenwithamaskratherthaneachspan.However,thisprac-ticecausestheencoderanddecoderinputtobesimilar,anditmayforcethedecodertopredictresultsthroughitsowncontext,ignoringtheinfor-mationfromtheencoder.Inordertopreventthephenomenonofdecoderdegeneration,wefurthermaskmorespansfordecoderinputtoensurethatthedecoderneedstocombinetheencoderinforma-tiontomakeaprediction,asshowninFigure3.8054

Afterpretraining,weusethetraditionalARandNARapproachtofinetuneitondownstreamtasks.WeprovidenewtargetlengthpredictionmoduleswithrandominitializationforNARmodels,whichtakethisproblemasaclassificationtask.Weaver-agetheencoderrepresentationsofthewholesourcesentenceandmapittoavectorwiththemaximalsentencesizetoobtainthetargetlength.Then,weapplythepredictedlengthtoinitializethedecoderinputduringinference.4ExperimentalSetup4.1TaskWevalidateourapproachonmultiplesequencegenerationtasks.Forneuralmachinetranslation,wetrainourmodelfromscratch.Forothertasks,wefirstperformpre-trainingusingourmethodsonBART-basetoobtainBART-JANUS,thenusethedownstreamtasktofinetunetheBART-JANUS.Allexperimentsaredoneusingthefairseq3library.NeuralMachineTranslationWeconductourexperimentsonthreepublicdatasets:IWSLT14GermanâEnglish,WMT14GermanâEnglish4,andWMT16RomanianâEnglish5.Weonlyusetherawdatainsteadofthedistilleddatatoverifytheeffectivenessofourmethod.GLGEWeusetheGLGE6(GeneralLanguageGenerationEvaluationBenchmark)(Liuetal.,2021)toevaluatetheabilityofBART-JANUSonsequencegenerationtasks.Weselectedthreediffer-enttasks:AbstractiveTextSummarizationdatasetXsum(Rushetal.,2015)(227Karticleandsum-marypairs),Answer-awareQuestionGenerationdatasetSQuAD1.1(Rajpurkaretal.,2016)(98Kanswer,passage,andquestiondatatriples),andConversationalQuestionAnsweringdatasetPer-sonaChat(Zhangetal.,2018a)(150kpersonapro-file,conversationhistory,andresponsedatatriples).Inparticular,wechooseGLGE-Easyfromdifferentranksofdifficultysetasourdataset.Foradetaileddescription,pleaserefertotheoriginalpaper.4.2ImplementationFormachinetranslationexperiments,weusetra-ditionaltransformer_iwslt_de_ensettingfor3https://github.com/facebookresearch/fairseq4https://github.com/facebookresearch/fairseq/tree/main/examples/nonautoregressive_translation5https://github.com/facebookresearch/DisCo/issues/56https://github.com/microsoft/glgeIWSLT14dataset,whichcontains6layersinbothencoderanddecoder,theembeddingsizeis512andtheFFNlayerdimensionis1,024,thedropoutandweightdecayis0.3and0.0001respectively.WeusethemodelconfigurationtransformerforWMTdataset,with6layers,theembeddingsizeis512andtheFFNlayersizeis2,048,thedropoutvalueissettobe0.2forDeâEnand0.3forRoâEn.Weadoptthedefaultoptimizational-gorithmandlearningratescheduleasin(Vaswanietal.,2017),thatisAdam(KingmaandBa,2014)optimizerwithinitiallearningrate0.0005,learningratescheduleinverse_sqrtwith10,000warmupsteps.Labelsmoothingisutilizedinthelossfunc-tionwithavalueof0.1.TherawtextsareencodedusingBPE(Sennrichetal.,2016)asthesubwordunitsandreporttestsetperformancebymeasuringBLEU(Papinenietal.,2002).Specifically,weevaluatethefinaltranslationaccuracybyaveraging5checkpointsinbothARandNARsettings.Forpretrainingexperiments,weutilizeBART-basetoinitializeourmodel,whichcontains6layersinbothencoderanddecoder,theembeddingsizeis768andtheFFNlayersizeis3072,dropoutandattentiondropoutis0.1,and140Mmodelparame-tersintotal.Wemask30%tokenspanforencoderinputand50%fordecoderinput.Wepre-trainBART-JANUSusingthe16GBcorpus(WikipediaandBookCorpus)followingQietal.(2021)on8NVIDIA40GBA100GPUs,withalearningrateof1e-4andabatchsizeof1024sentencesonlyfor2epochs.ThetotaltrainingprocedureusesFP16forspeedupandneeds2days.ForGLGEdownstreamtasks,weloadtheBART-JANUSandfinetuneitwiththetraditionalARorNARstrategy.ForNARsettings,weuselearningrate1e-4,warmupstepsof1000,Adamoptimizer,andalabelsmoothnessof0.1for50epochs.Wesetthemaximaloutputlengthas64,64,and32forSQuAD,XSumandPersonaChat,respectively.ForARsettings,weuselearningrate3e-5,200warmupstepsfor10epochs.Forinferencestage,wesetthebeamsizeas5forARmanners,andtaketop-5predictedlengthsandselectthetranslationwiththehighestaveragetokenprobabilityforNARmanners.Wesavethecheckpointforevery10epochsandselectthebestcheckpointbasedontheperformanceonthevalidationset.Theofficialscript7isusedtoevaluatethemodelperformance.7https://github.com/microsoft/ProphetNet/blob/master/GLGE_baselines/script/eval.py8055

MethodIWSLTâ14DeâEnWMTâ16EnâRoWMTâ14EnâDeIterDeâEnRoâEnEnâRoDeâEnEnâDeTransformer(Vaswanietal.,2017)34.7434.4634.1631.0627.74#ConvolutionalTransformer(Yangetal.,2019a)----28.2#AdversarialTraining(Wangetal.,2019b)35.2---28.4#Flowseq(Maetal.,2019)-32.9132.3528.2923.641GLAT(Qianetal.,2021)32.4932.0031.1929.8425.211CMLM(Ghazvininejadetal.,2019)32.1032.8732.8629.4024.6110DisCo(Kasaietal.,2020)-32.25--25.644CMLMC(Huangetal.,2021)34.2834.1334.1430.9226.4010CMLM+Corr33.9033.9833.7530.5526.1010JANUS-AR37.24â 35.8436.0133.0928.72#JANUS-NAR34.21â 34.3634.0030.9026.4010Table1:ResultsofNMTdataset.TheAR-basedmodeldoesnotusetheaveragecheckpoint,butJANUSusesitfollowingtheNARstandardsetting.(â representthep-value<0.01accordingtosignificancetest).PatternMethodIterROUGE-1ROUGE-2ROUGE-LOVERALLLatency(ms/Sample)ARTransformer(Vaswanietal.,2017)30.6610.8024.4821.98262.47ProphetNet-base(Qietal.,2020)39.8917.1232.0729.69N/ABANG(Qietal.,2021)41.0918.3733.2230.89N/ABART-base(Lewisetal.,2020)38.7916.1630.6128.52N/ABART-base(Ourimpl)41.2217.9832.6930.63322.78*(1.0Ã)BART-JANUS41.2918.1632.7830.74332.49*NARBANGsemi-NAR(Qietal.,2021)34.7111.7129.1625.19109.77BANG(Qietal.,2021)32.598.9827.4122.9915.97BART-JANUS033.299.7428.2123.7436.66*(8.7Ã)138.0513.5031.1827.5745.51*(7.0Ã)440.6516.5632.8630.0271.85*(4.5Ã)1041.5317.7233.4030.88128.94*(2.5Ã)Table2:ResultsonXSumdataset.WereimplementedtheBART-baseresult,anotherresultfromBANG.(ârepresenttheresultbasedonourenvironment.ms/Samplerepresentsthenumberofmillisecondsconsumedpersample).5Results5.1NeuralMachineTranslationTheResultsoftheNMTdatasetareshowninTa-ble1.OurJANUS-ARmodelsurpassesthero-bustbaselinewiththeconvolutionaltransformerortheadversarialtrainingstrategy.Moreover,theJANUS-NARmodelachievessignificantim-provementabovetheCMLMbaselinebynearly1.5BLEUpoints8.WecanseethatJANUS-NARperformsbetterthanseveralstrongbaselines,suchasFlowseq,GLAT,andDisco.OurmodelachievessimilarresultscomparedwiththerecentSOTANARmodelCMLMConmultipledatasets(trainedwithrawdata)andsurpassesthemonRoâEnandEnâDedatasets.Forafaircompari-son,CMLM+CorrisbasedontheCMLMmodelwithoutintroducingadditionalparameterstoad-dresstheproblemofindistinguishabilityoftokens,andweconsistentlysurpassit.Atthesametime,wecanseethattheJANUS-ARmodeloutperforms8Wechoosethecommonlyusedmosesdecoder-scriptbootstrap-hypothesis-difference-significance.pltoconductthesignificancetest.MethodIter=1Iter=4Iter=10CMLM7.42%1.58%0.83%JANUS6.8%1.43%0.65%Table3:Tokenrepetitionofdifferentiterationstep.allbaselineswithmorethan1.5BLEUscoresonaverage.TheseresultsallsupportthatJANUScanmaketwodifferentmannersbenefitfromeachotherbyjointtraininganddistributionregularization.Furthermore,themodelmaysufferfromthemulti-modalityproblemwhentrainedwithrawdatabecauseeachsourcehasmultipletargetcandidates.Wecalculatethetokenrepetitionratioofgener-atedresults,whichcanreflectthedegreeofthemulti-modality.TheresultsareshowninTable3.JANUShaslowtokenrepetitionratiothanCMLM,demonstratingthatiteffectivelyalleviatesthemulti-modalityprobleminNARbycarryingautoregres-siveinformation.WecankeepinferencelatencythesameasCMLMsinceourapproachonlymodi-fiesthetrainingparadigm(Section6.2givesamorethoroughanalysis).8056

PatternMethodIterROUGE-LBLEU-4METEOROVERALLLatency(ms/Sample)ARTransformer(Vaswanietal.,2017)29.434.619.8614.63159.49ProphetNet-base(Qietal.,2020)48.0019.5823.9430.51N/ABANG(Qietal.,2021)49.3221.4024.2531.66N/ABART-base(Qietal.,2021)42.5517.0823.1927.61N/ABART-base(Ourimpl)43.0217.5723.5228.03157.49*(1.0Ã)BART-JANUS44.7017.3624.0728.71163.16*NARBANGsemi-NAR(Qietal.,2021)47.3917.6221.6928.90111.11BANG(Qietal.,2021)44.0712.7518.9925.2715.69BART-JANUS044.9911.1817.8924.6833.52*(4.6Ã)147.1814.3520.1427.2242.51*(3.7Ã)447.9116.3821.3628.5568.7*(2.3Ã)1048.0016.8721.5828.81127.51*(1.2Ã)Table4:ResultonSQuAD1.1.PatternMethodIterBLEU-1BLEU-2Distinct-1Distinct-2OVERALLLatency(ms/Sample)ARTransformer(Vaswanietal.,2017)41.5632.950.30.818.90138.31ProphetNet-base(Qietal.,2020)46.0038.401.37.323.25N/ABANG(Qietal.,2021)45.7735.541.48.422.78N/ABART-base(Lewisetal.,2020)47.6039.361.16.123.54N/ABART-base(Ourimpl)50.3340.111.27.324.73183.09*(1.0Ã)BART-JANUS51.1640.321.27.224.97187.77*NARBANGsemi-NAR(Qietal.,2021)39.8230.721.914.221.66109.17BANG(Qietal.,2021)31.1123.902.522.720.0514.89BART-JANUS036.1634.372.417.822.6833.81*(5.4Ã)142.7637.362.116.424.6541.88*(4.4Ã)444.6837.731.813.124.3267.90*(2.7Ã)1045.2137.901.611.123.95124.42*(1.5Ã)Table5:ResultonPersonaChat.5.2GLGEWepresenttheresultsoftheXSumtasksinTa-ble2.InARmodelgeneration,BART-JANUShasslightlyimprovedcomparedtoBART-baseonallmetricsROUGE-1,ROUGE-2,andROUGE-L.Theseresultsshowthatitischallengingtoimprovethepre-trainedBARTmodelinashorttimethroughcontinualtraining.Moreover,ourfocusisonwhetherBART-JANUScangainnon-autoregressivegenerationcapabilities.Wecanseethatwhenthenumberofiterationsisequalto0,themodelhasexceededtheBANG-NAR0.75score.Surprisingly,theNARmodeloutperformstheARmodelwhentheiterationnumberreaches10.Thisresultimpliesthatthebi-directionalattentionunderNARcaneffectivelyusethecomprehensiveinfor-mationofthedecoderinputtoobtainbeneficialrepresentations,whichisamoresuitablewaythantheuni-directionalattentionoftheARmodel.Ininferencelatency,wesetbatchsizeas1tocalculatethelatencyandseethattheNARmodelcansurpasstheARmodelwith2.5timesspeedup.Furthermore,theBART-JANUScanprogressivelyimprovethemodelperformancebyvaryingtheiterationsize.TheresultsofSQuAD1.1areshowninTable4.Sinceourmodelisatwo-stagepre-trainingwitharelativelylowtrainingcost,theARmodelmaynotobtainasignificantimprovementandlimitstheupperboundoftheNARmodel.WecanobservethatBART-JANUSdoesnotexceedBANGinmostmetricsduetothelimitationofBART-baseperfor-mance.Nevertheless,ourNARmodelachievescomparableresultstoBANGsemi-NARwhentheiterationnumberreaches10withasimilarspeedup.FromTable5,wecanseethatourmodelcon-sistentlyoutperformsallbaselineonallevaluationscoresforPersonachatdatasets.Fordialoggen-eration,theDistinct-1andDistinct-2metricsaretopreventthemodelfromgeneratingmeaninglessandboringresponses.TheBART-JANUSobtainsahighBLEUandDistinctvalue,confirmingthatourmodelcansimultaneouslyconsiderthefluencyanddiversityinthegenerateddialogresponse.6AnalysisandDiscussion6.1AblationStudyWepresentexhaustiveinvestigationsonIWSLT14DeâEnandWMT16RoâEndatasetsinbothARandNARandusetheTransformer(Vaswanietal.,2017)andCMLM(Ghazvininejadetal.,2019)as8057

LossIWSLTDeâEnWMTRoâEnARNARAUXKLARNARARNAR!%%%34.74-34.46-%!%%-32.10-32.87%%!%0.9124.41--!!%%35.2532.1234.5033.01!!%!36.0833.4535.1333.64!!!!37.2434.2135.8434.36Table6:ResultsandablationstudyontheNMTdatasets.Figure4:Resultsbyvaryingthenumberofitera-tions(solidlineisperformance,dottedlineisinferencespeed).correspondingtrainingstrategiesforbaseline.TheresultsareshowninTable6.Wecomparethecom-binationsofvariouslossestoinvestigatetheinter-actionbetweenthem,i.e.,AR,NAR,AUX,andKLloss.TheNARandARmodelsoutperformthemodelwithjointtrainingbyusingtheKLregular-izationitem,provingthatoutputspaceismorerea-sonablethanparameterspaceforenhancingeachother.TheAUXcanfurtherimprovetheperfor-mance(37.24v.s.36.80,34.21v.s.33.25,35.84v.s.35.13,and34.36v.s.33.64)byalleviatingthedistributiondiscrepancybetweenARandNARpatterns.Besides,weexaminewhetherAUXisapowerfulteacherthatcandistillknowledgetoNARandARmodelsinthethirdline.TheresultsshowAUXitselfisnotagoodteacher,whichdemon-stratespoorperformancewithboththeARandNARinferencepatterns.Inshort,alltheaboveevi-dencesupportthatJANUScanachievethemutualenhancementofARandNARratherthandistillingtheAUXknowledgeforARandNARtraininginamutuallearningfashion.6.2EffectofIterationStepTheanalysisofiterativerefinementisshowninFig-ure4.ForbothCMLMandJAUNS,wecomputetheBLEUscoreatdifferentiterations.TheJANUScansignificantlyimproveperformanceineachit-Figure5:BLEUcurvesalongwiththemodeltraining.Figure6:Traininglosscurvesalongwithtrainingepoch.erationstepandsurpassCMLMwithonlythreeiterations.TheinferencespeedisconsistentwithCMLMsinceoursolutiononlyfocusesonimprov-ingthetrainingstrategy,calculatedbytranslatingtheIWSLT14DeâEntestset.6.3TrainingAnalysisTobettershowthetrainingcostandperformanceofJANUS,westudythemodeltrainingfromdif-ferentaspects.WegivedetailedcomparisonsofthetrainingtimeontheIWSLT14DeâEndatasetwhenthemodelconverges.Specifically,thetotaltrainingcostofJANUSisabout25,235secondsfor170epochson2GPUs,whileCMLMandARTransformerscostsabout9,852secondsand3,975secondson1GPU,respectively.WecanseethatJANUSdoesrequire4or5timesthetrainingcost.Althoughthemodelneedsmoretimetoconverge,JANUShasachievedsignificantperformanceim-provement.Furthermore,wealsostudythetrainingprocessandvisualizetheperformanceimprove-mentsinFigure5.WeplotthetestBLEUcurvesalongwiththetrainingupdates.Fromthecurves,wecanfindthatJANUSexceedsthebaselineatthesameepochandcancontinuetogrow.AsshowninFigure6,weplotthecurvesofthreedifferenttraininglossesofthemodelonIWSLT14DeâEnneuralmachinetranslationtasks.Wecanseethatthelossvaluebelongstothesamescale8058

MethodROUGLE-LBLEU-4METEOROVERALLBART-base42.458.9216.4222.59BART-CMLM41.939.0116.8622.60BART-JANUS44.9911.1817.8924.68Table7:ResultsonSQuAD1.1withvariouspretraining.MethodROUGLE-1ROUGLE-2ROUGLE-LOVERALLBART-base31.558.4726.9522.32BART-CMLM32.478.9727.6023.01BART-JANUS33.299.7428.2123.74Table8:ResultsonXSumwithvariouspretraininganddoesnotrequireanexternalbalancecoefficient.TheauxiliarylosscankeepasimilarscopetoARlossintraining,showingthattheAUXismorereasonabletobridgeARandNARdistributions.6.4EffectofVariousPretrainingInthissection,weconsidertheimpactofmodelperformancefordifferentpretrainedmodelsasini-tializationsontheNARmodel.WeprovideBART-baseasmodelinitializationandthenuseCMLMasfinetuningmethod.Further,WeuseCMLMtopre-trainBARTandkeepconsistentwithBART-JANUSinothersettings.ResultsshowthattheJANUScanimproveNARfinetuningresultssig-nificantlyinbothtasks.Thisresultdemonstratesthattheproposedpretrainingstrategyviatransfer-ringtheknowledgefromARtoNARiscriticaltoachievingabetterperformanceinNARgeneration.7ConclusionInthiswork,weproposeanewtrainingstrategynamedJANUS,whichsupportstheAR,NAR,anditerativerefinementgenerationmechanisms.Mean-while,wetackletheproblemofdistributiondis-crepancybetweentheARandNARbyintroducinganauxiliarydistribution.ExperimentsshowthatJANUScansignificantlyimprovetheARandNARmodelperformance.Further,wepre-trainBART-JANUSandachievecomparableperformancewiththeNARpertainedmodelBANG.Wearealsoin-terestedindesigningeffectivefinetuningstrategiestoapplyJANUS,whichleavesitasfuturework.8LimitationOurmodelgainsnoticeableperformancebutsuf-fersfromtrainingcostsnotbeingneglected.How-ever,werequireaparticularattentionmatrixtobuildtheauxiliarydistribution.Thissample-awarematrixisdifferentfromthevanillatransformerintermsofimplementation.Thevanillatransformerattentionisa2Dmatrix,butJANUSneedsa3Dmatrix.Consequently,itleadstoalotoftimecon-sumption,asshowninsection6.3.Inthefuture,wehopetospeedupmodeltrainingbyexploringabet-tersolutionorusinglower-levelCUDAoperatorstocreatethismatrix.AcknowledgementWewouldliketothanktheanonymousreviewersfortheirconstructivecomments.JuntaoLiisthecorrespondingauthor.ThisworkwassupportedbytheNationalScienceFoundationofChina(NSFCNo.62206194),theNaturalScienceFoundationofJiangsuProvince,China(GrantNo.BK20220488),andtheProjectFundedbythePriorityAcademicProgramDevelopmentofJiangsuHigherEduca-tionInstitutions.ReferencesTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneuralinformationprocessingsystems,33:1877â1901.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171â4186.WilliamFedus,BarretZoph,andNoamShazeer.2022.Switchtransformers:Scalingtotrillionparametermodelswithsimpleandefficientsparsity.JournalofMachineLearningResearch,23(120):1â39.MarjanGhazvininejad,OmerLevy,YinhanLiu,andLukeZettlemoyer.2019.Mask-predict:Parallelde-codingofconditionalmaskedlanguagemodels.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages6112â6121.JiataoGu,JamesBradbury,CaimingXiong,VictorOKLi,andRichardSocher.2018.Non-autoregressiveneuralmachinetranslation.InInternationalConfer-enceonLearningRepresentations.JunliangGuo,XuTan,LinliXu,TaoQin,EnhongChen,andTie-YanLiu.2020.Fine-tuningbycurriculumlearningfornon-autoregressiveneuralmachinetrans-lation.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages7839â7846.8059

YongchangHao,ShilinHe,WenxiangJiao,ZhaopengTu,MichaelLyu,andXingWang.2021.Multi-tasklearningwithsharedencoderfornon-autoregressivemachinetranslation.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages3989â3996.GeoffreyHinton,OriolVinyals,andJeffDean.2015.Distillingtheknowledgeinaneuralnetwork.arXivpreprintarXiv:1503.02531.XiaoShiHuang,FelipePerez,andMaksimsVolkovs.2021.Improvingnon-autoregressivetranslationmod-elswithoutdistillation.InInternationalConferenceonLearningRepresentations.MandarJoshi,DanqiChen,YinhanLiu,DanielSWeld,LukeZettlemoyer,andOmerLevy.2020.Spanbert:Improvingpre-trainingbyrepresentingandpredict-ingspans.TransactionsoftheAssociationforCom-putationalLinguistics,8:64â77.JungoKasai,JamesCross,MarjanGhazvininejad,andJiataoGu.2020.Non-autoregressivemachinetrans-lationwithdisentangledcontexttransformer.InIn-ternationalConferenceonMachineLearning,pages5144â5155.PMLR.DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.InternationalConferenceonLearningRepresentations.JasonLee,ElmanMansimov,andKyunghyunCho.2018.Deterministicnon-autoregressiveneuralse-quencemodelingbyiterativerefinement.InProceed-ingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1173â1182.MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMohamed,OmerLevy,VeselinStoyanov,andLukeZettlemoyer.2020.Bart:Denoisingsequence-to-sequencepre-trainingfornat-urallanguagegeneration,translation,andcomprehen-sion.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages7871â7880.XiaoboLiang,LijunWu,JuntaoLi,YueWang,QiMeng,TaoQin,WeiChen,MinZhang,Tie-YanLiu,etal.2021.R-drop:Regularizeddropoutforneuralnetworks.AdvancesinNeuralInformationProcessingSystems,34:10890â10905.JindËrichLibovick`yandJindËrichHelcl.2018.End-to-endnon-autoregressiveneuralmachinetranslationwithconnectionisttemporalclassification.InPro-ceedingsofthe2018ConferenceonEmpiricalMeth-odsinNaturalLanguageProcessing,pages3016â3021.DayihengLiu,YuYan,YeyunGong,WeizhenQi,HangZhang,JianJiao,WeizhuChen,JieFu,LinjunShou,MingGong,etal.2021.Glge:Anewgenerallan-guagegenerationevaluationbenchmark.InFind-ingsoftheAssociationforComputationalLinguis-tics:ACL-IJCNLP2021,pages408â420.XuezheMa,ChuntingZhou,XianLi,GrahamNeu-big,andEduardHovy.2019.Flowseq:Non-autoregressiveconditionalsequencegenerationwithgenerativeflow.InProceedingsofthe2019Confer-enceonEmpiricalMethodsinNaturalLanguagePro-cessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages4282â4292.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.Bleu:amethodforautomaticevalu-ationofmachinetranslation.InProceedingsofthe40thannualmeetingoftheAssociationforComputa-tionalLinguistics,pages311â318.WeizhenQi,YeyunGong,JianJiao,YuYan,WeizhuChen,DayihengLiu,KewenTang,HouqiangLi,JiushengChen,RuofeiZhang,etal.2021.Bang:Bridgingautoregressiveandnon-autoregressivegen-erationwithlargescalepretraining.InInternationalConferenceonMachineLearning,pages8630â8639.PMLR.WeizhenQi,YuYan,YeyunGong,DayihengLiu,NanDuan,JiushengChen,RuofeiZhang,andMingZhou.2020.Prophetnet:Predictingfuturen-gramforsequence-to-sequencepre-training.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages2401â2410.LihuaQian,HaoZhou,YuBao,MingxuanWang,LinQiu,WeinanZhang,YongYu,andLeiLi.2021.Glancingtransformerfornon-autoregressiveneuralmachinetranslation.InProceedingsofthe59thAn-nualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConfer-enceonNaturalLanguageProcessing(Volume1:LongPapers),pages1993â2003.ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJLiu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttrans-former.JournalofMachineLearningResearch,21:1â67.PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang.2016.Squad:100,000+questionsformachinecomprehensionoftext.InProceedingsofthe2016ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages2383â2392.AlexanderMRush,SumitChopra,andJasonWeston.2015.Aneuralattentionmodelforabstractivesen-tencesummarization.InProceedingsofthe2015ConferenceonEmpiricalMethodsinNaturalLan-guageProcessing,pages379â389.RicoSennrich,BarryHaddow,andAlexandraBirch.2016.Neuralmachinetranslationofrarewordswithsubwordunits.InProceedingsofthe54thAnnualMeetingoftheAssociationforComputationalLin-guistics(Volume1:LongPapers),pages1715â1725.8060

DinghanShen,MingzhiZheng,YelongShen,YanruQu,andWeizhuChen.2020.Asimplebuttough-to-beatdataaugmentationapproachfornaturallan-guageunderstandingandgeneration.arXivpreprintarXiv:2009.13818.DavidSo,QuocLe,andChenLiang.2019.Theevolvedtransformer.InInternationalConferenceonMachineLearning,pages5877â5886.PMLR.KaitaoSong,XuTan,TaoQin,JianfengLu,andTie-YanLiu.2020.Mpnet:Maskedandpermutedpre-trainingforlanguageunderstanding.AdvancesinNeuralInformationProcessingSystems,33:16857â16867.YixuanSu,DengCai,YanWang,DavidVandyke,Si-monBaker,PijiLi,andNigelCollier.2021.Non-autoregressivetextgenerationwithpre-trainedlan-guagemodels.InProceedingsofthe16thConfer-enceoftheEuropeanChapteroftheAssociationforComputationalLinguistics:MainVolume,pages234â243.ZhiqingSunandYimingYang.2020.Anemapproachtonon-autoregressiveconditionalsequencegenera-tion.InInternationalConferenceonMachineLearn-ing,pages9249â9258.PMLR.ChaoTian,YifeiWang,HaoCheng,YijiangLian,andZhihuaZhang.2020.Trainonce,anddecodeasyoulike.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages280â293.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.Advancesinneuralinformationprocessingsystems,30.AlexWang,KyunghyunCho,andCIFARAzrieliGlobalScholar.2019a.Berthasamouth,anditmustspeak:Bertasamarkovrandomfieldlanguagemodel.Pro-ceedingsofthe2019ConferenceoftheNorthAmer-icanChapteroftheAssociationforComputationalLinguistics,page30.DilinWang,ChengyueGong,andQiangLiu.2019b.Improvingneurallanguagemodelingviaadversarialtraining.InInternationalConferenceonMachineLearning,pages6555â6565.PMLR.MinghanWang,JiaxinGuo,YuxiaWang,DaimengWei,HengchaoShang,YingluLi,ChangSu,YimengChen,MinZhang,ShiminTao,etal.2022.Diformer:Directionaltransformerforneuralmachinetransla-tion.InProceedingsofthe23rdAnnualConferenceoftheEuropeanAssociationforMachineTranslation,pages81â90.BaosongYang,LongyueWang,DerekFWong,LidiaSChao,andZhaopengTu.2019a.Convolutionalself-attentionnetworks.InProceedingsofthe2019Con-ferenceoftheNorthAmericanChapteroftheAsso-ciationforComputationalLinguistics:HumanLan-guageTechnologies,Volume1(LongandShortPa-pers),pages4040â4045.ZhilinYang,ZihangDai,YimingYang,JaimeCar-bonell,RussRSalakhutdinov,andQuocVLe.2019b.Xlnet:Generalizedautoregressivepretrainingforlan-guageunderstanding.Advancesinneuralinforma-tionprocessingsystems,32.SaizhengZhang,EmilyDinan,JackUrbanek,ArthurSzlam,DouweKiela,andJasonWeston.2018a.Per-sonalizingdialogueagents:Ihaveadog,doyouhavepetstoo?InProceedingsofthe56thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages2204â2213.YingZhang,TaoXiang,TimothyMHospedales,andHuchuanLu.2018b.Deepmutuallearning.InPro-ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages4320â4328.ChuntingZhou,JiataoGu,andGrahamNeubig.2019.Understandingknowledgedistillationinnon-autoregressivemachinetranslation.InInternationalConferenceonLearningRepresentations.LongZhou,JiajunZhang,andChengqingZong.2020.Improvingautoregressivenmtwithnon-autoregressivemodel.InProceedingsoftheFirstWorkshoponAutomaticSimultaneousTranslation,pages24â29.