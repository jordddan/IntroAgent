Universal Conditional Masked Language Pre-training
for Neural Machine Translation

Pengfei Li1

Liangyou Li1

Meng Zhang1

Minghao Wu2

Qun Liu1

1Huawei Noah’s Ark Lab

2Monash University

{lipengfei111, liliangyou, zhangmeng92, qun.liu}@huawei.com
minghao.wu@monash.edu

2
2
0
2

n
u
J

2

]
L
C
.
s
c
[

3
v
0
1
2
9
0
.
3
0
2
2
:
v
i
X
r
a

Abstract

Pre-trained
sequence-to-sequence models
have signiﬁcantly improved Neural Machine
Translation (NMT). Different
from prior
works where pre-trained models usually
adopt an unidirectional decoder,
this paper
demonstrates that pre-training a sequence-
to-sequence model but with a bidirectional
decoder can produce notable performance
gains
for both Autoregressive and Non-
autoregressive NMT. Speciﬁcally, we propose
CeMAT, a conditional masked language
model pre-trained on large-scale bilingual
and monolingual corpora in many languages.1
We also introduce two simple but effective
methods to enhance the CeMAT, aligned
code-switching & masking and dynamic
dual-masking. We conduct extensive experi-
ments and show that our CeMAT can achieve
signiﬁcant performance
for
all scenarios from low- to extremely high-
resource languages, i.e., up to +14.4 BLEU on
low-resource and +7.9 BLEU on average for
Autoregressive NMT. For Non-autoregressive
NMT, we demonstrate it can also produce
consistent performance gains, i.e., up to +5.3
BLEU. To the best of our knowledge, this is
the ﬁrst work to pre-train a uniﬁed model for
ﬁne-tuning on both NMT tasks.

improvement

1

Introduction

Pre-trained language models have been widely
adopted in NLP tasks (Devlin et al., 2019; Radford
and Narasimhan, 2018). For example, XLM (Con-
neau and Lample, 2019) demonstrated that cross-
lingual pre-training is effective in improving neu-
ral machine translation (NMT), especially on low-
resource languages. These methods all directly pre-
train a bidirectional encoder or an unidirectional
decoder. The encoder and decoder in NMT models

1Code,
at

and pre-trained models

data,
avail-
https://github.com/huawei-noah/

able
Pretrained-Language-Model/tree/master/
CeMAT

are

Enc. Dec. Mono. Para.

Approach
•
mBERT (Devlin et al., 2019)
XLM (Conneau and Lample, 2019) •
MASS (Song et al., 2019)
mBART (Liu et al., 2020)
mRASP (Lin et al., 2020)
CeMAT (Ours)

•
•
• → •
• → •
• →
• ⇐⇒ •

•

•
•

Table 1: Comparison and summary of existing pre-
trained models for machine translation. Enc: encoder;
Dec: decoder; Mono: monolingual; Para: bilingual.
“•” denotes the corresponding model is pre-trained or
the corresponding data is used. “→” denotes the de-
coder of model is unidirectional, “⇐⇒” denotes the de-
coder is bidirectional.

are then independently initialized with them and
ﬁne-tuned (Guo et al., 2020; Zhu et al., 2020). Re-
cently, pre-training standard sequence-to-sequence
(Seq2Seq) models has shown signiﬁcant improve-
ments and become a popular paradigm for NMT
tasks (Song et al., 2019; Liu et al., 2020; Lin et al.,
2020).

However, some experimental results from XLM
(Conneau and Lample, 2019) have shown that the
decoder module initialized by the pre-trained bidi-
rectional masked language model (MLM) (Devlin
et al., 2019), rather than the unidirectional causal
language model (CLM, Radford and Narasimhan,
2018), would achieve better results on Autore-
gressive NMT (AT). Especially, compared to ran-
dom initialization, initialized by GPT (Radford and
Narasimhan, 2018) might result in performance
degradation sometimes. We conjecture that when
ﬁne-tuning on generation tasks (e.g., NMT), the
representation capability of the pre-trained models
may be more needed than the generation capability.
Therefore, during pre-training, we should focus on
training the representation capability not only for
the encoder, but also for the decoder more explic-
itly.

Inspired by that, we present CeMAT, a mul-
tilingual Conditional masked language prE-

 
 
 
 
 
 
Figure 1: The framework for CeMAT, which consists of an encoder and a bidirectional decoder. “Mono” denotes
monolingual, “Para” denotes bilingual. During the pre-training (left), the original monolingual and bilingual inputs
in many languages are augmented (the words are replaced with new words with same semantics or “[mask]”, please
see Figure 2 for more details) and fed into the model. Finally, we predict all the “[mask]” words on the source
side and target side respectively. For ﬁne-tuning (right), CeMAT provides uniﬁed initial parameter sets for AT and
NAT.

training model for MAchine Translation, which
consists of a bidirectional encoder, a bidirectional
decoder, and a cross-attention module for bridg-
ing them. Speciﬁcally, the model is jointly trained
by MLM on the encoder and Conditional MLM
(CMLM) on the decoder with large-scale monolin-
gual and bilingual texts in many languages. Table 1
compares our model with prior works. Beneﬁting
from the structure, CeMAT can provide uniﬁed
initialization parameters not only for AT task, but
also for Non-autoregressive NMT (NAT) directly.
NAT has been attracting more and more attention
because of its feature of parallel decoding, which
helps to greatly reduce the translation latency.

To better train the representation capability of
the model, the masking operations are applied in
two steps. First, some source words that have been
aligned with target words are randomly selected
and then substituted by new words of similar mean-
ings in other languages, and their corresponding tar-
get words are masked. We call this method aligned
code-switching & masking. Then, the remaining
words in both source and target languages will be
masked by dynamic dual-masking.

Extensive experiments on downstream AT and
NAT tasks show signiﬁcant gains over prior works.
Speciﬁcally, under low-resource conditions (< 1M
bitext pairs), our system gains up to +14.4 BLEU
points over baselines. Even for extremely high-

resource settings (> 25M), CeMAT still achieves
signiﬁcant improvements. In addition, experiments
on the WMT16 Romanian→English task demon-
strate that our system can be further improved
(+2.1 BLEU) by the Back-Translation (BT; Sen-
nrich et al., 2016a).

The main contributions of our work can be sum-

marized as follows:

• We propose a multilingual pre-trained model
CeMAT, which consists of a bidirectional en-
coder, a bidirectional decoder. The model is
pre-trained on both monolingual and bilingual
corpora and then used for initializing down-
stream AT and NAT tasks. To the best of our
knowledge, this is the ﬁrst work to pre-train a
uniﬁed model suitable for both AT and NAT.

• We introduce a two-step masking strategy to
enhance the model training under the setting
of bidirectional decoders. Based on a multi-
lingual translation dictionary and word align-
ment between source and target sentences,
aligned code-switching & masking is ﬁrstly
applied. Then, dynamic dual-masking is used.

• We carry out extensive experiments on AT
and NAT tasks with data of varied sizes. Con-
sistent improvements over strong competitors
demonstrate the effectiveness of CeMAT.

[en]Catsatonthemat[en]Wedanceonthegrass[de]WirtanzenaufdemgrasMono.Para.Original[en]Catsatonthemat[en][mask]satonthe[mask][en]Wedanse[mask]thegrass[de]Wir[mask]aufdem[mask]Mono.Para.Masked[en]Kedisatonthe[mask]EncoderSelf-AttentionFeed ForwardBidirectionalDecoderCross-AttentionSelf-AttentionFeed ForwardEncoderDecoder[en]whoareyou[de]WerbistduWerbistdu</s>Autoregressive NMTEncoderBidirectionalDecoder[en]whoareyou[de][mask]bist[mask]WerduNon-autoregressive NMTCatmatontanzengrasmatPre-trainingFine-tuningAligned code-switching & masking dynamic dual-maskingMono.Para.Figure 2: The details of our two-step masking. We ﬁrst obtain the aligned pair set Λ = {(“dance”,“tanzen”),...}
(marked with (cid:57)(cid:57)(cid:75)) from the original inputs by looking up the cross-lingual dictionary (denote as 1.Aligned), and
then randomly select a subset (marked as “dance”(cid:57)(cid:57)(cid:75)“tanzen” with red color) from it, in the lower left of the ﬁgure.
For each element in the subset, we select a new word by Fm(xi
m), and perform CSR to replace the source fragment
(“danse” marked as red color) and CSM for target (“[mask]” marked as red color) respectively. Finally, we do the
DM process to mask the contents of the source and target respectively (“[mask]” marked as light-blue color).

2 Pre-training Approach

Our CeMAT is jointly trained by MLM and CMLM
on the source side and the target side, respectively.
The overall framework is illustrated in Figure 1.
In this section, we ﬁrst introduce the multilingual
CMLM task (Section 2.1). Then, we describe
the two-step masking, including the aligned code-
switching & masking (Section 2.2) and the dynamic
dual-masking (Section 2.3). Finally, we present
training objectives of CeMAT (Section 2.4).

Formally, our training data consists of M
language-pairs D = {D1, D2, ..., DM }. Dk(m, n)
is a collection of sentence pairs in language Lm
and Ln, respectively. In the description below, we
denote a sentence pair as (Xm, Yn) ∈ Dk(m, n),
where Xm is the source text in the language Lm,
and Yn is the corresponding target text in the lan-
guage Ln. For monolingual corpora, we create
pseudo bilingual text by copying the sentence,
namely, Xm = Yn.

n

n

).

P (yj

. The probability of each yj

2.1 Conditional Masked Language Model
CMLM predicts masked tokens ymask
, given a
source sentence Xm and the remaining target sen-
tence Yn\ymask
n ∈
ymask
n

is independently calculated:
n|Xm, Yn\ymask
CMLM can be directly used to train a standard
Seq2Seq model with a bidirectional encoder, a uni-
directional decoder, and a cross attention. How-
ever, it is not restricted to the autoregressive feature
on the decoder side because of the independence
between masked words. Therefore, following prac-
tices of NAT, we use CMLM to pre-train a Seq2Seq
model with a bidirectional decoder, as shown in
Figure 1.

(1)

n

Although bilingual sentence pairs can be directly
used to train the model together with the conven-
tional CMLM (Ghazvininejad et al., 2019), it is
challenging for sentence pairs created from mono-
lingual corpora because of identical source and
target sentences. Therefore, we introduce a two-
step masking strategy to enhance model training
on both bilingual and monolingual corpora.

2.2 Aligned Code-Switching & Masking

We use aligned code-switching & masking strategy
to replace the source word or phrase with a new
word in another language, and then mask the corre-
sponding target word. Different from the previous
code-switching methods (Yang et al., 2020; Lin
et al., 2020) where source words always are ran-
domly selected and replaced directly, our method
consists of three steps:

m, yj

1. Aligning: We utilize a multilingual transla-
tion dictionary to get a set of aligned words
Λ = {· · · , (xi
n), · · · } between the source
Xm and target Yn. The word pair (xi
n) de-
notes that the i-th word in Xm and j-th word
in Yn are translations of each other. For sen-
tence pairs created from monolingual corpora,
words in an aligned word pair are identical.

m, yj

2. Code-Switching Replace (CSR): Given an
n) ∈ Λ, we ﬁrst se-
k in the language Lk that can
m in the source sentence

aligned word pair (xi
lect a new word ˆxi
be used to replace xi
Xm,

m, yj

k = Fm(xi
ˆxi

m)

where Fm(x) is a multilingual dictionary
lookup function for a word x in the language
Lm, ˆxi
k is a randomly selected word from the

[en]Wedanceonthegrass[de]WirtanzenaufdemgrasSpanish : danzaGerman : tanzenFrench  : danse  …dance2.𝐹𝐹𝑚𝑚(𝑥𝑥𝑚𝑚𝑖𝑖)[en]Wedanseonthegrass[de]Wir[mask]aufdemgrasCSRCSM[en]Wedanse[mask]thegrass[de]Wir[mask]aufdem[mask]DMDM1.Aligneddictionary, which is a translation of xi
language Lk.

m in the

m in the aligned pair (xi
k, we also mask yj

3. Code-Switching Masking (CSM): If the
m, yj
source word xi
n)
is replaced by ˆxi
n in Yn by
replacing it with a universal mask token. Then,
CeMAT will be trained to predict it in the out-
put layers of the bidirectional decoder.

For aligning and CSR, we only use available mul-
tilingual translation dictionary provided by MUSE
(Lample et al., 2018). Figure 2 shows the process
of aligned code-switching & masking. According
to the given dictionary, “dance” and “tanzen” are
aligned, then a new French word “danse” is se-
lected to replace “dance”, and “tanzen” replaced
by “[mask]” (marked as red color).

During training, at most 15% of the words in the
sentence will be performed by CSR and CSM. For
monolingual data, we set this ratio to 30%. We use

(CSR(Xm), CSM(Yn))

to denote the new sentence pair after aligned code-
switching & masking, which will be further dynam-
ically dual-masked at random.

2.3 Dynamic Dual-Masking

Limited by the dictionary, the ratio of aligned word
pairs is usually small. In fact, we can only match
aligned pairs for 6% of the tokens on average in
the bilingual corpora. To further increase the train-
ing efﬁciency, we perform dynamic dual-masking
(DM) on both bilingual and monolingual data.

• Bilingual data: We ﬁrst sample a masking
ratio υ from a uniform distribution between
[0.2, 0.5], then randomly select a subset of tar-
get words which are replaced by “[mask]”.
Similarly, we select a subset on the source
texts and mask them with a ratio of µ in a
range of [0.1, 0.2]. Figure 2 shows an exam-
ple of dynamic dual-masking on bilingual data.
We set υ ≥ µ to force the bidirectional de-
coder to obtain more information from the
encoder.

• Monolingual data: Since the source and target
are identical before masking, we sample υ =
µ from a range [0.3, 0.4] and mask the same
subset of words on both sides. This will avoid
the decoder directly copying the token from
the source.

Follow practices of pre-trained language models,
10% of the selected words for masking remain un-
changed, and 10% replaced with a random token.
Words replaced by the aligned code-switching &
masking will not be selected to prevent the loss of
cross-lingual information. We use

(DM(CSR(Xm)), DM(CSM(Yn)))

to denote the new sentence pair after dynamic dual-
masking, which will be used for pre-training.

2.4 Multilingual Pre-training Objectives

We jointly train the encoder and decoder on MLM
and CMLM tasks. Given the sentence pair

( ˆXm, ˆYn) = (DM(CSR(Xm)), DM(CSM(Yn)))

from the masked corpora ˆD, the ﬁnal training ob-
jective is formulated as follows:

L = −

(cid:88)

(cid:88)

λ

log P (yj

n| ˆXm, ˆYn)

( ˆXm, ˆYn)∈ ˆD

+(1 − λ)

yj
n∈ymask
n
(cid:88)

log P (xi

m| ˆXm)

m∈xmask
xi
m

n

(2)
where ymask
are the set of masked target words,
xmask
are the set of masked source words, and λ is
m
a hyper-parameter to balance the inﬂuence of both
tasks. In our experiments, we set λ = 0.7.

3 Pre-training Settings

Pre-training Data We use the English-centric
multilingual parallel corpora of PC322, and then
collect 21-language monolingual corpora from
common crawl3. In this paper, we use ISO lan-
guage code4 to identify each language. A “[lan-
guage code]” token will be prepended to the be-
ginning of the source and target sentence as shown
in Figure 2. This type of token helps the model
to distinguish sentences from different languages.
The detailed correspondence and summary of our
pre-training corpora can be seen in Appendix A.

Data pre-processing We directly learn a shared
BPE (Sennrich et al., 2016b) model on the entire
data sets after tokenization. We apply Moses to-
kenization (Sennrich et al., 2016b) for most lan-
guages, and for other languages, we use KyTea5

2https://github.com/linzehui/mRASP
3https://commoncrawl.org/
4https://www.loc.gov/standards/

iso639-2/php/code_list.php

5http://www.phontron.com/kytea/

Lang-Pairs
Source
Size
Direction
Direct
mBART
mRASP
CeMAT
∆

En-Kk
En-Tr
En-Et
WMT19
WMT17
WMT18
91k(low)
207k(low)
1.94M(medium)
←
→
←
→
→
0.8
0.2
12.2
9.5
17.9
7.4
2.5
22.5
17.8
21.4
23.4
12.3
8.3
20.0
20.9
22.2
23.6
23.9
12.9
8.8
+8.6 +12.1 +14.4 +11.4 +4.3

←
22.6
27.8
26.8
28.5
+5.9

En-Fi
WMT17
2.66M(medium)
→
20.2
22.4
24.0
25.4
+5.2

←
21.8
28.5
28.0
28.7
+6.9

En-Lv
WMT17
4.5M(medium)
→
12.9
15.9
21.6
22.0
+9.1

←
15.6
19.3
24.4
24.3
+8.7

En-Cs
WMT19
11M(high)
→
16.5
18.0
19.9
21.5
+5.0

En-De
WMT19
38M(extr-high)
→
30.9
30.5
35.2
39.2
+8.3

En-Fr
WMT14
41M(extr-high)
→
41.4
41.0
44.3
43.7
+2.3

Avg

17.1
21.2
23.8
25.0
+7.9

Table 2: Comprehensive comparison with mRASP and mBART. Best results are highlighted in bold. CeMAT out-
performs them on AT for all language pairs but two directions. Even for extremely high-resource scenarios(denoted
as “extr-high”), we observe gains of up to +8.3 BLEU on En→De language pair.

for Japanese and jieba6 for Chinese, and a spe-
cial normalization for Romanian (Sennrich et al.,
2016a). Following Liu et al. (2020), we balance the
vocabulary size of languages by up/down-sampling
text based on their data size when learning BPE.

Model and Settings As shown in Figure 1, we
apply a bidirectional decoder so that it can utilize
left and right contexts to predict each token. We use
a 6-layer encoder and 6-layer bidirectional decoder
with a model dimension of 1024 and 16 attention
heads. Following Vaswani et al. (2017), we use
sinusoidal positional embedding, and apply layer
normalization for word embedding and pre-norm
residual connection following Wang et al. (2019a).
Our model is trained on 32 Nvidia V100 GPUs
for 300K steps, The batch size on each GPU is 4096
tokens, and we set the value of update frequency
to 8. Following the training settings in Trans-
former, we use Adam optimizer ((cid:15) = 1e − 6, β1 =
0.9, β2 = 0.98) and polynomial decay scheduling
with a warm-up step of 10,000.

4 Autoregressive Neural Machine

Translation

In this section, we verify CeMAT provides consis-
tent performance gains in low to extremely high
resource scenarios. We also compare our method
with other existing pre-training methods and fur-
ther present analysis for better understanding the
contributions of each component.

4.1 Fine-Tuning Objective

The AT model consists of an encoder and a uni-
directional decoder. The encoder maps a source
sentence Xm into hidden representations which are
then fed into the decoder. The unidirectional de-
coder predicts the t-th token in a target language Ln
conditioned on Xm and the previous target tokens

6https://github.com/fxsjy/jieba

y<t
n . The training objective of AT is to minimize
the negative log-likelihood:

L(θ) =

(cid:88)

|Yn|
(cid:88)

(Xm,Yn)∈D(m,n)

t=1

− log P (yt

n|Xm, y<t

n ; θ)

(3)

4.2 Experimental Settings

Benchmarks We selected 9 different language
pairs and then use CeMAT to ﬁne-tune on them.
They are divided into four categories according
to their data size: low-resource (< 1M), medium-
resource (> 1M and < 10M), high-resource (>
10M and < 25M), and extremely high-resource (>
25M). See Appendix B for more details.

Conﬁguration We adopt a dropout rate of 0.1
for extremely high-resource En→Fr, En→De
(WMT19); for all other language pairs, we set the
value of 0.3. We ﬁne-tune AT with a maximum
learning rate of 5e − 4, a warm-up step of 4000
and label smoothing of 0.2. For inference, we use
beam search with a beam size of 5 for all transla-
tion directions. For a fair comparison with previous
works, all results are reported with case-sensitive
and tokenized BLEU scores.

4.3 Results and Analysis

Main Results We ﬁne-tune AT systems initial-
ized by our CeMAT on 8 popular language pairs,
which are the overlapping language pairs in exper-
iments of mBART (Liu et al., 2020) and mRASP
(Lin et al., 2020). Table 2 shows the results. Com-
pared to directly training AT models, our systems
with CeMAT as initialization obtain signiﬁcant im-
provements on all four scenarios. We observe gains
of up to +14.4 BLEU and over +11.4 BLEU on
three of the four tasks on low-resource scenarios,
i.e., En↔Tr. Without loss of generality, as the scale
of the dataset increases, the beneﬁts of pre-training

models are getting smaller and smaller. How-
ever, we can still obtain signiﬁcant gains when the
data size is large enough (extremely high-resource:
> 25M), i.e. +8.3 and +2.3 BLEU for En→De
and En→Fr respectively. This notable improve-
ment shows that our model can further enhance
extremely high-resource translation. Overall, we
obtain performance gains of more than +8.0 BLEU
for most directions, and ﬁnally observe gains of
+7.9 BLEU on average on all language pairs.

We further compare our CeMAT with mBART
(Liu et al., 2020) and mRASP (Lin et al., 2020),
which are two pre-training methods of current
SOTA. As illustrated in Table 2, CeMAT outper-
forms mBART on all language pairs with a large
margin (+3.8 BLEU on average), for extremely
high-resource, we can obtain signiﬁcant improve-
ments when mBART hurts the performance. Com-
pared to mRASP, we achieve better performance
on 11 out of the total 13 translation directions, and
outperforms this strong competitor with an average
improvement of +1.2 BLEU on all directions.

Comparison with Existing Pre-training Models
We further compare our CeMAT with more existing
multilingual pre-trained models on three popular
translation directions, including WMT14 En→De,
WMT16 En↔Ro. Results are shown in Table 3.
Our CeMAT obtains competitive results on these
languages pairs on average, and achieves the best
performance on En→Ro.

Our model also outperforms BT (Sennrich et al.,
2016a), which is a universal and stable approach
to augment bilingual with monolingual data. In
addition, when combining back-translation with
our CeMAT on Ro →En, we obtain a signiﬁcantly
improvement from 36.8 to 39.0 BLEU, as shown
in Table 3. This indicates that our method is com-
plementary to BT.

The Effectiveness of Aligned Code-Switching
and Masking We investigate the effectiveness of
aligned code-switching & masking as shown in Ta-
ble 4. We ﬁnd that utilizing aligned code-switching
& masking can help CeMAT improve the perfor-
mance for all different scenarios with gains of +0.5
BLEU on average, even though we can only match
the aligned word pairs for 6% of the tokens on av-
erage in the bilingual corpora. We presume the
method can be improved more signiﬁcantly if we
adopt more sophisticated word alignment methods.

The Effectiveness of Dynamic Masking
In the
pre-training phase, we use a dynamic strategy when
doing dual-masking on the encoder and decoder
respectively. We verify the effectiveness of this
dynamic masking strategy. As illustrated in Table 4
and Appendix C, we achieve signiﬁcant gains with
margins from +0.4 to +4.5 BLEU, when we ad-
justed the ratio of masking from a static value to
a dynamically and randomly selected value. The
average improvement on all language pairs is +2.1
BLEU. This suggests the importance of dynamic
masking.

Lang-Pairs En → De En → Ro Ro → En Ro → En
( +BT )
597K
Size
36.8
34.3
Direct
38.8
37.7
mBART
38.9
37.6
mRASP
39.1
–
MASS
38.5
–
XLM
–
–
mBERT
39.0
38.0
CeMAT

597K
34.0
37.8
36.9
–
35.6
–
37.1

4.5M
29.3
-
30.3
28.9
28.8
28.6
30.0

Table 3:
Comparison with recent multilingual
pre-training models on WMT14 En→De, WMT16
En↔Ro. We reach comparable results on all three di-
rections. When combining back-translation, we further
obtain gains of +2.2 BLEU on Ro→En.

5 Non-autoregressive Neural Machine

Translation

In this section, we will verify the performance of
our CeMAT on the NAT, which generates transla-
tions in parallel, on widely-used translation tasks.

5.1 Fine-Tuning Objective

As illustrated in Figure 1, NAT also adopts a
Seq2Seq framework, but consists of an encoder
and a bidirectional decoder which can be used to
predict the target sequences in parallel. The train-
ing objective of NAT is formulated as follows:

L(θ) =

(cid:88)

|Yn|
(cid:88)

(Xm,Yn)∈D(m,n)

t=1

− log P (yt

n|Xm; θ)

(4)

In this work, we follow Ghazvininejad et al.
(2019), which randomly sample some tokens ymask
for masking from target sentences and train the
model by predicting them given source sentences

n

Lang-Pairs
Direction
CeMAT
. w/o Aligned CS masking
. w/o Aligned CS masking & Dynamic

En-Tr

En-Kk

En-Et
→ ← → ← → ← → ← → ←
24.3
8.8
24.1
8.0
20.2
7.2

22.2
22.1
20.8

23.6
23.1
20.4

23.9
23.6
21.2

12.9
12.3
8.7

28.5
28.0
26.8

28.7
28.1
27.5

25.4
24.8
24.4

22.0
21.4
16.9

En-Lv

En-Fi

Avg

22.0
21.5
19.4

Table 4: Veriﬁcation of the effectiveness of different techniques. “. w/o Aligned CS masking” denotes that we
pre-train CeMAT without aligned code-switching & masking algorithm. “. w/o Aligned CS masking & Dynamic”
means that we further abandon the use of dynamic setting for dual-masking, where we only use a ﬁxed masking
ratio with 0.15 for the encoder and decoder. More details can be found in Appendix C. We can see two methods
are all critical components.

and remaining targets. The training objective is:

5.3 Main Results

L(θ) =

(cid:88)

(cid:88)

(Xm,Yn)∈D(m,n)
− log P (yj

n|Xm, Yn\ymask

n

(5)

; θ)

yj
n∈ymask
n

During decoding, given an input sequence to
translate, the initial decoder input is a sequence of
“[mask]” tokens. The ﬁne-tuned model generates
translations by iteratively predicting target tokens
and masking low-quality predictions. This process
can make the model re-predict the more challeng-
ing cases conditioned on previous high-conﬁdence
predictions.

5.2 Experimental Settings

NAT Benchmark Data We evaluate on three
popular datasets: WMT14 En↔De, WMT16
En↔Ro and IWSLT14 En↔De. For a fair com-
parison with baselines, we only use the bilingual
PC32 corpora to pre-train our CeMAT. We only use
knowledge distillation (Gu et al., 2018) on WMT14
En↔De tasks.

Baselines We use our CeMAT for initialization
and ﬁne-tune a Mask-Predict model (Ghazvinine-
jad et al., 2019) as in Section 4. To better quantify
the effects of the proposed pre-training models, we
build two strong baselines.

Direct. We directly train a Mask-Predict model

with randomly initialized parameters.

mRASP. To verify that our pre-trained model
is more suitable for NAT, we use a recently pre-
trained model mRASP (Lin et al., 2020) to ﬁne-
tune on downstream language pairs.

Conﬁguration We use almost the same conﬁg-
uration as the pre-training and AT except the fol-
lowing differences. We use learned positional em-
beddings (Ghazvininejad et al., 2019) and set the
max-positions to 10,000.

The main results on three language pairs are pre-
sented in Table 5. When using CeMAT to initialize
the Mask-Predict model, we observe signiﬁcant
improvements (from +0.9 to +5.3 BLEU) on all
different tasks, and ﬁnally obtain gains of +2.5
BLEU on average. We also achieve higher results
than the AT model on both En→De (+2.8 BLEU)
and De→En (+0.9 BLEU) directions on IWSLT14
datasets, which is the extremely low-resource sce-
narios where training from scratch is harder and
pre-training is more effective.

As illustrated in Table 5, on all different tasks,
CeMAT outperforms mRASP with a signiﬁcant
margin. On average, we obtain gains of +1.4 BLEU
over mRASP. Especially under low-resource set-
tings on IWSLT14 De→En, we achieve a large
gains of +3.4 BLEU over mRASP. Overall, mRASP
shows limited improvement (+0.4 to +1.9 BLEU)
compared to CeMAT. This also suggests that al-
though we can use the traditional pre-training
method to ﬁne-tune the NAT task, it does not bring
a signiﬁcant improvement like the AT task because
of the gap between pre-training and ﬁne-tuning
tasks.

We further compare the dynamic performance
on three language pairs during iterative decoding,
as shown in Appendix D. We only need 3 to 6
iterations to achieve the best score. During the it-
eration, we always maintain rapid improvements.
In contrast, mRASP obtains the best result after 6
to 9 iterations. We also observe a phenomenon that
the performance during iterations is also unstable
on both mRASP and Mask-Predict, but CeMAT
appears more stable. We conjecture that our pre-
trained model can learn more related information
between words in both the same and different lan-
guages. This ability alleviated the drawback of
NAT assumptions: the individual token predictions
are conditionally independent of each other.

Source
Lang-Pairs
Transformer (Vaswani et al., 2017)
Mask-Predict (Ghazvininejad et al., 2019)
mRASP (Lin et al., 2020)
CeMAT (Ours)

IWSLT14

WMT16
En→De De→En En→Ro Ro→En En→De De→En

WMT14

23.9
22.0
23.9
26.7

32.8
28.4
30.3
33.7

34.1
31.5
32.2
33.3

34.5
31.7
32.1
33.0

28.0
26.1
26.7
27.2

32.7
29.0
29.8
29.9

Avg

31.0
28.1
29.2
30.6

Table 5: Comprehensive comparison with two strong baselines. “mRASP” denotes using mRASP to initialize
Mask-Predict, “CeMAT (Ours)” denotes using our CeMAT to initialize. We obtain consistent and signiﬁcant
improvements on all language pairs, outperforming AT on IWSLT14 tasks. Best non-autoregressive results are
highlighted in bold.

6 Related Work

Multilingual Pre-training Task Conneau and
Lample (2019) and Devlin et al. (2019) proposed to
pre-train a cross-lingual language model on multi
language corpora, then the encoder or decoder of
model are initialized independently for ﬁne-tuning.
Song et al. (2019), Yang et al. (2020) and Lewis
et al. (2020) directly pre-trained a Seq2Seq model
by reconstructing part or all of inputs and achieve
signiﬁcant performance gains. Recently, mRASP
(Lin et al., 2020) and CSP (Yang et al., 2020) apply
the code-switching technology to simply perform
random substitution on the source side. Another
similar work, DICT-MLM (Chaudhary et al., 2020)
introduce multilingual dictionary, pre-training the
MLM by mask the words and then predict its cross-
lingual synonyms. mRASP2 (Pan et al., 2021) also
used code-switching on monolingual and bilingual
data to improve the effectiveness, but it is essen-
tially a multilingual AT model.

Compared to previous works: 1) CeMAT is the
ﬁrst pre-trained Seq2Seq model with a bidirectional
decoder; 2) We introduce aligned code-switching &
masking, different from traditional code-switching,
we have two additional steps: align between source
and target, and CSM; 3) We also introduce a dy-
namic dual-masking method.

Autoregressive Neural Machine Translation
Our work is also related to AT, which adopts an
encoder-decoder framework to train the model
(Sutskever et al., 2014). To improve the perfor-
mance, back-translation, forward-translation and
related techniques were proposed to utilize the
monolingual corpora (Sennrich et al., 2016a; Zhang
and Zong, 2016; Edunov et al., 2018; Hoang et al.,
2018). Prior works also attempted to jointly train a
single multilingual translation model that translates
multi-language directions at the same time (Firat
et al., 2016; Johnson et al., 2017; Aharoni et al.,

2019; Wu et al., 2021). In this work, we focus on
pre-training a multilingual language model, which
can provide initialization parameters for the lan-
guage pairs. On the other hand, our method can use
other languages to further improve high-resource
tasks.

(2018) ﬁrst

Non-autoregressive Neural Machine Trans-
lation Gu et al.
introduced a
transformer-based method to predict the complete
target sequence in parallel.
In order to reduce
the gap with the AT model, Lee et al. (2018) and
Ghazvininejad et al. (2019) proposed to decode the
target sentence with iterative reﬁnement. Wang
et al. (2019b) and Sun et al. (2019) utilized aux-
iliary information to enhance the performance of
NAT. One work related to us is Guo et al. (2020),
which using BERT to initialize the NAT. In this
work, CeMAT is the ﬁrst attempt to pre-train a mul-
tilingual Seq2Seq language model on NAT task.

7 Conclusion

In this paper, we demonstrate that multilingually
pre-training a sequence-to-sequence model but
with a bidirectional decoder produces signiﬁcant
performance gains for both Autoregressive and
Non-autoregressive Neural Machine Translation.
Beneﬁting from conditional masking, the decoder
module, especially the cross-attention can learn the
word representation and cross-lingual representa-
tion ability more easily. We further introduce the
aligned code-switching & masking to align the rep-
resentation space for words with similar semantics
but in different languages, then we use a dynamic
dual-masking strategy to induce the bidirectional
decoder to actively obtain the information from the
source side. Finally, we veriﬁed the effectiveness
of these two methods. In the future, we will inves-
tigate more effective word alignment method for
aligned code-switching & masking.

8 Acknowledgments

We would like to thank anonymous reviewers for
their helpful feedback. we also thank Wenyong
Huang, Lu Hou, Yinpeng Guo, Guchun Zhang for
their useful suggestion and help with experiments.

References

Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.
Massively multilingual neural machine translation.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-
7, 2019, Volume 1 (Long and Short Papers), pages
3874–3884. Association for Computational Linguis-
tics.

Aditi Chaudhary, Karthik Raman, Krishna Srinivasan,
and Jiecao Chen. 2020. DICT-MLM: improved
multilingual pre-training using bilingual dictionar-
ies. CoRR, abs/2010.12566.

Alexis Conneau and Guillaume Lample. 2019. Cross-
In Advances
lingual language model pretraining.
in Neural Information Processing Systems 32: An-
nual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, pages 7057–7067.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers), pages 4171–4186. Association for Computa-
tional Linguistics.

Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding back-translation at
In Proceedings of the 2018 Conference on
scale.
Empirical Methods in Natural Language Processing,
Brussels, Belgium, October 31 - November 4, 2018,
pages 489–500. Association for Computational Lin-
guistics.

Orhan Firat, KyungHyun Cho, and Yoshua Bengio.
2016. Multi-way, multilingual neural machine trans-
lation with a shared attention mechanism. CoRR,
abs/1601.01073.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel
decoding of conditional masked language models.
In Proceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019, pages 6111–
6120. Association for Computational Linguistics.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O. K. Li, and Richard Socher. 2018. Non-
In 6th
autoregressive neural machine translation.
International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings. Open-
Review.net.

Junliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei,
Incorpo-
Boxing Chen, and Enhong Chen. 2020.
rating BERT into parallel sequence decoding with
In Advances in Neural Information Pro-
adapters.
cessing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual.

Cong Duy Vu Hoang, Philipp Koehn, Gholamreza
Iterative back-
Haffari, and Trevor Cohn. 2018.
In Pro-
translation for neural machine translation.
ceedings of the 2nd Workshop on Neural Machine
Translation and Generation, NMT@ACL 2018, Mel-
bourne, Australia, July 20, 2018, pages 18–24. As-
sociation for Computational Linguistics.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Viégas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. 2017.
Google’s multilingual neural machine translation
system: Enabling zero-shot translation. Trans. As-
soc. Comput. Linguistics, 5:339–351.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018. Unsupervised ma-
chine translation using monolingual corpora only.
In 6th International Conference on Learning Rep-
resentations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceed-
ings. OpenReview.net.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
In Pro-
quence modeling by iterative reﬁnement.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, Brussels, Bel-
gium, October 31 - November 4, 2018, pages 1173–
1182. Association for Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020,
pages 7871–7880. Association for Computational
Linguistics.

Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,
Jiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-
training multilingual neural machine translation by
In Proceedings
leveraging alignment information.
of the 2020 Conference on Empirical Methods in

Natural Language Processing, EMNLP 2020, On-
line, November 16-20, 2020, pages 2649–2663. As-
sociation for Computational Linguistics.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
pre-training for neural machine translation. Trans.
Assoc. Comput. Linguistics, 8:726–742.

Xiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li.
2021. Contrastive learning for many-to-many mul-
In Proceed-
tilingual neural machine translation.
ings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Pro-
cessing, ACL/IJCNLP 2021, (Volume 1: Long Pa-
pers), Virtual Event, August 1-6, 2021, pages 244–
258. Association for Computational Linguistics.

Alec Radford and Karthik Narasimhan. 2018.

Im-
proving language understanding by generative pre-
training.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany, Volume 1: Long Papers. The Asso-
ciation for Computer Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2016, August 7-12, 2016, Berlin,
Germany, Volume 1: Long Papers. The Association
for Computer Linguistics.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. MASS: masked sequence to se-
quence pre-training for language generation. In Pro-
ceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, volume 97 of Pro-
ceedings of Machine Learning Research, pages
5926–5936. PMLR.

Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He,
Zi Lin, and Zhi-Hong Deng. 2019. Fast structured
decoding for sequence models. In Advances in Neu-
ral Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver, BC, Canada, pages 3011–3020.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems 27: Annual Conference on Neural Informa-
tion Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998–6008.

Qiang Wang, Bei Li, Tong Xiao,

Jingbo Zhu,
Changliang Li, Derek F. Wong, and Lidia S. Chao.
2019a. Learning deep transformer models for ma-
chine translation. In Proceedings of the 57th Confer-
ence of the Association for Computational Linguis-
tics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers, pages 1810–1822. As-
sociation for Computational Linguistics.

Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang
Zhai, and Tie-Yan Liu. 2019b. Non-autoregressive
machine translation with auxiliary regularization. In
The Thirty-Third AAAI Conference on Artiﬁcial In-
telligence, AAAI 2019, The Thirty-First Innovative
Applications of Artiﬁcial Intelligence Conference,
IAAI 2019, The Ninth AAAI Symposium on Edu-
cational Advances in Artiﬁcial Intelligence, EAAI
2019, Honolulu, Hawaii, USA, January 27 - Febru-
ary 1, 2019, pages 5377–5384. AAAI Press.

Minghao Wu, Yitong Li, Meng Zhang, Liangyou
Li, Gholamreza Haffari, and Qun Liu. 2021.
Uncertainty-aware balancing for multilingual and
multi-domain neural machine translation training.
In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2021, Virtual Event / Punta Cana, Dominican Re-
public, 7-11 November, 2021, pages 7291–7305. As-
sociation for Computational Linguistics.

Zhen Yang, Bojie Hu, Ambyera Han, Shen Huang,
and Qi Ju. 2020. CSP: code-switching pre-training
In Proceedings of
for neural machine translation.
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, pages 2624–2636. Associ-
ation for Computational Linguistics.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2016, Austin, Texas, USA, November
1-4, 2016, pages 1535–1545. The Association for
Computational Linguistics.

Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,
Wengang Zhou, Houqiang Li, and Tie-Yan Liu.
Incorporating BERT into neural machine
2020.
In 8th International Conference on
translation.
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net.

A Statistics of the Pre-Training Data.

We present dataset statistics for pre-training cor-
pora in Table 6.

B Statics of Five Different Scenarios

We present dataset statistics for ﬁne-tuning corpora
in Table 7.

C Detailed Ablation Experiments

We show more detailed results of the ablation ex-
periments on two language pairs in Table 8.

D Performance with Iterations for NAT

We present the dynamic performance on three
language-pair datasets during iterative decoding
in Figure 3, 4, 5, 6, 7 and 8.

Figure 5: The performance of WMT16 En2Ro when
decoding with different number of iterations.

Figure 3: The performance of IWSLT14 En2De when
decoding with different number of iterations.

Figure 6: The performance of WMT16 Ro2En when
decoding with different number of iterations.

Figure 4: The performance of IWSLT14 De2En when
decoding with different number of iterations

Figure 7: The performance of WMT14 De2En when
decoding with different number of iterations.

345678910Iterations192021222324252627Fine-tuning BLEUIWSLT14 En2Demask-predict+mRASP+CeMAT345678910Iterations242628303234Fine-tuning BLEUIWSLT14 De2Enmask-predict+mRASP+CeMAT345678910Iterations27282930313233Fine-tuning BLEUWMT16 En2Romask-predict+mRASP+CeMAT345678910Iterations282930313233Fine-tuning BLEUWMT16 Ro2Enmask-predict+mRASP+CeMAT345678910Iterations26.026.527.027.528.028.529.029.530.0Fine-tuning BLEUWMT14 De2Enmask-predict+mRASP+CeMATFigure 8: The performance of WMT14 En2De when decoding with different number of iterations.

ISO Language

Bilingual Monolingual

ISO Language

Bilingual Monolingual

Gujarati
Belarusian

Gu
Be
My Burmese
Mn Mongolian
Af
Afrikaans
Esperanto
Eo
Kazakh
Kk
Sr
Serbian
Mt Maltese
Kannada
Ka
Hebrew
He
Turkish
Tr
Romanian
Ro
Czech
Cs
Arabic
Ar
Greek
El
Hindi
Hi

11K
24K
28K
28K
40K
66K
122K
133K
174K
198K
330K
383K
770K
814K
1.2M
1.3M
1.3M

815K
–
–
–
–
–
1.8M
3.7M
–
–
–
9.9M
20M
9.9M
–
8.3M
9.9M

Ko
Korean
Ms Malay
Ru
Fi
Ja
It
Es
Et
Lt
Lv
Bg
Vi
De
Zh
Fr
En

Russian
Finnish
Japanese
Italian
Spanish
Estonian
Lithuanian
Latvian
Bulgarian
Vietnamese
German
Chinese
French
English

1.4M
1.6M
1.8M
2M
2M
2M
2.1M
2.2M
2.3M
3.0M
3.1M
3.1M
4.6M
21M
36M
–

–
–
9.9M
9.9M
3.4M
9.9M
9.9M
5.3M
2.8M
11.3M
9.9M
–
15M
4.4M
15M
15M

Table 6: A list of 32 Enlish-centric language pair datasets. Among them, 21 languages have corresponding mono-
lingual data. In this work, we using the ISO code represent the language name, and put them at the beginning of
the source and target.

345678910Iterations24.024.525.025.526.026.527.0Fine-tuning BLEUWMT14 En2Demask-predict+mRASP+CeMATLang-Pairs Source

Size

Category

En-Kk
De-En
En-Tr
En-Ro
En-Et
En-Fi
En-Lv
En-De
En-Cs
En-De
En-Fr

WMT19
IWSLT14
WMT17
WMT16
WMT18
WMT17
WMT17
WMT14
WMT19
WMT19
WMT14

low-resource
low-resource
low-resource
low-resource
medium-resource
medium-resource
medium-resource
medium-resource
high-resource

97K
159K
207K
597K
1.9M
2.7M
4.5M
4.5M
11M
38M extremely high-resource
41M extremely high-resource

Table 7: The statistical information of the language pairs on low- / medium- / high- / extremely high-resource for
the machine translation task.

Lang-Pairs
Direction
w/ Bilingual
w/ Monolingual
w/ Bi- & Monolingual
w/o Aligned CS masking
w/o Dynamic (masking:0.15)
w/o Dynamic (masking:0.35)

Et-En

Kk-En
→ ← → ←
19.1
7.8
18.9
5.4
19.0
9.0
18.2
8.4
17.7
7.3
18.1
8.8

24.4
23.5
25.2
24.3
23.5
23.7

5.5
5.4
5.6
5.1
4.4
5.6

Avg

14.2
13.3
14.7
14.0
13.2
14.1

Table 8: Veriﬁcation of the effectiveness of different techniques on two language pairs: Kk-En and Et-En. “w/
Bilingual” denotes that we use only bilingual data when pre-training CeMAT; “w/ Monolingual” denotes that
we use only monolingual data when pre-training CeMAT; “w Bi- & Monolingual” denotes that when pre-training
CeMAT, we use both bilingual and monolingual data; “w/o Aligned CS masking” denotes that we pre-train CeMAT
without aligned code-switching & masking algorithm; “w/o Dynamic (masking:0.15)” means that we use a ﬁxed
masking ratio with 0.15 for dual-masking; “w/o Dynamic (masking:0.35)” means that we use a ﬁxed masking
ratio with 0.35 for dual-masking to make a more fair comparison with dynamic masking. To save computational
resources, we use Transformer-base to obtain all the results of this experiment.

