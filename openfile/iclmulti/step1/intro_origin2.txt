Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) language modeling, which estimates the probability distribution of the text corpus, is widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning. However, AR-based models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction. This issue stems from the fact that local correlations such as bigram combination are usually stronger than long-term dependencies, and teacher forcing, where the model focuses on one-step-ahead prediction for each time step, has no explicit bias toward future token planning and modeling. As a result, the model may learn a bias for language modeling, which leads to overfitting on strong local correlations and underfitting on global coherence and long-term dependency.

To address this issue, this paper proposes BANG, a new large-scale pretraining model designed for Non-autoregressive (NAR) and semi-NAR generation, which bridges the gap between Autoregressive (AR) and NAR via pretraining a generative model. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

The proposed BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. The experiments on question generation (SQuAD 1.1), summarization (XSum), and dialogue generation (PersonaChat) show that BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong NAR baselines.

Related works:

Previous works have proposed various approaches to address the issue of AR-based models. XLNet proposed a new simple network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The Transformer, another approach, is based on attention mechanisms and connects the encoder and decoder through an attention mechanism. ProphetNet introduced a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. CMLMC proposed the Conditional Masked Language Model with Correction that addresses the problems of the indistinguishability of tokens and mismatch between training and inference. CeMAT proposed a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages and demonstrated that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. 

In summary, this paper proposes BANG, a new large-scale pretraining model designed for Non-autoregressive (NAR) and semi-NAR generation, which bridges the gap between Autoregressive (AR) and NAR via pretraining a generative model. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. The proposed BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.