Introduction:

Pre-training language models have become a popular approach to improve the performance of downstream natural language processing tasks. However, existing pre-training models, such as BERT and XLNet, are based on autoregressive (AR) generation models, which have limitations in capturing long-term dependencies and suffer from slow decoding speed. To address these limitations, we propose BANG, a large-scale pre-training model designed for non-autoregressive (NAR) and semi-NAR generation, which bridges the gap between AR and NAR via pretraining a generative model.

BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. This approach enables BANG to support NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

To demonstrate the effectiveness of BANG, we conduct experiments on various tasks for NAR and semi-NAR finetuning, and for AR finetuning. The results show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning. For AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.

Previous works have attempted to address the limitations of AR models by introducing non-autoregressive (NAR) models, such as the iterative generative paradigm like CMLM and the two-stream self-attention proposed in XLNet. However, these approaches only consider the relevance of model parameters, ignoring the correlations between the two manners. In contrast, BANG bridges the gap between AR and NAR by designing a novel model structure for large-scale pretraining, which can support both AR and NAR generation.

Other related works have proposed new pre-training models, such as ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism, and MPNet, which leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. CeMAT is another pre-trained sequence-to-sequence model that pre-trains a sequence-to-sequence model but with a bidirectional decoder, which can produce notable performance gains for both Autoregressive and Non-autoregressive NMT.

In summary, BANG is the first large-scale pretraining model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pretraining a generative model. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. The experiments show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.