Introduction:

Sequence generation tasks have been widely studied in natural language processing, and autoregressive (AR) and non-autoregressive (NAR) models have been the two most successful pretraining objectives. AR models generate tokens sequentially, while NAR models generate tokens in parallel, which reduces generation latency. However, NAR models generally come with a decrease in accuracy compared to AR models. In this paper, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. Specifically, JANUS maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach overcomes the limitations of both AR and NAR models and improves performance.

To demonstrate the effectiveness of JANUS, we conduct experiments on multiple neural machine translation (NMT) datasets and autoregressive pretraining models. The results show that JANUS achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average. Moreover, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Related works have explored different pretraining objectives and architectures for sequence generation tasks. Autoregressive language modeling and autoencoding have been the two most successful pretraining objectives. BERT, a popular autoencoding-based pretraining model, has achieved better performance than pretraining approaches based on autoregressive language modeling. However, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. XLNet, a generalized autoregressive pretraining method, enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. BANG, a new pretraining model, bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining. Attention mechanisms have also been widely used in sequence generation tasks, and the Transformer architecture has achieved state-of-the-art results in machine translation tasks.

In summary, JANUS is a novel method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance. The auxiliary distribution P AUX bridges the discrepancy between P AR and P NAR, and the experiments show that JANUS achieves state-of-the-art results on multiple NMT datasets and autoregressive pretraining models. The related works have explored different pretraining objectives and architectures for sequence generation tasks, and JANUS builds upon these works to propose a new method that overcomes the limitations of both AR and NAR models.