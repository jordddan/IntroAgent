Introduction:

Pre-training language models have significantly improved the performance of NLP tasks in recent years. However, existing pre-training methods such as BERT and XLNet have limitations that affect their performance on downstream tasks. In this paper, we propose a new pre-training method called Conditional Masked Language Model with Correction (CMLMC) that addresses the shortcomings of the Conditional Masked Language Model (CMLM) in non-autoregressive (NAR) machine translation.

Previous works have shown that autoregressive (AR) models achieve better performance than non-autoregressive (NAR) models in sequence generation tasks. However, NAR models bring fast decoding speed for inference. To bridge the gap between AR and NAR generation, JANUS [0] proposes a joint autoregressive and non-autoregressive training method. BANG [2] introduces a new pre-training model that bridges AR and NAR generation by designing a novel model structure for large-scale pre-training. ProphetNet [4] presents a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet [5] leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. CeMAT [7] proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages.

To improve the performance of NAR machine translation, we propose CMLMC, which modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. Our proposed method achieves new state-of-the-art undistilled NAR results and approaches autoregressive (AR) performance on multiple NMT benchmarks.

In the next section, we will provide context and background information on the limitations of existing pre-training language models, such as BERT and XLNet, and how they affect downstream tasks. We will also clearly state the problem being addressed, which is the limitations of existing pre-training language models, and the specific research questions or objectives, which is to propose a new pre-training method called CMLMC that addresses these limitations and improves downstream task performance. We will explain the proposed CMLMC method in detail, including its novel techniques or algorithms used to enhance the model training, and the dataset used, with a clear explanation of the methodology used. We will provide a comprehensive overview of related works, highlighting the main differences from CMLMC and a comparison with other state-of-the-art methods in the field, and how CMLMC addresses the limitations of these methods. Finally, we will present a summary of the experimental results and main findings, with detailed analysis and interpretation, and clearly state the limitations of the proposed approach and provide insights into future research directions.