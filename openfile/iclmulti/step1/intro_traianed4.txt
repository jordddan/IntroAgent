Paragraph 1: General Introduction
Pre-training large-scale neural language models has become a popular approach to improve the performance of downstream natural language processing tasks. However, existing pre-training methods such as BERT and XLNet have limitations that affect their performance on certain tasks. In this paper, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. 

Paragraph 2: Background and Limitations of Existing Pre-training Methods
BERT and XLNet are two popular pre-training methods that have achieved state-of-the-art results on various natural language processing tasks. However, BERT relies on corrupting the input with masks, which neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. On the other hand, XLNet addresses the limitations of BERT by enabling learning bidirectional contexts and overcoming the limitations of BERT's autoregressive formulation. However, XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. 

Paragraph 3: Problem Statement and Research Objectives
The limitations of existing pre-training methods motivate us to propose a new pre-training method called ProphetNet. The main objective of this paper is to develop a method that can pre-train a large-scale Seq2Seq model with a novel self-supervised objective future n-gram prediction. The proposed method should overcome the limitations of existing pre-training methods and achieve state-of-the-art results on various natural language processing tasks.

Paragraph 4: Proposed Method
ProphetNet is pre-trained using a novel self-supervised objective future n-gram prediction. Unlike traditional sequence-to-sequence models that optimize one-step-ahead prediction, ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. ProphetNet also extends the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

Paragraph 5: Related Works
Several related works have been proposed to address the limitations of existing pre-training methods. JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously and effectively alleviates the problem of distribution discrepancy. BANG bridges the gap between autoregressive and non-autoregressive generation by designing a novel model structure for large-scale pretraining. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between autoregressive and non-autoregressive approaches and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Universal Conditional Masked Language Pre-training demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. 

Paragraph 6: Experimental Results and Main Findings
ProphetNet is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Fine-tuning ProphetNet on several NLG tasks achieves the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. The experimental results demonstrate that ProphetNet outperforms existing pre-training methods and achieves state-of-the-art results on various natural language processing tasks. 

Paragraph 7: Conclusion and Future Work
In this paper, we propose a new pre-training method called ProphetNet with a novel self-supervised objective future n-gram prediction. The proposed method overcomes the limitations of existing pre-training methods and achieves state-of-the-art results on various natural language processing tasks. Future work includes exploring the potential of ProphetNet on other natural language processing tasks and investigating ways to further improve the performance of ProphetNet.