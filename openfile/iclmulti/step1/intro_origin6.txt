Introduction:

Pre-training language models have significantly improved the performance of neural machine translation (NMT) in recent years. However, the autoregressive (AR) framework of NMT models, which translates one token at a time, can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed, which translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts and only become competitive when trained with distillation. 

In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC), which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. Specifically, we modify the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 

Our proposed CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. To further improve the performance of NAR models, recent work has been exploring bridging the gap between AR and NAR generation. JANUS [0] proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. BANG [2] bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. 

In addition, we also draw inspiration from recent successful pre-training models, such as XLNet [1], which introduces permuted language modeling (PLM) for pre-training to capture the dependency among the predicted tokens. However, PLM has its own limitation: each token can only see its preceding tokens in a permuted sequence but does not know the position information of the full sentence during the autoregressive pre-training, which brings discrepancy between pre-training and fine-tuning. MPNet [5] leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. 

In this paper, we propose CMLMC, which combines the advantages of both AR and NAR models and addresses their limitations. Our proposed model achieves new state-of-the-art results on multiple NMT benchmarks and demonstrates the effectiveness of our proposed correction loss. The rest of the paper is organized as follows. Section 2 describes the related work in detail. Section 3 presents the proposed CMLMC model. Section 4 describes the experimental setup and results. Finally, Section 5 concludes the paper and discusses future work.