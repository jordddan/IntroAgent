Introduction:

Pre-training language models have become a popular approach to improve the performance of downstream natural language processing tasks. Autoregressive (AR) language modeling and non-autoregressive (NAR) generation models are two of the most successful pre-training objectives. AR models estimate the probability distribution of a text corpus with an autoregressive model, while NAR models generate tokens in parallel, reducing generation latency. However, AR models are limited in modeling deep bidirectional contexts, while NAR models suffer from lower accuracy. To address these limitations, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses.

JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, allowing AR and NAR models to learn from each other. Specifically, JANUS maximizes the performance of both AR and NAR models simultaneously, enhancing the model performance in both AR and NAR manner. We demonstrate the effectiveness of JANUS on multiple neural machine translation (NMT) datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average.

To achieve these results, we propose a novel pre-training method that combines the strengths of AR and NAR models. We demonstrate that JANUS outperforms the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Related works in the field of pre-training language models include XLNet, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. XLNet also overcomes the limitations of BERT, a pretraining approach based on autoencoding, by using an autoregressive formulation. BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In contrast, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin.

Another related work is BANG, a pretraining model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining. BANG supports AR, NAR, and semi-NAR generation to meet different requirements. Experiments on question generation, summarization, and dialogue generation show that BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models.

In summary, JANUS is a novel pre-training method that combines the strengths of AR and NAR models while avoiding their weaknesses. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Our proposed method outperforms the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.