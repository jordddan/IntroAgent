INTRODUCTION

Pre-training models on large-scale text corpora has become a popular approach to improve the performance of various natural language processing (NLP) tasks. Among the pre-training objectives, Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have been the two most successful approaches. MLM aims to estimate the probability distribution of a text corpus with an autoregressive model, while PLM maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order. However, both MLM and PLM have their limitations, and a unified pre-training method that combines the advantages of both approaches is still lacking.

In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations. MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. 

We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting. 

Related works have explored various pre-training methods, including JANUS, which is a Joint Autoregressive and Non-autoregressive training method that uses an Auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. BANG is a new pre-training model that bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation by designing a novel model structure for large-scale pre-training. ProphetNet is a sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Universal Conditional Masked Language Pre-training (CeMAT) is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages, which can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages.

In summary, our proposed MPNet pre-training method unifies the advantages of MLM and PLM while addressing their limitations, and achieves state-of-the-art performance on various downstream benchmark tasks. The related works have explored various pre-training methods, but our proposed MPNet method provides a novel approach that outperforms previous well-known models.