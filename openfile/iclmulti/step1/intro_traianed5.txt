INTRODUCTION

Pre-training language models have become a crucial component in natural language processing, achieving state-of-the-art performance on various downstream tasks. However, existing pre-training methods such as Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have their limitations. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations.

The proposed MPNet method splits the tokens in a sequence into non-predicted and predicted parts, allowing it to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. This approach enables MPNet to leverage the full position information of a sentence, reducing the position discrepancy and improving performance.

We pre-trained MPNet on a large-scale text corpus and fine-tuned it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. The experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Related works have explored various pre-training methods, including autoregressive (AR) language modeling and autoencoding (AE). AR language modeling estimates the probability distribution of a text corpus with an autoregressive model, while AE-based pre-training aims to reconstruct the original data from corrupted input. BERT adopts MLM for pre-training, while XLNet introduces PLM for pre-training to address the dependency among predicted tokens. However, XLNet suffers from position discrepancy, which MPNet addresses by taking auxiliary position information as input.

Other related works have proposed joint autoregressive and non-autoregressive training, bridging AR and NAR generation, and improving non-autoregressive translation models without distillation. These works have shown significant improvements in various sequence generation tasks, including machine translation and GLGE benchmarks.

In summary, this paper proposes a new pre-training method called MPNet that unifies the advantages of MLM and PLM while addressing their limitations. The proposed approach splits the tokens in a sequence into non-predicted and predicted parts, allowing it to leverage the full position information of a sentence and reduce the position discrepancy. The experimental results show that MPNet outperforms previous well-known models on various downstream benchmark tasks.