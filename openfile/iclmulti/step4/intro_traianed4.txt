Introduction:

Neural machine translation (NMT) models have achieved remarkable success in recent years, especially with the advent of Transformer-based models (Vaswani et al., 2017). However, these models are typically trained in an autoregressive (AR) manner, which can be computationally expensive and time-consuming for long sequences. To address this issue, non-autoregressive (NAR) models have been proposed, which can generate tokens in parallel and significantly reduce inference latency (Gu et al., 2018; Ghazvininejad et al., 2019; Kasai et al., 2020). However, NAR models still lag behind their AR counterparts in terms of accuracy, and require sequence-level knowledge distillation to achieve competitive performance (Ghazvininejad et al., 2019). 

To bridge the gap between AR and NAR models, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction. The model is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. 

To achieve this, the paper develops a method to simultaneously predict the future n-gram at each time step during the training phase, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. This extends the two-stream self-attention proposed in XLNet to n-stream self-attention. 

ProphetNet is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. The model is then fine-tuned on several NLG tasks and achieves the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Related works have explored various pre-training methods for NMT models, including JANUS (Liang et al., 2021), which proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. Another work, CeMAT (Wang et al., 2021), proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. 

In summary, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. The model is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The paper also extends the two-stream self-attention proposed in XLNet to n-stream self-attention. ProphetNet achieves new state-of-the-art results on CNN/DailyMail and Gigaword, and the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.