Neural language models have achieved remarkable success in natural language processing tasks. Among them, pre-training models have been widely used to improve the performance of downstream tasks. However, existing pre-training methods have their limitations. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations.

MLM and PLM are two popular pre-training methods that have been widely used in natural language processing. MLM is an autoregressive method that estimates the probability distribution of a text corpus with a forward or backward product. However, MLM only encodes a unidirectional context, which is not effective at modeling deep bidirectional contexts. On the other hand, PLM is a permutation-based method that maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order. PLM captures bidirectional context by allowing each position to utilize contextual information from both left and right. However, PLM does not leverage the full position information of a sentence, which results in a position discrepancy between pre-training and fine-tuning.

To address the limitations of MLM and PLM, we propose MPNet, which splits the tokens in a sequence into non-predicted and predicted parts. This approach allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Related works have explored various pre-training methods to improve the performance of natural language processing tasks. JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously. CMLMC is a conditional masked language model that addresses the problems of token indistinguishability and mismatch between training and inference. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Universal Conditional Masked Language Pre-training pre-trains a unified model for fine-tuning on both Autoregressive and Non-autoregressive NMT tasks. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts and overcomes the limitations of BERT. 

In summary, our proposed MPNet unifies the advantages of MLM and PLM while addressing their limitations. We split the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. Experimental results show that MPNet outperforms previous well-known models on various downstream benchmark tasks.