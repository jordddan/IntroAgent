Introduction:

Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT) in recent years. However, autoregressive (AR) models, which translate one token at a time, can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been explored, which translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts and only become competitive when trained with distillation. 

To bridge the gap between AR and NAR generation, this paper proposes the Conditional Masked Language Model with Correction (CMLMC). CMLMC addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation. The decoder structure of CMLM is modified by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Additionally, a novel correction loss is proposed that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 

The proposed CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. This is a significant contribution to the field of NMT, as it provides a more efficient and accurate approach to machine translation. 

Previous works have explored various approaches to improve NAR machine translation. JANUS is a joint autoregressive and non-autoregressive training method that uses an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, which has achieved state-of-the-art results on multiple NMT benchmarks. BANG is a pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism to improve sequence-to-sequence pre-training. Universal Conditional Masked Language Pre-training is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts and overcomes the limitations of BERT. 

In summary, this paper proposes the Conditional Masked Language Model with Correction (CMLMC) for NAR machine translation, which addresses the shortcomings of the Conditional Masked Language Model (CMLM). The proposed model achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. This contribution is significant in the field of NMT, as it provides a more efficient and accurate approach to machine translation. The related works have explored various approaches to improve NAR machine translation, but CMLMC stands out as a novel and effective solution to the problem.