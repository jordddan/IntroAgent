Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) models have been widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning, but they suffer from slow inference speed due to the need to predict one token at a time. Non-autoregressive (NAR) models have been proposed to address this issue by predicting blocks of tokens in parallel, but they still lag behind AR models in terms of performance. To bridge the gap between AR and NAR models, this paper proposes BANG, the first large-scale pre-training model designed for NAR and semi-NAR generation.

Previous works have explored the combination of AR and NAR mechanisms, but they only consider the relevance of model parameters, ignoring the correlations between the two manners. BANG addresses this issue by pre-training a generative model that combines the strengths of both AR and NAR mechanisms. BANG is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR fine-tuning to meet different requirements with the same pre-trained model structure.

To evaluate the effectiveness of BANG, experiments were conducted on various natural language generation tasks, including question generation, summarization, and dialogue generation. The results show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning. For AR fine-tuning, BANG can attain comparable performance with strong AR pre-trained models.

Related works have explored various approaches to improve the performance of pre-trained language models. JANUS proposes a joint AR and NAR training method to enhance the model performance in both AR and NAR manners simultaneously. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes a new method to address these problems. Attention Is All You Need proposes a new simple network architecture based solely on attention mechanisms, which achieves superior quality while being more parallelizable and requiring significantly less time to train. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism to optimize n-step ahead prediction. Universal Conditional Masked Language Pre-training demonstrates that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both AR and NAR NMT. XLNet proposes a generalized autoregressive pre-training method that enables learning bidirectional contexts and overcomes the limitations of BERT. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence, achieving better results on various downstream tasks.

In summary, this paper proposes BANG, a new large-scale pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pre-training a generative model. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with strong AR pre-trained models. The related works have explored various approaches to improve the performance of pre-trained language models, and they provide valuable insights for the development of BANG.