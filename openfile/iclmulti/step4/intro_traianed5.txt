Introduction:

Machine translation has been a popular research topic in the field of natural language processing. The development of pre-training models has significantly improved the performance of machine translation. However, most pre-training models are based on either Autoregressive NMT (AT) or Non-autoregressive NMT (NAT) tasks, which have their own limitations. To address this issue, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. 

Previous studies have shown that the bidirectional decoder is a promising approach for machine translation. However, the training of bidirectional decoders is challenging due to the difficulty of modeling the dependencies between the source and target languages. To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. 

To evaluate the effectiveness of CeMAT, we conduct experiments on both low-resource and high-resource settings. The results show that CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Specifically, CeMAT achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. 

Several related works have been proposed to improve the performance of machine translation. JANUS is a joint autoregressive and non-autoregressive training method that uses an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. BANG is a new pre-training model that bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. ProphetNet presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet is a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. 

In summary, our proposed CeMAT model provides a promising solution for machine translation by pre-training on both monolingual and bilingual corpora and providing unified initialization parameters for both AT and NAT tasks. The aligned code-switching & masking and dynamic dual-masking techniques enhance the model training under the setting of bidirectional decoders. The experimental results demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings.