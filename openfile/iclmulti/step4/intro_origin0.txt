Introduction:

Neural machine translation (NMT) has achieved remarkable progress in recent years, with the Transformer-based models being the state-of-the-art. Autoregressive (AR) models translate one token at a time, which can be time-consuming, especially for long sequences. Non-autoregressive (NAR) models, on the other hand, translate blocks of tokens in parallel, which significantly speeds up the inference process. However, NAR models still lag behind AR models in terms of performance, and only become competitive when trained with distillation data. In this paper, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. The proposed method effectively alleviates the problem of distribution discrepancy and achieves similar results to the state-of-the-art NAR model without distillation data. Moreover, JANUS improves the AR model performance by more than 1.5 BLEU scores on average. 

To the best of our knowledge, JANUS is the first method that combines AR and NAR models in a joint training framework. Previous works have explored either AR or NAR models separately, such as the Conditional Masked Language Model with Correction (CMLMC) and BANG. CMLMC addresses the indistinguishability of tokens and mismatch between training and inference, while BANG bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining. However, both methods still suffer from performance gaps compared to the state-of-the-art models. 

Our proposed JANUS method is evaluated on multiple NMT datasets and autoregressive pretraining models. Empirical results show that JANUS achieves comparable performance to the state-of-the-art NAR model without distillation data and outperforms the AR model by more than 1.5 BLEU scores on average. Furthermore, JANUS exceeds the NAR pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

In summary, our proposed JANUS method introduces a novel approach to combine the strengths of both AR and NAR models while avoiding their weaknesses. The auxiliary distribution P AUX bridges the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. Empirical results demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving comparable performance to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average.