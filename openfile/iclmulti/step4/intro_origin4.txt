Introduction:

Neural machine translation (NMT) models have achieved remarkable success in recent years, especially those based on the Transformer architecture (Vaswani et al., 2017). However, the autoregressive (AR) principle used in these models, where translation is done one token at a time conditioning on already translated tokens, can be time-consuming for long sequences. To address this issue, non-autoregressive (NAR) models have been proposed, which translate subsets of tokens in parallel (Gu et al., 2018; Ghazvininejad et al., 2019; Kasai et al., 2020). Despite their faster inference speed, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation (Ghazvininejad et al., 2019). 

To bridge the gap between AR and NAR, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet, which introduces a novel self-supervised objective future n-gram prediction. The model is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. This encourages the model to plan for future tokens and prevents overfitting on strong local correlations. 

To achieve this, the paper develops a method to simultaneously predict the future n-gram at each time step during the training phase. This method extends the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. 

ProphetNet is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. The paper also fine-tunes ProphetNet on several NLG tasks and achieves the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Related works have explored various pre-training methods for NMT models, including JANUS (Liang et al., 2021), which proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. Another related work is BANG (Gong et al., 2021), which proposes a new pre-training model designed for NAR and semi-NAR generation. BANG bridges the gap between AR and NAR by considering arbitrary previous [MASK] length during large-scale pretraining. 

Other related works include Attention Is All You Need (Vaswani et al., 2017), which proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, and IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION (Ding et al., 2021), which investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) to address these problems. 

In summary, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. The paper develops a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. ProphetNet achieves new state-of-the-art results on CNN/DailyMail and Gigaword, and the best performance on several NLG tasks. The related works have explored various pre-training methods for NMT models, including JANUS, BANG, Attention Is All You Need, and IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION.