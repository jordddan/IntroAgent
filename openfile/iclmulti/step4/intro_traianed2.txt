Introduction:

Neural network architectures for sequence modeling and transduction problems have been widely studied in recent years. The traditional approach relies on recurrent networks, which have been shown to be effective but suffer from slow training and inference times. In this paper, we introduce the Transformer, a new neural network architecture that relies entirely on an attention mechanism to draw global dependencies between input and output, without the need for recurrent networks. 

The Transformer architecture allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. This is achieved through the use of scaled dot-product attention, multi-head attention, and the parameter-free position representation, which are key components of the Transformer architecture. 

To implement and evaluate the Transformer, we developed tensor2tensor, a library for training deep learning models in a distributed and efficient manner. Tensor2tensor was used to train the Transformer and demonstrate its effectiveness in various sequence modeling and transduction tasks. 

In related works, recent studies have explored the use of autoregressive and non-autoregressive approaches for sequence generation tasks. Autoregressive models can obtain excellent performance, while non-autoregressive models bring fast decoding speed for inference. However, the performance of non-autoregressive models still lags behind their autoregressive counterparts, and only becomes competitive when trained with distillation. 

To address these issues, several studies have proposed new pre-training methods, such as JANUS, BANG, and MPNet, which aim to bridge the gap between autoregressive and non-autoregressive generation. These studies have shown significant improvements in performance and demonstrate the potential of pre-training methods for sequence generation tasks. 

In summary, this paper introduces the Transformer, a new neural network architecture for sequence modeling and transduction problems that relies entirely on an attention mechanism. We demonstrate the effectiveness of the Transformer in various tasks and propose new pre-training methods to bridge the gap between autoregressive and non-autoregressive generation.