Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. Autoregressive (AR) models have been widely used for sequence modeling and sequence-to-sequence (Seq2Seq) learning, while non-autoregressive (NAR) models have shown great potential to reduce inference latency by introducing parallel decoding. However, NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. To bridge the gap between AR and NAR, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation. 

BANG is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. This approach enables BANG to support NAR, semi-NAR, and AR fine-tuning to meet different requirements with the same pre-trained model structure. 

To evaluate the effectiveness of BANG, we conduct experiments on various natural language generation tasks, including question generation, summarization, and dialogue generation. The results show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR fine-tuning. For AR fine-tuning, BANG can attain comparable performance with the comparison to strong AR pre-trained models. 

Previous works have attempted to address the limitations of AR and NAR models. For example, JANUS proposed a joint training method using an auxiliary loss to enhance the model performance in both AR and NAR manners simultaneously. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION proposed a new method to improve NAR translation models without distillation. Attention Is All You Need introduced a new simple network architecture based solely on attention mechanisms. ProphetNet presented a new sequence-to-sequence pre-training model with a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Universal Conditional Masked Language Pre-training proposed a unified model for fine-tuning on both AR and NAR tasks. XLNet proposed a generalized autoregressive pre-training method that enables learning bidirectional contexts and overcomes the limitations of BERT. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. 

In this paper, we propose BANG, a new pre-training model that bridges the gap between AR and NAR via pre-training a generative model. BANG achieves significant performance improvements on various natural language generation tasks for NAR and semi-NAR fine-tuning, and for AR fine-tuning, it can attain comparable performance with strong AR pre-trained models. The rest of the paper is organized as follows. Section 2 describes the details of the BANG model. Section 3 presents the experimental results and analysis. Finally, Section 4 concludes the paper and discusses future work.