Introduction:

Large-scale pre-trained language models have achieved remarkable success in various natural language processing tasks. However, existing models such as BERT suffer from limitations such as the pretrain-finetune discrepancy and the independence assumption made in factorizing the joint probability of predicted tokens. In this paper, we introduce XLNet, a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. 

XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach eliminates the independence assumption made in BERT and provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens. 

XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. 

To improve architectural designs for pretraining, we incorporate the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. We also propose a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Empirical results demonstrate that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Our proposed method has also been compared with other related works such as Attention Is All You Need, BANG, ProphetNet, and Universal Conditional Masked Language Pre-training, and has shown superior performance. 

In summary, this paper introduces XLNet, a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. We demonstrate that XLNet outperforms BERT on a wide range of natural language processing tasks and propose improvements to architectural designs for pretraining. Our proposed method has shown superior performance compared to other related works.