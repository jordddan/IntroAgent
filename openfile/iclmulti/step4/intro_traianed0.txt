Introduction:

Neural machine translation (NMT) models have achieved remarkable success in recent years, with Transformer-based models being the state-of-the-art. However, there are two main approaches to NMT: autoregressive (AR) and non-autoregressive (NAR). AR models translate one token at a time, which can be time-consuming for long sequences, while NAR models translate blocks of tokens in parallel, which is faster but often leads to lower quality. In this paper, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

Related works have explored the limitations of AR and NAR models and proposed solutions to bridge the gap between them. Attention Is All You Need introduced the Transformer architecture, which relies solely on attention mechanisms and achieves superior performance while being more parallelizable and requiring less training time. BANG proposed a pre-training model that bridges AR and NAR generation by designing a novel model structure for large-scale pre-training. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigated the performance gap between AR and NAR models and proposed the Conditional Masked Language Model with Correction (CMLMC) to address the problem of distribution discrepancy. ProphetNet introduced a new self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism to optimize n-step ahead prediction. Universal Conditional Masked Language Pre-training proposed CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages, to improve performance for both AR and NAR NMT. MPNet leveraged the dependency among predicted tokens through permuted language modeling and took auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy.

Our proposed method, JANUS, introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Furthermore, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

In conclusion, our proposed method, JANUS, combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.