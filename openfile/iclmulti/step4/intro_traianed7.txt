Introduction:

Pre-training has become a crucial step in natural language processing (NLP) tasks, and various pre-training methods have been proposed to improve the performance of downstream tasks. Among them, Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have been the two most successful pre-training objectives. However, MLM only considers the dependency between the predicted tokens, while PLM neglects the full position information of a sentence, leading to a position discrepancy between pre-training and fine-tuning. To address these limitations, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet), which unifies the advantages of both MLM and PLM.

In MPNet, we introduce a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. Specifically, we propose a novel pre-training objective that maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context.

We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

In related works, recent studies have proposed various pre-training methods to improve the performance of NLP tasks. For example, JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously and effectively alleviates the problem of distribution discrepancy. CMLMC is a conditional masked language model that addresses the problems of token indistinguishability and mismatch between training and inference in NAR translation models. ProphetNet is a sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages, which can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. BANG is a new pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training.

In summary, our proposed MPNet pre-training method unifies the advantages of MLM and PLM while addressing their limitations. We introduce a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. Our experiments show that MPNet outperforms previous well-known models on various downstream benchmark tasks.