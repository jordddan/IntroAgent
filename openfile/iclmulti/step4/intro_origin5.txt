Introduction:

Machine translation has been a popular research topic in natural language processing (NLP) for decades. Recently, pre-training models have shown significant improvements in machine translation performance. In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

Previous works have shown that NAR models can reduce generation latency, but they generally come with a decrease in accuracy. To address this issue, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages.

Our main contribution is the demonstration of the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. To evaluate the performance of CeMAT, we conduct experiments on multiple datasets, including low-resource and high-resource languages. Our results show that CeMAT outperforms strong competitors in both AT and NAT tasks, achieving significant improvements in machine translation performance.

Related works have explored various pre-training methods for machine translation. JANUS proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) to address these problems. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. BANG proposes a new pre-training model to bridge the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. ProphetNet presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. MPNet proposes a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations.

In summary, our proposed CeMAT model is a novel pre-training model for machine translation that achieves significant improvements in both AT and NAT tasks. Our experiments demonstrate the effectiveness of aligned code-switching & masking and dynamic dual-masking techniques in enhancing the model training under the setting of bidirectional decoders. Compared to related works, CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.