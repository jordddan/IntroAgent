Introduction:

Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT) in recent years. However, non-autoregressive (NAR) models have been explored to accelerate inference, which translates blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their autoregressive (AR) counterparts, and only become competitive when trained with distillation. In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC) that addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation.

Related works have explored the use of attention mechanisms in sequence-to-sequence models, such as the Transformer model, which has shown superior performance in quality, parallelizability, and training time. Other works have proposed pre-training models for NMT, such as ProphetNet and CeMAT, which have achieved state-of-the-art results on multiple datasets. Additionally, JANUS and BANG have proposed joint AR and NAR training methods to enhance model performance in both AR and NAR manners simultaneously.

The proposed CMLMC model modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Furthermore, a novel correction loss is proposed that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. These modifications address the problems of indistinguishability of tokens and mismatch between training and inference, resulting in new state-of-the-art undistilled NAR results and approaching AR performance on multiple NMT benchmarks.

In summary, this paper proposes the CMLMC model, which addresses the shortcomings of CMLM in NAR machine translation. The proposed model achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. The related works have explored attention mechanisms, pre-training models, and joint AR and NAR training methods to enhance NMT performance. The following sections will provide a comprehensive overview of the related work, describe the methodology and approach used in the research, and provide technical details about the proposed approach. Finally, the main findings and results of the study will be summarized, the implications of the research will be discussed, and future directions for further investigation will be identified, including the limitations of the proposed method.