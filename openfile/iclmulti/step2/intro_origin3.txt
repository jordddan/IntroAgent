Introduction:

Large-scale pre-trained language models have achieved remarkable success in downstream tasks, especially in sequence-to-sequence (Seq2Seq) learning. Autoregressive (AR) language modeling, which estimates the probability distribution of the text corpus, is widely used for Seq2Seq learning. However, AR-based models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction. To address this issue, recent works have been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. 

In this paper, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevents overfitting on strong local correlations. 

To achieve this, we develop a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. We extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. 

ProphetNet is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. We also fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. 

In related works, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION proposes a new method to improve NAR translation models without distillation. BANG: Bridging Autoregressive and Non-autoregressive Generation with introduces a new pre-training model to bridge the gap between AR and NAR generation. MPNet: Masked and Permuted Pre-training for Abstract Acknowledgements and Disclosure of Funding Broader Impact References proposes a new pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. Universal Conditional Masked Language Pre-training pre-trains a sequence-to-sequence model but with a bidirectional decoder to produce notable performance gains for both Autoregressive and Non-autoregressive NMT. JANUS: Joint Autoregressive and Non-autoregressive Training proposes a joint training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. XLNet: Generalized Autoregressive Pretraining proposes a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. 

In summary, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. We develop a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. We extend the two-stream self-attention proposed in XLNet to n-stream self-attention for future n-gram prediction. ProphetNet achieves new state-of-the-art results on CNN/DailyMail and Gigaword, and the best performance on several NLG tasks compared to the models using the same base scale pre-training dataset.