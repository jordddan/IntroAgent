Introduction:

Sequence-to-sequence (Seq2Seq) models have achieved remarkable success in natural language processing (NLP) tasks, especially in neural machine translation (NMT). Autoregressive (AR) models, which generate tokens sequentially, have been the dominant approach in Seq2Seq models. However, AR models suffer from slow inference speed, which limits their practical applications. Non-autoregressive (NAR) models, which generate tokens in parallel, have been proposed to address this issue. Despite the significant progress made in NAR models, they still lag behind AR models in terms of performance, and only become competitive when trained with distillation. 

To bridge the gap between AR and NAR models, this paper proposes JANUS, a method that combines the strengths of both models while avoiding their weaknesses. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. 

To demonstrate the effectiveness of JANUS, experiments are conducted on multiple NMT datasets and autoregressive pretraining models. The results show that JANUS achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average. 

Previous works have proposed various pretraining methods for Seq2Seq models, such as BERT, XLNet, and MPNet. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) to capture the dependency among predicted tokens. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. BANG is another pretraining model that bridges the gap between AR and NAR models. However, JANUS is the first method that combines the strengths of both AR and NAR models while avoiding their weaknesses. 

In summary, this paper proposes JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. Experiments show that JANUS achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average. Compared to previous pretraining models, JANUS is the first method that combines the strengths of both AR and NAR models while avoiding their weaknesses.