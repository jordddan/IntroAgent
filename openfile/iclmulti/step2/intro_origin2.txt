Introduction:

Pre-training models have become a popular approach for improving the performance of natural language processing tasks. Among them, Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) are two widely used pre-training methods. However, MLM neglects the dependency among predicted tokens, while PLM does not leverage the full position information of a sentence, leading to a position discrepancy between pre-training and fine-tuning. To address these limitations, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM.

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. Specifically, MPNet is optimized by predicting the next n tokens simultaneously based on previous context tokens at each time step, which encourages the model to plan for the future tokens and prevent overfitting on strong local correlations.

We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. The experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting. 

Related works have proposed various pre-training methods to improve the performance of natural language processing tasks. For example, BERT adopts MLM for pre-training, while XLNet introduces PLM to address the dependency among predicted tokens. However, these methods have their limitations, such as neglecting the dependency among predicted tokens or suffering from a pretrain-finetune discrepancy. To address these limitations, CeMAT proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages, while JANUS proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. XLNet proposes a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation.

In summary, our proposed MPNet unifies the advantages of MLM and PLM while addressing their limitations, and achieves state-of-the-art performance on various downstream benchmark tasks.