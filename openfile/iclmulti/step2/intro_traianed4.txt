Machine translation has been a long-standing challenge in natural language processing (NLP). Recently, pre-training models have shown remarkable success in improving machine translation performance. However, most of the existing pre-training models are designed for either Autoregressive NMT (AT) or Non-autoregressive NMT (NAT) tasks, and they require different initialization parameters. To address this issue, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. 

To enhance the model training under the setting of bidirectional decoders, we introduce two techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques are designed to improve the model's ability to capture the dependencies between the source and target languages.

We evaluate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our experiments demonstrate that CeMAT outperforms the state-of-the-art models on both low-resource and high-resource machine translation tasks.

Related works have shown that pre-training models have significantly improved NMT performance. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) to address the problem of dependency among predicted tokens. However, XLNet suffers from position discrepancy between pre-training and fine-tuning. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously and effectively alleviates the problem of distribution discrepancy. BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. 

In summary, our proposed CeMAT model is a bidirectional pre-training model for machine translation that can provide unified initialization parameters for both AT and NAT tasks. We introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Our experiments demonstrate that CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.