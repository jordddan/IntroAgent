Introduction:

Pre-training models have significantly improved the performance of Neural Machine Translation (NMT) tasks. Autoregressive (AR) models have achieved high-quality results, but their inference latency is a well-known limitation for online real-time usage. Non-autoregressive (NAR) models have been proposed to reduce generation latency, but they generally come with a decrease in accuracy. To balance latency and accuracy, semi-NAR models have been proposed. However, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance.

JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. Specifically, JANUS is designed to improve the performance of NMT tasks by combining the advantages of AR and NAR models. JANUS is a joint AR and NAR training method that uses an auxiliary loss to enhance the model performance in both AR and NAR manners simultaneously and effectively alleviate the problem of distribution discrepancy.

To demonstrate the effectiveness of JANUS, we conduct experiments on multiple NMT datasets and autoregressive pretraining models. The results show that JANUS achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average. Moreover, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Related works have proposed various pre-training methods to improve the performance of NMT tasks. Conditional Masked Language Model with Correction (CMLMC) addresses the problems of indistinguishability of tokens and mismatch between training and inference. BANG bridges the gap between AR and NAR by considering arbitrary previous [MASK] length during large-scale pretraining. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Universal Conditional Masked Language Pre-training proposes CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation.

In summary, JANUS is a novel method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance. The auxiliary distribution P AUX bridges the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. The experiments show that JANUS achieves significant improvements on multiple NMT datasets and autoregressive pretraining models, exceeding the non-autoregressive pretraining model BANG on the same GLGE tasks and achieving comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.