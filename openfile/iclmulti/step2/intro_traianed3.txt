Introduction:

Large-scale pre-trained language models have achieved remarkable success in downstream tasks, including sequence-to-sequence (Seq2Seq) learning. Autoregressive (AR) language modeling, which estimates the probability distribution of the text corpus, is widely used for sequence modeling and Seq2Seq learning. However, AR-based models may prefer to focus on the latest tokens rather than capture long-term dependencies for the next token prediction. To address this issue, we propose a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. 

The main contributions of this paper are as follows: First, we introduce ProphetNet, a new pre-training model that optimizes n-step ahead prediction to predict the next n tokens simultaneously based on previous context tokens at each time step. Second, we develop a method to simultaneously predict the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. Third, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. Fourth, we pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Finally, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Previous works have explored non-autoregressive (NAR) approaches that translate blocks of tokens in parallel to accelerate inference. However, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. To address this issue, we investigate possible reasons behind this performance gap and propose the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Empirically, we show that CMLMC achieves state-of-the-art NAR performance when trained on raw data without distillation, and approaches AR performance on multiple datasets.

In addition, we propose BANG, a new pre-training model to bridge the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. The pretrained BANG model can simultaneously support AR, NAR, and semi-NAR generation to meet different requirements. Experiments on question generation (SQuAD 1.1), summarization (XSum), and dialogue generation (PersonaChat) show that BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models.

Moreover, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. We pre-train MPNet on a large-scale dataset and fine-tune on a variety of downstream tasks, achieving better results on these tasks compared with previous state-of-the-art pre-trained methods.

Finally, we propose JANUS, a Joint Autoregressive and Non-autoregressive training method using a Uxiliary los S to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. Empirically, we show our approach and BART-JANUS can achieve significant improvement on multiple generation tasks, including machine translation and GLGE benchmarks.

In summary, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction, which achieves state-of-the-art results on several NLG tasks. The proposed model and related works provide new insights into improving the performance of Seq2Seq models and have the potential to advance the field of natural language processing.