INTRODUCTION

Neural machine translation (NMT) models have achieved remarkable performance in recent years, especially with the advent of Transformer-based models (Vaswani et al., 2017). However, the autoregressive (AR) framework used in these models translates one token at a time, which can be time-consuming for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed, which translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation (Gu et al., 2018; Ghazvininejad et al., 2019). 

To bridge the gap between AR and NAR, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

Our proposed model achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning. For AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. Our model bridges the gap between AR and NAR, and provides a unified solution for different generation tasks. 

Prior works have explored pretraining models for NMT, such as IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION (Ding et al., 2021), Universal Conditional Masked Language Pre-training (Liu et al., 2021), and MPNet: Masked and Permuted Pre-training for Abstract Acknowledgements and Disclosure of Funding Broader Impact References (Dai et al., 2020). These works have shown that pretraining can significantly improve NMT performance. However, our proposed model is the first to focus on NAR and semi-NAR generation, and to provide a unified solution for different generation tasks. 

In addition, JANUS: Joint Autoregressive and Non-autoregressive Training with Xiaobo Liang (Liang et al., 2021) proposed a joint training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. ProphetNet: Predicting Future N-gram for Sequence-to-Sequence (Qi et al., 2020) introduced a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Attention Is All You Need (Vaswani et al., 2017) proposed a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. XLNet: Generalized Autoregressive Pretraining (Yang et al., 2019) proposed a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. 

In summary, our proposed model, BANG, bridges the gap between AR and NAR, and provides a unified solution for different generation tasks. Our model achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.