Introduction:

Transformer-based autoregressive (AR) machine translation models have achieved remarkable performance improvements, nearing human-level accuracy on some languages. However, the AR framework translates one token at a time, which can be time-consuming, especially for long sequences. To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts and only become competitive when trained with distillation. In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC), which addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation.

The principal difference between AR and NAR is that they use distinct attention mechanisms. In particular, the AR uses unidirectional attention to simplify the sentence probability distributions by introducing Markov Hypothesis, which outputs the next token only depending on the previous context. This pattern makes the output distribution of each token accurate and unambiguous but also lacks diversity in inference. The NAR introduces bi-directional attention that can capture bi-directional context by considering the whole sentence information. This pattern needs to predict multiple tokens simultaneously, which causes the token distribution to be ambiguous, stemming from the multi-modality problem. 

To address these issues, we propose CMLMC, which modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. CMLMC leverages the strengths of both AR and NAR mechanisms while avoiding their weaknesses to improve their performance.

Empirically, we show that CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. Our proposed method outperforms previous state-of-the-art pre-trained models, including BERT, XLNet, and RoBERTa, under the same model setting. 

Related Works:

Several related works have been proposed to improve the performance of NAR machine translation models. BANG is a pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training. MPNet is a pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. ProphetNet is a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Universal Conditional Masked Language Pre-training is a pre-trained sequence-to-sequence model that demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both AR and NAR NMT. JANUS is a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. Attention Is All You Need is a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 

In summary, our proposed CMLMC method addresses the shortcomings of the CMLM in NAR machine translation and achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks.