Machine translation has been a popular research topic in natural language processing, and transformer-based autoregressive (AR) models have achieved state-of-the-art performance in this field. However, AR models have a disadvantage of being time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed to translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts and only become competitive when trained with distillation. 

To improve NAR machine translation models without distillation, this paper proposes the Conditional Masked Language Model with Correction (CMLMC). CMLMC addresses the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation by modifying the decoder structure of CMLM. The proposed model exposes the positional encodings and incorporates causal attention layers to differentiate adjacent tokens. Additionally, a novel correction loss is proposed to teach the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 

The proposed CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. To investigate possible reasons behind the performance gap between AR and NAR models, recent works have been exploring the indistinguishability of tokens and mismatch between training and inference. BANG, a new pre-training model, has been proposed to bridge the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining. 

Other pre-training models have also been proposed to improve NMT performance, such as MPNet, ProphetNet, and Universal Conditional Masked Language Pre-training. MPNet leverages the dependency among predicted tokens through permuted language modeling, while ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Universal Conditional Masked Language Pre-training proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. 

In comparison, CMLMC proposes a novel correction loss to teach the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. The proposed model achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. The proposed model is a significant improvement over the existing NAR models and can be used to accelerate inference, especially for long sequences.