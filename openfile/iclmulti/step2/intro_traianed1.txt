INTRODUCTION

Neural machine translation (NMT) models have achieved remarkable performance in recent years, especially with the advent of Transformer-based models (Vaswani et al., 2017). Autoregressive (AR) models, which translate one token at a time, have been the dominant approach in NMT. However, AR inference can be time-consuming, especially for long sequences. To address this issue, non-autoregressive (NAR) approaches have been proposed, which translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation (Gu et al., 2018; Ghazvininejad et al., 2019). 

To bridge the gap between AR and NAR, we propose BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

Previous works have shown that pretraining models can significantly improve NMT performance (Conneau and Lample, 2019; Lin et al., 2020). However, most pretraining models adopt an unidirectional decoder, which may not be optimal for both AR and NAR generation. In contrast, BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining. 

To evaluate the effectiveness of BANG, we conduct experiments on question generation (SQuAD 1.1), summarization (XSum), and dialogue generation (PersonaChat). The results show that BANG improves NAR and semi-NAR performance significantly, as well as attaining comparable performance with strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong NAR baselines. 

In recent years, pretraining models have been widely adopted in NLP tasks, such as masked language modeling (MLM) and permuted language modeling (PLM) (Devlin et al., 2019; Yang et al., 2019). However, these models neglect the dependency among predicted tokens, which may lead to performance degradation. In contrast, BANG leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy. 

In conclusion, BANG is a novel pretraining model that bridges the gap between AR and NAR generation. It achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models. The proposed model structure and pretraining method can be applied to other NLP tasks and may lead to further improvements in performance.