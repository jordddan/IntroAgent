Machine translation has been a long-standing challenge in natural language processing (NLP). Recently, pre-training models have shown remarkable success in improving machine translation performance. However, most existing pre-training models are designed for either autoregressive NMT (AT) or non-autoregressive NMT (NAT) tasks, and cannot provide unified initialization parameters for both tasks. In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. 

To enhance the model training under the setting of bidirectional decoders, we introduce two techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. These techniques are designed to improve the model's ability to capture the dependencies between the source and target languages, and to prevent overfitting on strong local correlations.

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our experiments show that CeMAT outperforms the state-of-the-art models on several benchmarks, including WMT14 English-German and WMT14 English-French. 

Related works have explored various pre-training models for NMT, such as BERT, XLNet, and MPNet. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) to address the problem of dependency among predicted tokens. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. JANUS proposes a joint autoregressive and non-autoregressive training method to enhance the model performance in both AR and NAR manner simultaneously. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. These works have achieved significant improvements in machine translation performance, but none of them provide unified initialization parameters for both AT and NAT tasks like CeMAT does. 

In summary, CeMAT is a novel pre-training model for machine translation that provides unified initialization parameters for both AT and NAT tasks. We introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Our experiments demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. Compared with related works, CeMAT provides a unique solution to the problem of providing unified initialization parameters for both AT and NAT tasks.