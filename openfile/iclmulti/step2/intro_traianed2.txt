Introduction:

Pre-training models have become a popular approach for improving the performance of natural language processing tasks. Among the pre-training methods, Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) have been widely used. However, MLM neglects the dependency among predicted tokens, while PLM suffers from position discrepancy. To address these limitations, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM. 

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows the model to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. 

Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting. 

Related Works:

Previous works have proposed various pre-training methods to improve the performance of natural language processing tasks. For example, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION proposed a new non-autoregressive translation model that achieves state-of-the-art performance without distillation. BANG: Bridging Autoregressive and Non-autoregressive Generation with introduced a new pre-training model that bridges the gap between autoregressive and non-autoregressive generation. ProphetNet: Predicting Future N-gram for Sequence-to-Sequence proposed a new sequence-to-sequence pre-training model that achieves new state-of-the-art results on multiple datasets. Universal Conditional Masked Language Pre-training proposed a unified model for fine-tuning on both Autoregressive and Non-autoregressive NMT tasks. JANUS: Joint Autoregressive and Non-autoregressive Training proposed a joint training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. Attention Is All You Need proposed a new simple network architecture, the Transformer, based solely on attention mechanisms, which achieves superior performance in quality while being more parallelizable and requiring significantly less time to train. XLNet: Generalized Autoregressive Pretraining proposed a generalized autoregressive pre-training method that outperforms BERT on 20 tasks, often by a large margin. 

In summary, our proposed MPNet method unifies the advantages of MLM and PLM while addressing their limitations, and outperforms previous well-known models on various downstream benchmark tasks.