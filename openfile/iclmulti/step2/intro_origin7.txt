Introduction:

Transformer-based pretraining models have revolutionized the field of natural language processing (NLP) by achieving state-of-the-art performance on various NLP tasks. However, these models still suffer from limitations such as the pretrain-finetune discrepancy and the independence assumption made in pretraining. In this paper, we introduce XLNet, a generalized autoregressive pretraining method that overcomes these limitations and achieves superior performance on a wide range of NLP tasks.

XLNet combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. Unlike previous models, XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach eliminates the independence assumption made in pretraining and provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens.

XLNet also incorporates the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which improves performance, especially for tasks involving longer text sequences. Furthermore, we propose a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. Empirically, XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task.

In addition, we provide a comparison with related works such as IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, BANG: Bridging Autoregressive and Non-autoregressive Generation with, MPNet: Masked and Permuted Pre-training for Abstract Acknowledgements and Disclosure of Funding Broader Impact References, ProphetNet: Predicting Future N-gram for Sequence-to-Sequence, Universal Conditional Masked Language Pre-training, and JANUS: Joint Autoregressive and Non-autoregressive Training. These works have made significant contributions to the field of NLP, but XLNet stands out as a generalized autoregressive pretraining method that overcomes the limitations of previous models and achieves superior performance on a wide range of NLP tasks.

In summary, XLNet is a generalized autoregressive pretraining method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. It overcomes the pretrain-finetune discrepancy and the independence assumption made in pretraining, and achieves superior performance on a wide range of NLP tasks.