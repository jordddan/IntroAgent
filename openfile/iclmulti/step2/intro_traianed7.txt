Introduction:

The field of natural language processing has seen significant advancements in recent years, with the development of various pre-training models that have achieved state-of-the-art performance on a wide range of tasks. One such model is XLNet, which is the focus of this paper. The main contributions of this paper are:

Firstly, we introduce XLNet, a generalized autoregressive method that combines the best of both autoregressive language modeling and autoencoding while avoiding their limitations. XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. This approach overcomes the limitations of previous models and provides a more comprehensive understanding of the context.

Secondly, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. This makes XLNet a more reliable and accurate model for natural language processing tasks.

Thirdly, we propose an improved architectural design for pretraining by incorporating the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. This design enhances the model's ability to capture long-term dependencies and improves its performance on tasks that require a deeper understanding of the context.

Fourthly, we propose a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling. This reparameterization improves the model's ability to capture the context and provides a more accurate representation of the language.

Finally, we empirically demonstrate that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Our experiments show that XLNet is a more reliable and accurate model for natural language processing tasks, and its performance is consistently better than that of BERT.

Related works:

Several related works have been proposed in the field of natural language processing. For example, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION proposes a method to improve non-autoregressive translation models without distillation. BANG: Bridging Autoregressive and Non-autoregressive Generation proposes a new pre-training model for bridging autoregressive and non-autoregressive generation. MPNet: Masked and Permuted Pre-training for Abstract Acknowledgements and Disclosure of Funding Broader Impact References proposes a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. ProphetNet: Predicting Future N-gram for Sequence-to-Sequence presents a new sequence-to-sequence pre-training model called ProphetNet. Universal Conditional Masked Language Pre-training proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. JANUS: Joint Autoregressive and Non-autoregressive Training proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. These related works have contributed to the development of the field of natural language processing and have paved the way for the development of XLNet.