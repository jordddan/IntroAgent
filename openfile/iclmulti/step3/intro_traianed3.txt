Introduction:

Pre-training models have significantly improved the performance of natural language processing tasks. Autoregressive (AR) models, such as BERT and XLNet, have achieved high-quality results in various natural language generation tasks. However, they suffer from the problem of slow decoding speed. Non-autoregressive (NAR) models, on the other hand, have shown great potential to reduce inference latency by introducing parallel decoding. In this paper, we propose BANG, a new pre-training model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training.

In related works, MPNet proposes a new pre-training method that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations. Universal Conditional Masked Language Pre-training (CeMAT) demonstrates that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both AR and NAR NMT. JANUS proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems.

The main contributions of this paper are:

1. BANG is proposed as the first large-scale pre-training model designed for NAR and semi-NAR generation, which bridges the gap between AR and NAR via pre-training a generative model.

2. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK].

3. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure.

4. BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning, and for AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.

In this paper, we propose BANG, a new pre-training model that bridges the gap between AR and NAR generation. BANG is designed as the first large-scale pre-training model for NAR and semi-NAR generation, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization. BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. We conduct experiments on question generation (SQuAD 1.1), summarization (XSum), and dialogue generation (PersonaChat) to show that BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat, respectively, compared with the strong NAR baselines.