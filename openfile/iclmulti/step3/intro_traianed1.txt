Introduction:

Machine translation has been a popular research topic in natural language processing (NLP) for decades. Recently, pre-training models have shown significant improvements in machine translation performance. However, most pre-training models are based on autoregressive (AR) language modeling, which generates tokens sequentially and suffers from high latency. Non-autoregressive (NAR) models have been proposed to reduce generation latency, but they generally come with a decrease in accuracy. To balance latency and accuracy, semi-NAR generation models have been proposed. 

In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AR and NAT tasks. CeMAT is designed to bridge the gap between AR and NAR by considering arbitrary previous [MASK] length during large-scale pre-training. 

To enhance the model training under the setting of bidirectional decoders, we introduce two simple but effective techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. 

We conduct extensive experiments to demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AR and NAT tasks with data of varied sizes. Our experiments show that CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. 

In related works, MPNet adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) for pre-training to address the problem of dependency among predicted tokens. Universal Conditional Masked Language Pre-training proposes CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. BANG proposes a new pre-training model to bridge the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. JANUS proposes a Joint Autoregressive and Non-autoregressive training method using a Uxiliary los S to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. ProphetNet presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. 

In summary, our proposed CeMAT model is a bidirectional encoder-decoder model that bridges the gap between AR and NAR models. We introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. Our experiments show that CeMAT achieves significant performance improvement for all scenarios from low- to extremely high-resource languages, making it a promising pre-training model for machine translation.