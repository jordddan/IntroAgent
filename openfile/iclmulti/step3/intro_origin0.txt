The field of natural language processing has seen significant advancements in recent years, particularly in the area of pre-training models. Two popular pre-training methods are Masked Language Modeling (MLM) and Permuted Language Modeling (PLM). While MLM is effective in capturing the context of a sentence, it neglects the dependency among predicted tokens. On the other hand, PLM addresses this issue but does not leverage the full position information of a sentence. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both MLM and PLM while addressing their limitations.

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts. This allows the model to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Previous works have explored pre-training models for natural language processing tasks. CeMAT is a pre-training model that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. It is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. BANG is a pre-training model that bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. JANUS is a Joint Autoregressive and Non-autoregressive training method using an Auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. ProphetNet is a sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet is a generalized autoregressive pre-training method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION is a pre-training model that addresses the problems of indistinguishability of tokens and mismatch between training and inference in non-autoregressive translation models.

In summary, our proposed MPNet pre-training method unifies the advantages of MLM and PLM while addressing their limitations. We introduce a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. Our experiments show that MPNet outperforms previous well-known models on various downstream benchmark tasks.