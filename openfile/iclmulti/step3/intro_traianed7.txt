INTRODUCTION

Neural machine translation (NMT) has achieved remarkable success in recent years, with the Transformer architecture being one of the most widely used models (Vaswani et al., 2017; Barrault et al., 2019; Huang et al., 2020). While autoregressive (AR) models have been the dominant approach, non-autoregressive (NAR) models have been explored as a way to speed up inference, especially for long sequences (Gu et al., 2018; Ghazvininejad et al., 2019; Kasai et al., 2020). However, leading NAR models still require knowledge distillation to achieve competitive accuracy (Ghazvininejad et al., 2019), which is expensive and non-standard. 

To address this issue, this paper proposes the Conditional Masked Language Model with Correction (CMLMC), which improves upon the Conditional Masked Language Model (CMLM) in NAR machine translation (Ghazvininejad et al., 2019). CMLMC modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. Additionally, CMLMC proposes a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. 

The proposed CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks. This is a significant contribution to the field of NMT, as it eliminates the need for knowledge distillation and improves the accuracy of NAR models. 

Related works have explored various approaches to improve NMT models. For instance, Universal Conditional Masked Language Pre-training (CeMAT) is a pre-trained sequence-to-sequence model that adopts a bidirectional decoder to improve Autoregressive and Non-autoregressive NMT (Liu et al., 2021). Attention Is All You Need (Vaswani et al., 2017) proposes a new network architecture, the Transformer, based solely on attention mechanisms, which achieves superior quality and requires less time to train. ProphetNet (Qi et al., 2020) introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism to improve sequence-to-sequence pre-training. 

In conclusion, this paper proposes a novel approach to improve NAR machine translation by introducing the Conditional Masked Language Model with Correction (CMLMC). The proposed model modifies the decoder structure of CMLM and introduces a novel correction loss to improve translation accuracy. The related works have explored various approaches to improve NMT models, and the proposed CMLMC contributes to the field by eliminating the need for knowledge distillation and improving the accuracy of NAR models.