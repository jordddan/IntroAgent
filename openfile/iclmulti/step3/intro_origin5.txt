Introduction:

Pre-training has become a popular approach for improving the performance of sequence-to-sequence models in natural language generation tasks. However, most pre-training models are based on autoregressive (AR) language modeling, which suffers from high latency and is not suitable for online real-time usage. To address this issue, non-autoregressive (NAR) models have been proposed, which generate tokens in parallel and have lower inference latency. However, NAR models generally come with a decrease in accuracy compared to AR models. To balance latency and accuracy, semi-NAR models have been proposed, but most of them focus on translation tasks rather than general natural language generation tasks.

In this paper, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which is designed for both AR and NAR generation. ProphetNet is pre-trained with a novel self-supervised objective called future n-gram prediction, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. During the training phase, we develop a method to simultaneously predict the future n-gram at each time step, which further improves the performance of the model.

To achieve efficient parallelization, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS.

Furthermore, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. Our results demonstrate the effectiveness of ProphetNet in improving the performance of both AR and NAR generation tasks.

Related works have proposed various pre-training methods for sequence-to-sequence models. BERT adopts masked language modeling (MLM) for pre-training, while XLNet introduces permuted language modeling (PLM) to address the dependency issue among predicted tokens. MPNet leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence. CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages, which can provide unified initialization parameters for both AR and NAR tasks. JANUS is a joint autoregressive and non-autoregressive training method that enhances the model performance in both AR and NAR manner simultaneously. 

In summary, our contributions include introducing a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction, developing a method to simultaneously predict the future n-gram at each time step during the training phase, extending the two-stream self-attention proposed in XLNet to n-stream self-attention, pre-training ProphetNet on two scale pre-trained datasets and achieving new state-of-the-art results on CNN/DailyMail and Gigaword, and fine-tuning ProphetNet on several NLG tasks and achieving the best performance compared to the models using the same base scale pre-training dataset.