Introduction:

Pre-trained language models have achieved remarkable success in various natural language processing tasks. However, traditional autoregressive language modeling (AR) and autoencoding have their limitations. AR models may focus on the latest tokens rather than capturing long-term dependencies for the next token prediction, while autoencoding models suffer from the pretrain-finetune discrepancy. To address these limitations, this paper introduces XLNet, a generalized autoregressive method that combines the best of both AR and autoencoding while avoiding their weaknesses.

XLNet maximizes the expected log likelihood of a sequence with respect to all possible permutations of the factorization order, allowing each position to utilize contextual information from both left and right, capturing bidirectional context. Unlike BERT, XLNet does not suffer from the pretrain-finetune discrepancy, and the autoregressive objective provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

To further improve the architectural designs for pretraining, this paper incorporates the recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves performance, especially for tasks involving longer text sequences. Additionally, this paper proposes a reparameterization of the Transformer(-XL) network to remove the ambiguity in naively applying a Transformer(-XL) architecture to permutation-based language modeling.

Empirical results demonstrate that XLNet consistently outperforms BERT on a wide spectrum of problems, including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. 

Related works have also explored the combination of AR and non-autoregressive (NAR) generation models. CeMAT leverages the dependency among predicted tokens through permuted language modeling and takes auxiliary position information as input to make the model see a full sentence, achieving significant performance improvement for all scenarios from low- to extremely high-resource languages. BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining, improving NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models. JANUS proposes a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism, achieving new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus. 

In summary, this paper introduces XLNet, a generalized autoregressive method that combines the best of both AR and autoencoding while avoiding their weaknesses. The proposed method improves the architectural designs for pretraining and achieves state-of-the-art results on various natural language processing tasks. Related works have also explored the combination of AR and NAR generation models, providing insights into the development of more effective pretraining methods.