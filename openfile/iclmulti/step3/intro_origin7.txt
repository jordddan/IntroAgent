INTRODUCTION

Neural machine translation (NMT) has achieved remarkable success in recent years, with the Transformer architecture being the most widely used. While autoregressive (AR) models have been the dominant approach, non-autoregressive (NAR) models have been explored to mitigate the problem of linear inference time with respect to sequence length. However, leading NAR models still require sequence-level knowledge distillation to achieve competitive accuracy. In this paper, we propose the Conditional Masked Language Model with Correction (CMLMC) to address the shortcomings of the Conditional Masked Language Model (CMLM) in NAR machine translation.

CMLM is one of the leading NAR approaches in NMT, but its performance drops significantly below AR benchmarks without distillation. We identify two shortcomings of CMLM that, when addressed, significantly improve NAR translation quality and narrow the gap between raw and distilled performance. First, input token representations in CMLM can become nearly indistinguishable, especially for adjacent positions. Second, there is a misalignment between CMLMâ€™s training and inference procedures. 

To address these problems, we propose CMLMC, which modifies the decoder structure of CMLM by exposing the positional encodings and incorporating causal attention layers to differentiate adjacent tokens. We also propose a novel correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. With these improvements, CMLMC achieves new state-of-the-art undistilled NAR results and approaches AR performance on multiple NMT benchmarks.

Related works have explored various pre-training methods for NMT, such as ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Another work, Universal Conditional Masked Language Pre-training, demonstrates that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both AR and NAR NMT. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, which achieves superior quality while being more parallelizable and requiring significantly less time to train. 

In conclusion, our proposed CMLMC model addresses the shortcomings of CMLM in NAR machine translation and achieves new state-of-the-art undistilled NAR results. The related works provide insights into various pre-training methods for NMT, which can be further explored to improve the performance of NAR models.