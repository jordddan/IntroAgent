Introduction:

Pre-training language models have become a popular approach to improve the accuracy of natural language processing tasks. Autoregressive (AR) models, such as BERT and XLNet, have achieved great success in this field. However, they suffer from the limitation of slow inference speed due to their sequential nature. Non-autoregressive (NAR) models, on the other hand, can generate tokens in parallel, but they often lack the accuracy of AR models. To bridge the gap between AR and NAR models, this paper proposes BANG, the first large-scale pretraining model designed for NAR and semi-NAR generation.

BANG is pre-trained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. This approach allows BANG to support NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

To the best of our knowledge, BANG is the first model that can support both NAR and AR generation with the same pre-trained model structure. Experimental results show that BANG achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning. For AR finetuning, it can attain comparable performance with the comparison to strong AR pretrained models.

Related works have explored various approaches to improve the performance of AR and NAR models. For example, MPNet proposes a new pre-training method that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations. JANUS introduces a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. Universal Conditional Masked Language Pre-training demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both AR and NAR NMT. 

In conclusion, BANG is a novel pre-training model that bridges the gap between AR and NAR models. It achieves significant performance improvements on all the tasks for NAR and semi-NAR finetuning and can attain comparable performance with strong AR pretrained models for AR finetuning. The related works have explored various approaches to improve the performance of AR and NAR models, and this paper contributes to this field by proposing a new pre-training model that supports both NAR and AR generation with the same pre-trained model structure.