Introduction:

Machine translation has been a popular research topic in natural language processing (NLP) for decades. Recently, pre-training models have shown significant improvements in machine translation performance. However, most pre-training models are based on autoregressive (AR) language modeling, which generates tokens sequentially and suffers from high latency. Non-autoregressive (NAR) models have been proposed to reduce generation latency, but they generally come with a decrease in accuracy. To balance latency and accuracy, semi-NAR generation models have been proposed. 

In this paper, we propose CeMAT, a pre-training model for machine translation that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AR and NAT tasks. CeMAT is designed to bridge the gap between AR and NAR by considering arbitrary previous [MASK] length during large-scale pre-training. 

To enhance the model training under the setting of bidirectional decoders, we introduce two simple but effective techniques: aligned code-switching & masking and dynamic dual-masking. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. 

We conduct extensive experiments to demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AR and NAT tasks with data of varied sizes. Our experiments show that CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. 

Related works have shown that pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, CeMAT demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. Attention mechanisms have been shown to be effective in improving machine translation performance. The Transformer model, based solely on attention mechanisms, has been proposed and shown to be superior in quality while being more parallelizable and requiring significantly less time to train. Other related works have proposed novel pre-training models, such as MPNet, JANUS, and ProphetNet, that have achieved state-of-the-art results on various NLP tasks. 

In summary, this paper proposes CeMAT, a pre-training model for machine translation that bridges the gap between AR and NAR by considering arbitrary previous [MASK] length during large-scale pre-training. We introduce two simple but effective techniques to enhance the model training under the setting of bidirectional decoders. Our experiments demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. The related works have shown that pre-trained sequence-to-sequence models and attention mechanisms have significantly improved NLP tasks, and our proposed model is a novel contribution to this field.