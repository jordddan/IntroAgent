Introduction:

Pre-training has become a popular paradigm for improving the performance of natural language generation tasks. Most pre-training works are based on the Transformer architecture and designed with autoregressive (AR) language models. However, AR models suffer from a well-known limitation of high latency, which makes them unsuitable for online real-time usage. To address this issue, non-autoregressive (NAR) models have been proposed, which generate tokens in parallel and have lower inference latency. Semi-NAR models have also been proposed to balance latency and accuracy. However, most of the NAR and semi-NAR models focus on translation tasks rather than general natural language generation tasks.

To bridge the gap between AR and NAR models, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet. ProphetNet introduces a novel self-supervised objective called future n-gram prediction, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. The model is pre-trained using a method that simultaneously predicts the future n-gram at each time step during the training phase. This method encourages the model to plan for future tokens and prevents overfitting on strong local correlations.

ProphetNet extends the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction. The model is pre-trained on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS.

The proposed model is fine-tuned on several NLG tasks and achieves the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset. The model is capable of supporting AR, NAR, and semi-NAR generation to meet different requirements with the same pre-trained model structure.

Related works have proposed various pre-training methods, including MPNet, which leverages the dependency among predicted tokens through permuted language modeling, and CeMAT, which pre-trains a sequence-to-sequence model with a bidirectional decoder and achieves significant performance gains for both Autoregressive and Non-autoregressive NMT. Attention Is All You Need proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, which is superior in quality, more parallelizable, and requires significantly less time to train. BANG proposes a new pre-training model to bridge the gap between Autoregressive and Non-autoregressive Generation, which can simultaneously support AR, NAR, and semi-NAR generation to meet different requirements. JANUS proposes a Joint Autoregressive and Non-autoregressive training method using an Auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between NAR and AR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems.

In summary, this paper proposes a new large-scale pre-trained Seq2Seq model called ProphetNet with a novel self-supervised objective future n-gram prediction. The model is pre-trained using a method that simultaneously predicts the future n-gram at each time step during the training phase, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. The model is capable of supporting AR, NAR, and semi-NAR generation to meet different requirements with the same pre-trained model structure. The proposed model achieves new state-of-the-art results on CNN/DailyMail and Gigaword and achieves the best performance on several NLG tasks compared to the models using the same base scale pre-training dataset.