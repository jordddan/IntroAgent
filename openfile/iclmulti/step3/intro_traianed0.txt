Introduction:

Pre-training models have become a popular paradigm in natural language processing (NLP) tasks, and Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) are two of the most successful pre-training objectives. However, MLM neglects the dependency among predicted tokens, while PLM does not leverage the full position information of a sentence, leading to a position discrepancy between pre-training and fine-tuning. To address these limitations, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet), which unifies the advantages of both MLM and PLM.

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, allowing the model to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB, which outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Previous works have explored pre-training models with different objectives, such as Universal Conditional Masked Language Pre-training, Attention Is All You Need, BANG, JANUS, ProphetNet, and XLNet. Universal Conditional Masked Language Pre-training pre-trains a sequence-to-sequence model with a bidirectional decoder, while Attention Is All You Need proposes a new network architecture based solely on attention mechanisms. BANG bridges the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation, while JANUS proposes a Joint Autoregressive and Non-autoregressive training method. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION investigates possible reasons behind the performance gap between AR and NAR models and proposes the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems.

In summary, our proposed MPNet pre-training method unifies the advantages of MLM and PLM while addressing their limitations. We demonstrate the effectiveness of MPNet in improving machine learning performance on various downstream benchmark tasks. Our work contributes to the development of pre-training models and provides a new approach for NLP tasks.