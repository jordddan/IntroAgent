Earthformer: Exploring Space-Time Transformers for
Earth System Forecasting

Zhihan Gao⇤
Hong Kong University of Science and Technology
zhihan.gao@connect.ust.hk

Xingjian Shi†
Amazon Web Services
xjshi@amazon.com

Hao Wang
Rutgers University
hw488@cs.rutgers.edu

Yi Zhu
Amazon Web Services
yzaws@amazon.com

Yuyang Wang
Amazon Web Services
yuyawang@amazon.com

Mu Li
Amazon Web Services
mli@amazon.com

Dit-Yan Yeung
Hong Kong University of Science and Technology
dyyeung@cse.ust.hk

Abstract

Conventionally, Earth system (e.g., weather and climate) forecasting relies on
numerical simulation with complex physical models and hence is both expensive
in computation and demanding on domain expertise. With the explosive growth of
spatiotemporal Earth observation data in the past decade, data-driven models that
apply Deep Learning (DL) are demonstrating impressive potential for various Earth
system forecasting tasks. The Transformer as an emerging DL architecture, despite
its broad success in other domains, has limited adoption in this area. In this paper,
we propose Earthformer, a space-time Transformer for Earth system forecasting.
Earthformer is based on a generic, ﬂexible and efﬁcient space-time attention block,
named Cuboid Attention. The idea is to decompose the data into cuboids and apply
cuboid-level self-attention in parallel. These cuboids are further connected with a
collection of global vectors. We conduct experiments on the MovingMNIST dataset
and a newly proposed chaotic N -body MNIST dataset to verify the effectiveness
of cuboid attention and ﬁgure out the best design of Earthformer. Experiments on
two real-world benchmarks about precipitation nowcasting and El Niño/Southern
Oscillation (ENSO) forecasting show that Earthformer achieves state-of-the-art
performance.

1

Introduction

The Earth is a complex system. Variabilities of the Earth system, ranging from regular events like
temperature ﬂuctuation to extreme events like drought, hail storm, and El Niño/Southern Oscillation
(ENSO), impact our daily life. Among all the consequences, Earth system variabilities can inﬂuence
crop yields, delay airlines, cause ﬂoods and forest ﬁres. Precise and timely forecasting of these vari-
abilities can help people take necessary precautions to avoid crisis, or better utilize natural resources
such as wind and solar energy. Thus, improving forecasting models for Earth variabilities (e.g.,
weather and climate) has a huge socioeconomic impact. Despite its importance, the operational
weather and climate forecasting systems have not fundamentally changed for almost 50 years [34].
These operational models, including the state-of-the-art High Resolution Ensemble Forecast (HREF)

⇤Work done while being an intern at Amazon Web Services. †Contact person.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

Figure 1: Example Vertically Integrated Liquid (VIL) observation sequence from the Storm EVent
ImageRy (SEVIR) dataset. The observation intensity is mapped to pixel value of the range 0-255.
The larger value indicates the higher precipitation intensity.

rainfall nowcasting model used in National Oceanic and Atmospheric Administration (NOAA) [32],
rely on meticulous numerical simulation of physical models. Such simulation-based systems in-
evitably fall short in the ability to incorporate signals from newly emerging geophysical observation
systems [12], or take advantage of the Petabytes-scale Earth observation data [43].

As an appealing alternative, deep learning (DL) is offering a new approach for Earth system forecast-
ing [34]. Instead of explicitly incorporating physical rules, DL-based forecasting models are trained
on the Earth observation data [36]. By learning from a large amount of observations, DL models
are able to ﬁgure out the system’s intrinsic physical rules and generate predictions that outperform
simulation-based models [9]. Such technique has demonstrated success in several applications,
including precipitation nowcasting [32, 6] and ENSO forecasting [15]. Because the Earth system is
chaotic [21], high-dimensional, and spatiotemporal, designing appropriate DL architecture for model-
ing the system is particularly challenging. Previous works relied on the combination of Recurrent
Neural Networks (RNN) and Convolutional Neural Networks (CNN) [36, 37, 43, 13, 45]. These two
architectures impose temporal and spatial inductive biases that help capturing spatiotemporal patterns.
However, as a chaotic system, variabilities of the Earth system, such as rainfall and ENSO, are highly
sensitive to the system’s initial conditions and can respond abruptly to internal changes. It is unclear
whether the inductive biases in RNN and CNN models still hold for such complex systems.

On the other hand, recent years have witnessed major breakthroughs in DL brought by the wide
adoption of Transformer. The model was originally proposed for natural language processing [42, 7],
and later has been extended to computer vision [8, 22], multimodal text-image generation [31],
graph learning [52], etc. Transformer relies on the attention mechanism to capture data correlations
and is powerful at modeling complex and long-range dependencies, both of which appear in Earth
systems (See Fig. 1 for an example of Earth observation data). Despite being suitable for the problem,
Transformer sees limited adoption for Earth system forecasting. Naively applying the Transformer
architecture is infeasible because the O(N 2) attention mechanism is too computationally expensive
for the high-dimensional Earth observation data. How to design a space-time Transformer that is
good at predicting the future of the Earth systems is largely an open problem to the community.

In this paper, we propose Earthformer, a space-time Transformer for Earth system forecasting. To
better explore the design of space-time attention, we propose Cuboid Attention, which is a generic
building block for efﬁcient space-time attention. The idea is to decompose the input tensor to
non-overlapping cuboids and apply cuboid-level self-attention in parallel. Since we limit the O(N 2)
self-attention inside the local cuboids, the overall complexity is greatly reduced. Different types
of correlations can be captured via different cuboid decompositions. By stacking multiple cuboid
attention layers with different hyperparameters, we are able to subsume several previously proposed
video Transformers [19, 23, 4] as special cases, and also come up with new attention patterns that
were not studied before. A limitation of this design is the lack of a mechanism for the local cuboids to
communicate with each other. Thus, we introduce a collection of global vectors that attend to all the
local cuboids, thereby gathering the overall status of the system. By attending to the global vectors,
the local cuboids can grasp the general dynamics of the system and share information with each other.

To verify the effectiveness of cuboid attention and ﬁgure out the best design under the Earth system
forecasting scenario, we conducted extensive experiments on two synthetic datasets: the MovingM-
NIST [36] dataset and a newly proposed N -body MNIST dataset. Digits in the N -body MNIST
follow the chaotic 3-body motion pattern [25], which makes the dataset not only more challenging
than MovingMNIST but also more relevant to Earth system forecasting. The synthetic experiments
reveal the following ﬁndings: 1) stacking cuboid attention layers with the Axial attention pattern is
both efﬁcient and effective, achieving the best overall performance, 2) adding global vectors provides
consistent performance gain without increasing the computational cost, 3) adding hierarchy in the
encoder-decoder architecture can improve performance. Based on these ﬁndings, we ﬁgured out
the optimal design for Earthformer and made comparisons with other baselines on the SEVIR [43]

2

benchmark for precipitation nowcasting and the ICAR-ENSO dataset [15] for ENSO forecasting.
Experiments show that Earthformer achieves state-of-the-art (SOTA) performance on both tasks.

2 Related Work

Deep learning architectures for Earth system forecasting. Conventional DL models for Earth
system forecasting are based on CNN and RNN. U-Net with either 2D CNN or 3D CNN have
been used for precipitation nowcasting [43], Seasonal Arctic Sea ice prediction [1], and ENSO
forecasting [15]. Shi et al. [36] proposed the ConvLSTM network that combines CNN and LSTM
for precipitation nowcasting. Wang et al. [45] proposed PredRNN which adds the spatiotemporal
memory ﬂow structure to ConvLSTM. To better learn long-term high-level relations, Wang et al. [44]
proposed E3D-LSTM that integrates 3D CNN to LSTM. To disentangle PDE dynamics from unknown
complementary information, PhyDNet [13] incorporates a new recurrent physical cell to perform
PDE-constrained prediction in latent space. Espeholt et al. [9] proposed MetNet-2 that outperforms
HREF for forecasting precipitation. The architecture is based on ConvLSTM and dilated CNN.
Very recently, there are works that tried to apply Transformer for solving Earth system forecasting
problems. Pathak et al. [28] proposed the FourCastNet for global weather forecasting, which is
based on Adaptive Fourier Neural Operators (AFNO) [14]. Bai et al. [3] proposed Rainformer
for precipitation nowcasting, which is based on an architecture that combines CNN and Swin-
Transformer [22]. In our experiments, we can see that Earthformer outperforms Rainformer.

Inspired by the success of ViT [8] for image
Space-time Transformers for video modeling.
classiﬁcation, space-time Transformer is adopted for improved video understanding. In order to
bypass the huge memory consumption brought by joint spatiotemporal attention, several pioneering
work proposed efﬁcient alternatives, such as divided attention [4], axial attention [19, 4], factorized
encoder [27, 2] and separable attention [54]. Beyond minimal adaptation from ViT, some recent
work introduced more prior to the design of space-time transformers, including trajectory [29],
multi-scale [23, 11] and multi-view [49]. However, no prior work focuses on exploring the design of
space-time Transformers for Earth system forecasting.

Global and local attention in vision Transformers. To make self-attention more efﬁcient in terms
of both memory consumption and speed, recent works have adapted the essence of CNN to perform
local attention in transformers [16, 52]. HaloNets [41] develops a new self-attention model family
consisting of simple local self-attention and convolutional hybrids, which outperform both CNN
and vanilla ViT on a range of downstream vision tasks. GLiT [5] introduces a locality module and
uses neural architecture search to ﬁnd an efﬁcient backbone. Focal transformer [51] proposes focal
self-attention that can incorporate both ﬁne-grained local and coarse-grained global interactions.
However, these architectures are not directly applicable to spatiotemporal forecasting. Besides, they
are also different from our design because we keep K global vectors to summarize the statistics of
the dynamic system and connect the local cuboids; experiments show that such a global vector design
is crucial to successful spatiotemporal forecasting.

3 Model

Xi 2

RH
⇥
YT +i 2

Similar to previous works [36, 43, 3], we formulate Earth system forecasting as a spatiotempo-
ral sequence forecasting problem. The Earth observation data, such as radar echo maps from
NEXRAD [17] and climate data from CIMP6 [10], are represented as a spatiotemporal sequence
Cin . Based on these observations, the model predicts the K-step-ahead future
Xi]T
[
i=1,
Cout. Here, H, W denote the spatial resolution, and Cin, Cout denote the
YT +i]K
[
⇥
number of measurements available at each space-time coordinate from the input and target sequences,
respectively. As illustrated in Fig. 2, our proposed Earthformer is a hierarchical Transformer encoder-
decoder based on Cuboid Attention. The input observations are encoded as a hierarchy of hidden
states and then decoded to the prediction target. In what follows, we will present the detailed design
of cuboid attention and the hierarchical encoder-decoder architecture adopted in Earthformer.

W
⇥
RH

i=1,

W

⇥

3.1 Cuboid Attention

Compared with images and text, spatiotemporal data in Earth systems usually have higher dimension-
ality. As a consequence, applying Transformers to this task is challenging. For example, for a 3D

3

!×

Cuboid  Attention  Block ×	#

Cuboid  Attention  Block ×	#

!	×

Initial Positional Embedding

Downsample

Upsample

Cuboid  Attention  Block ×	#

Cuboid  Attention  Block ×	#

2D-CNN  + Downsample

2D-CNN  + Upsample

⋯

'
!$ $%&

⋯

"' ($ $%&

)

Figure 2: Illustration of the Earthformer architecture. It
is a hierarchical Transformer encoder-decoder based on
cuboid attention. The input sequence has length T and
the target sequence has length K. “
D” means to stack
D cuboid attention blocks with residual connection.
“M

” means to have M layers of hierarchies.

⇥

⇥

G

I(+)

(+)
I423

G423

Decompose

Attend

Merge

H

Attend

H423

Figure 3: Illustration of the cuboid atten-
tion layer with global vectors.

(a) strategy=“local”

shift=(0, 0, 0)

(b) strategy=“dilated”
shift=(0, 0, 0)

(c) strategy=“local”

shift = (0, 1, 1)

Figure 4: Illustration of cuboid decomposition strategies when the input shape is (T, H, W ) =
(6, 4, 4), and cuboid size (bT , bH , bW ) = (3, 2, 2). Cells that have the same color belong to the
same cuboid and will attend to each other. shift = (0, 1, 1) shifts the cuboid decomposition by
1 pixel along height and width dimensions. strategy = “local” means to aggregate contiguous
(bT , bH , bW ) pixels as a cuboid. strategy = “dilated” means to aggregate pixels every
,

(

T
bT e

d

H
bH e

d

W
bW e

d

) steps along time (height, width) dimension. (Best viewed in color).

tensor with shape (T, H, W ), the complexity of the vanilla self-attention is O(T 2H 2W 2) and can be
computationally infeasible. Previous literature proposed various structure-aware space-time attention
mechanisms to reduce the complexity [19, 23, 4].

These space-time attention mechanisms share the common design of stacking multiple elementary
attention layers that focus on different types of data correlations (e.g., temporal correlation and
spatial correlation). Steming from this observation, we propose the generic cuboid attention layer
that involves three steps: “decompose”, “attend”, and “merge”.

Decompose. We ﬁrst decompose the input spatiotemporal tensor
of cuboids

.

x(n)
{

}

RT

H

W

C into a sequence

⇥

⇥

⇥

X2

x(n)

{

}

= Decompose(

X

, cuboid_size, strategy, shift),

(1)

}

is

the size of

where cuboid_size = (bT , bH , bW )
2
“local”, “dilated”
controls whether to adopt the local decomposition strategy or the dilated
{
decomposition strategy [4], shift = (sT , sH , sW ) is the window shift offset [22]. Fig. 4 provides
three examples showing how an input tensor will be decomposed following different hyperparameters
T
of Decompose(
. To simplify the
bT ed
notation, we assume that T, H, W are divisible by bT , bH , bW . In the implementation, we pad the
input tensor if it is not divisible.

). There are a total number of

the local cuboid,

cuboids in

x(n)
{

H
bH ed

W
bW e

strategy

}

d

·

4

Assume x(n) is the (nT , nH , nW )-th cuboid in
mapped to the (i0, j0, k0)-th element of
is “dilated”.

X

. The (i, j, k)-th element of x(n) can be
}
via Eqn. 2 if the strategy is “local” or Eqn. 3 if the strategy

x(n)
{

i0

j0

k0

$

$

$

sT + bT (nT  
sH + bH (nH  
sW + bW (nW  

1) + i mod T

1) + j mod H

1) + k mod W

(2)

i0

j0

k0

$

$

$

 

sT + bT (i
sH + bH (j
sW + bW (k

 

1) + nT mod T
1) + nH mod H
1) + nW mod W

 

(3)

Since the mapping is bijective, one can then map the elements from
operation.

X

to

x(n)
{

}

via the inverse

Attend. After decomposing the input tensor into a sequence of non-overlapping cuboids
we apply self-attention within each cuboid in parallel.

x(n)
{

,

}

x(n)
out = Attention⇥(x(n), x(n), x(n)), 1

n





N.

(4)

⇣

(WQQ)(WKK)T /pC

and value matrices Q, K,

and V of Attention✓(Q, K, V ) =
The query, key,
(WV V ) are all ﬂattened versions of x(n), and we unravel
Softmax
the resulting matrix back to a 3D tensor. WQ, WK and WV are linear projection weights and are
abbreviated together as ⇥. The self-attention parameter ⇥ are shared across all cuboids. The compu-
T
bT bH bW ),
tational complexity of the “attend” step is O
bT ed
which scales linearly with the cuboid size. Since the cuboid size can be much smaller than the size of
the input tensor, the layer is more efﬁcient than full attention.

(bT bH bW )2

H
bH ed

O(T HW

W
bW e

⇡

⌘

⇣

⌘

d

·

) is the inverse operation of Decompose(

Merge. Merge(
). The sequence of cuboids obtained after
·
are merged back to the original input shape to produce the ﬁnal output of
the attention step
cuboid attention, as shown in Eqn. 5. The mapping follows the same bijections in Eqn. 2 and Eqn. 3.

x(n)
out }

{

·

Xout = Merge(

x(n)
out }n, cuboid_size, strategy, shift).
{

(5)

We combine the “decompose”, “attend” and “merge” steps described in Eqn. 1,4,5 to construct the
generic cuboid attention as in Eqn. 6.

Xout = CubAttn⇥(

X

, cuboid_size, strategy, shift).

(6)

Explore cuboid attention patterns. By stacking multiple cuboid attention layers with different
choices of “cuboid_size”, “strategy” and “shift”, we are able to efﬁciently explore existing and
potentially more effective space-time attention. In this paper, we explore the cuboid attention patterns
as listed in Table 1. From the table, we can see that cuboid attention subsumes previously proposed
space-time attention methods like axial attention, video swin-Transformer, and divided space-time
attention. Also, we manually picked the patterns that are reasonable and not computationally
expensive as our search space. The ﬂexibility of cuboid attention allows us to conduct Neural
Architecture Search (NAS) to automatically search for a pattern but we will leave it as future work.

3.2 Global Vectors

One limitation of the previous formulation is that the cuboids do not communicate with each other.
This is undesirable because each cuboid is not capable of understanding the global dynamics of the
system. Thus, inspired by the [CLS] token adopted in BERT [7, 53], we propose to introduce a
C to help cuboids scatter and gather crucial global information.
collection of P global vectors
When each cuboid is performing the self-attention, the elements will not only attend to the other
. We revise Eqn. 4 to Eqn. 7
elements within the same cuboid but also attend to the global vectors
to enable local-global information exchange. We also use Eqn. 8 to update the global vectors
by
aggregating the information from all elements of the input tensor

RP

G2

G

G

⇥

.

x(n)
out = Attention⇥
⇣
Gout = Attention  (
G

x(n), Cat(x(n),

G
), Cat(

, Cat(

,

G

X

)) .

,

G

X

X
), Cat(x(n),

G

)

, 1

⌘

n





N.

(7)

(8)

5

Table 1: Conﬁgurations of the cuboid attention patterns explored in the paper. The input tensor
has shape (T, H, W ). If “shift” or “strategy” is not given, we use shift = (0, 0, 0) and strategy =
“local” by default. When stacking multiple cuboid attention layers, each layer will be coupled with
layer normalization layers and feed-forward network as in the Pre-LN Transformer [48]. The ﬁrst
row shows the conﬁguration of the generic cuboid attention.

Name

Conﬁgurations

Values

Generic Cuboid Attention

Axial

Divided Space-Time

Video-Swin P

M

⇥

Spatial Local-Dilate-M

Axial Space Dilate-M

cuboid_size
shift
strategy

cuboid_size

cuboid_size

cuboid_size
shift

cuboid_size
strategy

cuboid_size
strategy

(T1, H1, W1)
(P1, M1, M1)
“loc./dil.”

!
!
!
(T, 1, 1)

(T2, H2, W2)
(P2, M2, M2)
“loc./dil.”

(TL, HL, WL)
(PL, ML, ML)
“loc./dil.”

!· · ·!
!· · ·!
!· · ·!

(1, H, 1)

(1, 1, W)

!
(T, 1, 1)

!
(1, H, W)

(P, M, M)
(0, 0, 0)

!
( P , M , M )
(P/2, M/2, M/2)

!
!
(1, M, M)
“local”

(T, 1, 1)

“local”

!

!

(1, M, M)
“dilated”

!
!

(T, 1, 1)
“local”

(1, H/M, 1)
!
“dilated”

(1, H/M, 1)
“local”

!
!

!

(1, 1, W/M)
“dilated”

!

(1, 1, W/M)
“local”

!

!
!

Here, Cat(
the overall computation of the cuboid attention layer with global vectors as in Eqn. 9.

) ﬂattens and concatenates its input tensors. By combining Eqn. 1,7,8,5, we abbreviate

·

Xout = CubAttn⇥(
Gout = Attnglobal
(
G

X
,

G
).

,

, cuboid_size, strategy, shift),

 
The additional complexity caused by the global vectors is approximately O
. Given
that P is usually small (in our experiments, P is at most 8), the computational overhead induced by
the global structure is negligible. The architecture of the cuboid attention layer is illustrated in Fig. 3.

P + P 2

T HW

X

 

 

·

(9)

3.3 Hierarchical Encoder-Decoder Architecture

Earthformer adopts a hierarchical encoder-decoder architecture illustrated in Fig. 2. The hierarchical
architecture gradually encodes the input sequence to multiple levels of representations and generates
the prediction via a coarse-to-ﬁne procedure. Each hierarchy stacks D cuboid attention blocks.
The cuboid attention block in the encoder uses one of the patterns described in Table 1, and each
cuboid block in the decoder adopts the “Axial” pattern. To reduce the spatial resolution of the
input to cuboid attention layers, we include a pair of initial downsampling and upsampling modules
that consist of stacked 2D-CNN and Nearest Neighbor Interpolation (NNI) layers. Different from
other papers that adopt Transformer for video prediction [19, 30], we generate the predictions in a
non-auto-regressive fashion rather than an auto-regressive patch-by-patch fashion. This means that
our decoder directly generates the predictions from the initial learned positional embeddings. We also
conducted experiments with an auto-regressive decoder based on visual codebook [33]. However, the
auto-regressive decoder underperforms the non-auto-regressive decoder in terms of forecasting skill
scores. The comparison between non-auto-regressive decoder and auto-regressive decoder is shown
in Appendix C.

4 Experiments

We ﬁrst conducted experiments on two synthetic datasets, MovingMNIST and a newly proposed
N -body MNIST, to verify the effectiveness of Earthformer and conduct ablation study on our
design choices. Results on these two datasets lead to the following ﬁndings: 1) Among all patterns
listed in Table 1, “Axial” achieves the best overall performance; 2) Global vectors bring consistent
performance gain with negligible increase in computational cost; 3) Using a hierarchical coarse-
to-ﬁne structure can boost the performance. Based on these ﬁndings, we ﬁgured out the optimal
design of Earthformer and compared it with other state-of-the-art models on two real-world datasets:
SEVIR [43] and ICAR-ENSO2. On both datasets, Earthformer achieved the best overall performance.
The statistics of all the datasets used in the experiments are shown in Table 2. We normalized the
data to the range [0, 1] and trained all the models with the Mean-Squared Error (MSE) loss. More
implementation details are shown in Appendix A.

2Dataset available at https://tianchi.aliyun.com/dataset/dataDetail?dataId=98942

6

Table 2: Statistics of the datasets used in the experiments.

Dataset

train

MovingMNIST
8,100
N -body MNIST 20,000
SEVIR
35,718
5,205
ICAR-ENSO

Size
val

900
1,000
9,060
334

Seq. Len.
out
in

10
10
13
12

10
10
12
14

test

1,000
1,000
12,159
1,667

Spatial Resolution

H

64
64
384
24

⇥

⇥
⇥
⇥
⇥

W

64
64
384
48

Table 3: Ablation study on the importance of adopting a hierarchical encoder-decoder. We conducted
experiments on MovingMNIST. “Depth D” means the model stacks D cuboid attention blocks and
there is no hierarchical structure. “Depth D1, D2” means the model stacks D1 cuboid attention
blocks, applies the pooling layer, and stacks another D2 cuboid attention blocks.

Model

#Param. (M) GFLOPS

Depth 2
Depth 4
Depth 6
Depth 8

Depth 1,1
Depth 2,2
Depth 3,3
Depth 4,4

1.4
3.1
4.9
6.6

1.4
3.1
4.9
6.6

17.9
36.3
54.6
73.0

11.5
18.9
26.3
33.7

MSE

#
63.80
52.46
50.49
48.04

60.99
50.41
47.69
46.91

Metrics
MAE

#
140.6
114.8
110.0
104.6

135.7
106.9
100.1
101.5

SSIM

"
0.8324
0.8685
0.8738
0.8797

0.8388
0.8805
0.8873
0.8825

4.1 Experiments on Synthetic Datasets

MovingMNIST. We follow [38] to use the public MovingMNIST dataset3. The dataset contains
10,000 sequences. Each sequence shows 2 digits moving inside a 64
64 frame. We split the dataset
to use 8,100 samples for training, 900 samples for validation and 1,000 samples for testing. The task
is to predict the future 10 frames for each sequence conditioned on the ﬁrst 10 frames.

⇥

N -body MNIST. The Earth is a complex system in which an extremely large number of variables
interact with each other. Compared with the Earth system, the dynamics of the synthetic MovingM-
NIST dataset, in which the digits move independently with constant speed, is over-simpliﬁed. Thus,
achieving good performance on MovingMNIST does not imply that the model is capable of modeling
complex interactions in the Earth system. On the other hand, the real-world Earth observation data,
though experiencing rapid development, are still noisy and may not provide useful insights for model
development. Therefore, we extend MovingMNIST to N -body MNIST, where N digits are moving
with the N -body motion pattern inside a 64
64 frame. Each digit has its mass and is subjected to
the gravity from other digits. We choose N = 3 in the experiments so that the digits will follow the
chaotic 3-body motion [25]. The highly non-linear interactions in N -body MNIST make it much
more challenging than the original MovingMNIST. We generate 20,000 sequences for training, 1,000
for validation and 1,000 for testing. Perceptual examples of the dataset can be found at the ﬁrst two
rows of Fig. 5. In Appendix D, we demonstrate the chaotic behavior of N -body MNIST.

⇥

Hierarchical v.s. non-hierarchical. We choose “Axial” without global vectors as our cuboid
attention pattern and compare the performance of non-hierarchical and hierarchical architectures on
MovingMNIST. The ablation study on the importance of adopting a hierarchical encoder-decoder
is shown in Table 3. We can see that the hierarchical architecture has similar FLOPS with the non-
hierarchical architectures while being better in MSE. This observation is consistent as we increase
the depth until the performance saturates.

Cuboid pattern search. The design of cuboid attention greatly facilitates the search for optimal
space-time attention. We compare the patterns listed in Table 1 on both MovingMNIST and N -body
MNIST to investigate the effectiveness and efﬁciency of different space-time attention methods on
spatiotemporal forecasting tasks. Besides the previously proposed space-time attention methods, we
also include new conﬁgurations that are reasonable and not computationally expensive in our search
space. For each pattern, we also compare the variant that uses global vectors. Results are summarized
in Table 4. We ﬁnd that the “Axial” pattern is both effective and efﬁcient and adding global vectors
improves performance for all patterns while having similar FLOPS. We thus pick “Axial + global” as
the pattern in Earthformer when conducting experiments on real-world datasets.

3MovingMNIST: https://github.com/mansimov/unsupervised-videos

7

Table 4: Ablation study of different cuboid attention patterns and the effect of global vectors on
MovingMNIST and N -body MNIST. The variant that achieved the best performance is in boldface
while the second best is underscored. We also compared the performance of the cuboid attention
patterns with and without global vectors and highlight the better one with grey background.

Model

#Param. (M) GFLOPS

Axial
+ global F

DST
+ global

Video Swin 2x8
+ global

Video Swin 10x8
+ global

Spatial Local-Global 2
+ global

Spatial Local-Global 4
+ global

Axial Space Dilate 2
+ global

Axial Space Dilate 4
+ global

6.61
7.61

5.70
6.37

5.66
6.33

5.89
6.56

6.61
7.61

6.61
7.61

8.59
10.30

8.59
10.30

33.7
34.0

35.2
35.5

31.1
31.4

39.2
39.4

33.3
33.7

33.5
33.9

41.8
42.4

41.6
42.2

MovingMNIST

N -body MNIST

MSE

#
46.91
41.79

MAE

#
101.5
92.78

SSIM

"
0.8825
0.8961

MSE

#
15.89
14.82

MAE

#
41.38
39.93

SSIM

"
0.9510
0.9538

57.43
52.92

54.45
52.70

63.34
62.15

59.88
59.42

58.72
54.84

50.11
46.86

47.40
45.11

118.6
108.3

111.7
108.5

125.3
123.4

122.1
122.9

118.5
115.5

104.4
98.95

99.31
95.98

0.8623
0.8760

0.8715
0.8766

0.8525
0.8541

0.8572
0.8565

0.8600
0.8585

0.8814
0.8884

0.8865
0.8928

18.24
17.77

19.89
19.53

23.35
22.81

23.24
21.88

21.02
19.82

15.97
15.73

19.49
17.91

45.88
45.84

49.02
48.43

53.17
52.94

54.63
52.49

49.82
48.12

42.19
41.85

51.04
46.35

0.9435
0.9433

0.9374
0.9389

0.9274
0.9293

0.9263
0.9305

0.9344
0.9371

0.9494
0.9510

0.9352
0.9440

Table 5: Comparison of Earthformer with baselines on MovingMNIST and N -body MNIST.

Model

#Param. (M) GFLOPS

UNet [43]
ConvLSTM [36]
PredRNN [45]
PhyDNet [13]
E3D-LSTM [44]
Rainformer [3]

Earthformer w/o global
Earthformer

16.6
14.0
23.8
3.1
12.9
19.2

6.6
7.6

0.9
30.1
232.0
15.3
302.0
1.2

33.7
34.0

MovingMNIST

MSE

#
110.4
62.04
52.07
58.70
55.31
85.83

MAE

#
249.4
126.9
108.9
124.1
101.6
189.2

SSIM

"
0.6170
0.8477
0.8831
0.8350
0.8821
0.7301

N -body MNIST
MAE

SSIM

MSE

#
38.90
32.15
21.76
28.97
22.98
38.89

#
94.29
72.64
54.32
78.66
62.52
96.47

"
0.8260
0.8886
0.9288
0.8206
0.9131
0.8036

46.91
41.79

101.5
92.78

0.8825
0.8961

15.89
14.82

41.38
39.93

0.9510
0.9538

Comparison to the state of the art. We evaluate six spatiotemporal forecasting algorithms:
UNet [43], ConvLSTM [36], PredRNN [45], PhyDNet [13], E3D-LSTM [44] and Rainformer [3].
The results are in Table 5. Note that the MovingMNIST performance on several papers [13] is
obtained by training the model with on-the-ﬂy generated digits while we pre-generate the digits and
train all models on a ﬁxed dataset. Comparing the numbers in the table with numbers shown in these
papers are not fair. We train all baselines from scratch on both MovingMNIST and N -body MNIST
using the default hyperparameters and conﬁgurations in their ofﬁcially released code4.

Qualitative results on N -body MNIST. Fig. 5 shows the generation results of different methods on
a sample sequence from the N -body MNIST test set. The qualitative example demonstrates that our
Earthformer is capable of learning long-range interactions among digits and correctly predicting their
future motion trajectories. Also, we can see that Earthformer is able to more accurately predict the
position of the digits with the help of global vectors. On the contrary, none of the baseline algorithms
that achieved solid performance on MovingMNIST gives the correct and precise position of the digit
“0” in the last frame. They either predict incorrect motion trajectories (PredRNN and E3D-LSTM), or
generate highly blurry predictions (Rainformer, UNet and PhyDNet) to accommodate the uncertainty
about the future.

4.2 SEVIR Precipitation Nowcasting

Storm EVent ImageRy (SEVIR) [43] is a spatiotemporally aligned dataset containing over 10,000
384 km image sequences spanning over 4 hours.
weather events. Each event consists of 384 km
⇥
Images in SEVIR were sampled and aligned across ﬁve different data types: three channels (C02,

4Except for Rainformer which originally has 212M parameters and thus suffers from overﬁtting severely.

8

Figure 5: A set of examples showing the perceptual quality of the predictions on the N -body MNIST
test set. From top to bottom: input frames, target frames, predictions by Rainformer [3], UNet [43],
ConvLSTM [36], PhyDNet [13], E3D-LSTM [44], PredRNN [45], Earthformer without using global
vectors, Earthformer. The results are sorted according to the MSE.
Table 6: Performance comparison on SEVIR. We include Critical Success Index (CSI) besides
MSE as evaluation metrics. The CSI, a.k.a intersection over union (IOU), is calculated at different
precipitation thresholds and denoted as CSI-thresh.

Model

#Param. (M) GFLOPS

Persistence
UNet [43]
ConvLSTM [36]
PredRNN [45]
PhyDNet [13]
E3D-LSTM [44]
Rainformer [3]

Earthformer w/o global
Earthformer

-
16.6
14.0
46.6
13.7
35.6
184.0

13.1
15.1

-
33
527
328
701
523
170

257
257

CSI-M

"
0.2613
0.3593
0.4185
0.4080
0.3940
0.4038
0.3661

0.4356
0.4419

CSI-219

CSI-181

CSI-160

"

"

"

CSI-133

"

Metrics

0.0526
0.0577
0.1288
0.1312
0.1288
0.1239
0.0831

0.1572
0.1791

0.0969
0.1580
0.2482
0.2324
0.2309
0.2270
0.1670

0.2716
0.2848

0.1278
0.2157
0.2928
0.2767
0.2708
0.2675
0.2167

0.3138
0.3232

0.2155
0.3274
0.4052
0.3858
0.3720
0.3825
0.3438

0.4214
0.4271

CSI-74

"
0.4705
0.6531
0.6793
0.6713
0.6556
0.6645
0.6585

0.6859
0.6860

CSI-16

"
0.6047
0.7441
0.7569
0.7507
0.7059
0.7573
0.7277

0.7637
0.7513

MSE (10 

3)

#

11.5338
4.1119
3.7532
3.9014
4.8165
4.1702
4.0272

3.7002
3.6957

C09, C13) from the GOES-16 advanced baseline imager, NEXRAD Vertically Integrated Liquid
(VIL) mosaics, and GOES-16 Geostationary Lightning Mapper (GLM) ﬂashes. The SEVIR bench-
mark supports scientiﬁc research on multiple meteorological applications including precipitation
nowcasting, synthetic radar generation, front detection, etc. We adopt SEVIR for benchmarking
precipitation nowcasting, i.e., to predict the future VIL up to 60 minutes (12 frames) given 65 minutes
context VIL (13 frames). Fig. 1 shows an example of VIL observation sequences in SEVIR.

Besides MSE, we also include the Critical Success Index (CSI), which is commonly used
in precipitation nowcasting and is deﬁned as CSI =
#Hits+#Misses+#F.Alarms . To count the
#Hits (truth=1, pred=1), #Misses (truth=1, pred=0) and #F.Alarms (truth=0, pred=1), the
prediction and the ground-truth are rescaled back to the range 0-255 and binarized at thresholds
[16, 74, 133, 160, 181, 219]. We report CSI at different thresholds and also their mean CSI-M.

#Hits

SEVIR is much larger than MovingMNIST and N -body MNIST and has higher resolution. We thus
slightly adjust the conﬁgurations of baselines based on those for MovingMNIST for fair comparison.
Detailed conﬁgurations are shown in Appendix A. The experiment results are listed in Table 6. Earth-
former consistently outperforms baselines on almost all metrics and brings signiﬁcant performance
gain especially at high thresholds like CSI-219, which are more valued by the communities.

9

Table 7: Performance comparison on ICAR-ENSO. C-Nino3.4-M and C-Nino3.4-WM are the mean
and the weighted mean of the correlation skill C Nino3.4 over K = 12 forecasting steps. C-Nino3.4-WM
assigns more weights to longer-term prediction scores. MSE is calculated between the spatiotemporal
SST anomalies prediction and the corresponding ground-truth.

Model

#Param. (M) GFLOPS

Persistence
UNet [43]
ConvLSTM [36]
PredRNN [45]
PhyDNet [13]
E3D-LSTM [44]
Rainformer [3]

Earthformer w/o global
Earthformer

-
12.1
14.0
23.8
3.1
12.9
19.2

6.6
7.6

-
0.4
11.1
85.8
5.7
99.8
1.3

23.6
23.9

C-Nino3.4-M

"

Metrics
C-Nino3.4-WM

MSE (10 

4)

#

"

0.3221
0.6926
0.6955
0.6492
0.6646
0.7040
0.7106

0.7239
0.7329

0.447
2.102
2.107
1.910
1.965
2.125
2.153

2.214
2.259

4.581
2.868
2.657
3.044
2.708
3.095
3.043

2.550
2.546

4.3

ICAR-ENSO Sea Surface Temperature Anomalies Forecasting

El Niño/Southern Oscillation (ENSO) has a wide range of associations with regional climate extremes
and ecosystem impacts. ENSO sea surface temperature (SST) anomalies forecasting for lead times
up to one year (12 steps) is a valuable and challenging problem. Nino3.4 index, which is the area-
averaged SST anomalies across a certain area (170 -120 W, 5 S-5 N) of the Paciﬁc, serves as a
crucial indicator of this climate event. The forecast quality is evaluated by the correlation skill [15]
of the three-month-moving-averaged Nino3.4 index C Nino3.4 =
RK
 
¯X)2
K is the ground-truth of K-step Nino3.4

calculated on the whole test set of size N , where Y
index, X

P
K is the corresponding prediction of Nino3.4 index.

¯Y )2 2

 
N (Y

RN

¯X)(Y

N (X
P

N (X

¯Y )

p

2

P

 

 

⇥

⇥

RN

2

ICAR-ENSO consists of historical climate observation and stimulation data provided by Institute for
Climate and Application Research (ICAR). We forecast the SST anomalies up to 14 steps (2 steps
more than one year for calculating three-month-moving-average) given a context of 12 steps of SST
anomalies observations. Table 7 compares the performance of our Earthformer with baselines on the
ICAR-ENSO dataset. We report the mean correlation skill C-Nino3.4-M = 1
and the
K
weighted mean correlation skill C-Nino3.4-WM = 1
over K = 12 forecasting steps5,
K
as well as the MSE between the spatiotemporal SST anomalies prediction and the corresponding
ground-truth. We can ﬁnd that Earthformer consistently outperforms the baselines in all concerned
evaluation metrics and that using global vectors further improves the performance.

k C Nino3.4
k

C Nino3.4

k ak·

P

P

k

5 Conclusions and Broader Impact

In this paper, we propose Earthformer, a space-time Transformer for Earth system forecasting.
Earthformer is based on a generic and efﬁcient building block called Cuboid Attention. It achieves
SOTA on MovingMNIST, our newly proposed N -body MNIST, SEVIR, and ICAR-ENSO.

Our work has certain limitations. The ﬁrst one is that Earthformer is a deterministic model that does
not model uncertainty. This may result in predicting the average of all plausible futures, causing the
model to generate blurry predictions of low perceptual quality and be lack of valuable small-scale
details. In fact, the community lacks appropriate metrics that measure the uncertainty component in
Earth system forecasting models. Extending Earthformer to a probabilistic forecasting model can be
an exciting future direction. We include more detailed discussions and preliminary experiments about
handling uncertainty in Appendix C. The second one is that the model is purely data-driven and does
not take advantage of the physical knowledge of the Earth system. Recent studies on adding physical
constraints [20, 26] and ensembling the predictions from a data-driven model and a physics-based
model [32] imply that it is an active and promising research direction to pursue. We plan to study
how to incorporate physical knowledge into Earthformer in the future.

Acknowledgments and Disclosure of Funding

This work has been made possible by a Research Impact Fund project (R6003-21) and an Innovation
and Technology Fund project (ITS/004/21FP) funded by the Hong Kong Government.

5ak = bk

·

ln k, where bk = 1.5, for k

4; bk = 2, for 4 < k





11; bk = 3, for k > 11.

10

References

[1] Tom R Andersson, J Scott Hosking, María Pérez-Ortiz, Brooks Paige, Andrew Elliott, Chris
Russell, Stephen Law, Daniel C Jones, Jeremy Wilkinson, Tony Phillips, et al. Seasonal Arctic
sea ice forecasting with probabilistic deep learning. Nature communications, 12(1):1–12, 2021.
[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luvci´c, and Cordelia
Schmid. ViViT: A video vision transformer. In International Conference on Computer Vision
(ICCV), 2021.

[3] Cong Bai, Feng Sun, Jinglin Zhang, Yi Song, and Shengyong Chen. Rainformer: Features
extraction balanced network for radar-based precipitation nowcasting. IEEE Geoscience and
Remote Sensing Letters, 19:1–5, 2022.

[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for

video understanding. arXiv preprint arXiv:2102.05095, 2(3):4, 2021.

[5] Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, Ming Sun, Junjie Yan, and
Wanli Ouyang. GLiT: Neural architecture search for global and local image transformer. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12–21,
2021.

[6] Christian Schroeder de Witt, Catherine Tong, Valentina Zantedeschi, Daniele De Martini,
Freddie Kalaitzis, Matthew Chantry, Duncan Watson-Parris, and Piotr Bilinski. RainBench:
towards global precipitation forecasting from satellite imagery. In AAAI, 2021.

[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.

[9] Lasse Espeholt, Shreya Agrawal, Casper Sønderby, Manoj Kumar, Jonathan Heek, Carla
Bromberg, Cenk Gazen, Jason Hickey, Aaron Bell, and Nal Kalchbrenner. Skillful twelve hour
precipitation forecasts using large context neural networks. arXiv preprint arXiv:2111.07470,
2021.

[10] Veronika Eyring, Sandrine Bony, Gerald A Meehl, Catherine A Senior, Bjorn Stevens, Ronald J
Stouffer, and Karl E Taylor. Overview of the coupled model intercomparison project phase 6
(CMIP6) experimental design and organization. Geoscientiﬁc Model Development, 9(5):1937–
1958, 2016.

[11] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and

Christoph Feichtenhofer. Multiscale vision transformers. In ICCV, 2021.

[12] Steven J Goodman, Timothy J Schmit, Jaime Daniels, and Robert J Redmon. The GOES-R

series: a new generation of geostationary environmental satellites. Elsevier, 2019.

[13] Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors
for unsupervised video prediction. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 11474–11484, 2020.

[14] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan
Catanzaro. Adaptive fourier neural operators: Efﬁcient token mixers for transformers. arXiv
preprint arXiv:2111.13587, 2021.

[15] Yoo-Geun Ham, Jeong-Hwan Kim, and Jing-Jia Luo. Deep learning for multi-year ENSO

forecasts. Nature, 573(7775):568–572, 2019.

[16] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in

transformer. NeurIPS, 2021.

[17] William H Heiss, David L McGrew, and Dale Sirmans. NEXRAD: next generation weather

radar (wsr-88d). Microwave Journal, 33(1):79–89, 1990.

[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint

arXiv:1606.08415, 2016.

11

[19] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in

multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.

[20] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney.
Characterizing possible failure modes in physics-informed neural networks. Advances in Neural
Information Processing Systems, 34:26548–26560, 2021.

[21] Christophe Letellier. Chaos in nature, volume 94. World Scientiﬁc, 2019.

[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 10012–10022, 2021.

[23] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin

transformer. arXiv preprint arXiv:2106.13230, 2021.

[24] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are GANs

created equal? a large-scale study. In NeurIPS, 2018.

[25] Valtonen MJ, Mauri Valtonen, and Hannu Karttunen. The three-body problem. Cambridge

University Press, 2006.

[26] Geoffrey Négiar, Michael W Mahoney, and Aditi S Krishnapriyan. Learning differentiable

solvers for systems with hard constraints. arXiv preprint arXiv:2207.08675, 2022.

[27] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network.

arXiv preprint arXiv:2102.00719, 2021.

[28] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay,
Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al.
FourCastNet: A global data-driven high-resolution weather model using adaptive fourier neural
operators. arXiv preprint arXiv:2202.11214, 2022.

[29] Mandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan Misra Florian Metze, Christoph
Feichtenhofer, Andrea Vedaldi, and João F. Henriques. Keeping your eye on the ball: Trajec-
tory attention in video transformers. In Advances in Neural Information Processing Systems
(NeurIPS), 2021.

[30] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev.

Latent video transformer. arXiv preprint arXiv:2006.10704, 2020.

[31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark
Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on
Machine Learning, pages 8821–8831. PMLR, 2021.

[32] Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski,
Megan Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skilful pre-
cipitation nowcasting using deep generative models of radar. Nature, 597(7878):672–677,
2021.

[33] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images

with VQ-VAE-2. Advances in neural information processing systems, 32, 2019.

[34] Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, Joachim Denzler, Nuno
Carvalhais, et al. Deep learning and process understanding for data-driven earth system science.
Nature, 566(7743):195–204, 2019.

[35] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. PixelCNN++: A PixelCNN
implementation with discretized logistic mixture likelihood and other modiﬁcations. In ICLR,
2017.

[36] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun
Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting.
In NeurIPS, volume 28, 2015.

[37] Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and
Wang-chun Woo. Deep learning for precipitation nowcasting: A benchmark and a new model.
In NeurIPS, volume 30, 2017.

[38] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video

representations using LSTMs. In ICML, pages 843–852. PMLR, 2015.

12

[39] Byron D Tapley, Srinivas Bettadpur, John C Ries, Paul F Thompson, and Michael M Watkins.
GRACE measurements of mass variability in the earth system. science, 305(5683):503–505,
2004.

[40] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in

neural information processing systems, 30, 2017.

[41] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and
Jonathon Shlens. Scaling local self-attention for parameter efﬁcient visual backbones. In CVPR,
2021.

[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, volume 30, 2017.
[43] Mark Veillette, Siddharth Samsi, and Chris Mattioli. SEVIR: A storm event imagery dataset for
deep learning applications in radar and satellite meteorology. Advances in Neural Information
Processing Systems, 33:22009–22019, 2020.

[44] Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, and Li Fei-Fei. Eidetic
3D LSTM: A model for video prediction and beyond. In International conference on learning
representations, 2018.

[45] Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, Philip Yu, and Mingsheng
Long. PredRNN: A recurrent neural network for spatiotemporal predictive learning. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2022.

[46] Dirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models.

In International Conference on Learning Representations, 2019.

[47] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference

on computer vision (ECCV), pages 3–19, 2018.

[48] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.
In International Conference on Machine Learning, pages 10524–10533. PMLR, 2020.
[49] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and Cordelia

Schmid. Multiview transformers for video recognition. In CVPR, 2022.

[50] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. VideoGPT: Video generation

using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.

[51] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng
Gao. Focal self-attention for local-global interactions in vision transformers. In NeurIPS, 2021.
[52] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,
and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in
Neural Information Processing Systems, 34, 2021.

[53] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird:
Transformers for longer sequences. Advances in Neural Information Processing Systems,
33:17283–17297, 2020.

[54] Yanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu, Biagio Brattoli, Hao Chen, Ivan
Marsic, and Joseph Tighe. Vidtr: Video transformer without convolutions. In Proceedings of
the IEEE/CVF International Conference on Computer Vision, pages 13577–13587, 2021.

13

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes] See Section 5.
(c) Did you discuss any potential negative societal impacts of your work? [No] Exploring

Earth system forecasting has no negative societal impacts.

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-

mental results (either in the supplemental material or as a URL)? [Yes]

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes]

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [No]

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes]

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [Yes]
(c) Did you include any new assets either in the supplemental material or as a URL? [No]
(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [N/A]

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

14

