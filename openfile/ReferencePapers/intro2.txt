
Introduction

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 22, 27, 28, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.

AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 27, 28]. Speciﬁcally, given a text sequence x = (x1, · · · , xT ), AR language
modeling factorizes the likelihood into a forward product p(x) = (cid:81)T
t=1 p(xt | x<t) or a backward
one p(x) = (cid:81)1
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.

In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].

Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.

• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.

• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.

• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.

• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a
wide spectrum of problems including GLUE language understanding tasks, reading comprehension
tasks like SQuAD and RACE, text classiﬁcation tasks such as Yelp and IMDB, and the ClueWeb09-B
document ranking task.

Related Work The idea of permutation-based AR modeling has been explored in [32, 12], but there
are several key differences. Firstly, previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction
distribution, XLNet incorporates the target position into the hidden state via two-stream attention
while previous permutation-based AR models relied on implicit position awareness inherent to their
MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that
“orderless” does not mean that the input sequence can be randomly permuted but that the model
allows for different factorization orders of the distribution.

Another related idea is to perform autoregressive denoising in the context of text generation [11],
which only considers a ﬁxed order though.
