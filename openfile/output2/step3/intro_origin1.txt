Machine learning has become an essential tool in various fields, including natural language processing (NLP). In recent years, pre-trained models have shown remarkable success in NLP tasks, such as machine translation, summarization, and question generation. However, traditional pre-trained models optimize one-step-ahead prediction, which may lead to overfitting on strong local correlations and fail to plan for future tokens. 

To address this issue, this paper introduces a new large-scale pre-trained Seq2Seq model called ProphetNet, which utilizes a novel self-supervised objective future n-gram prediction. The model is optimized by predicting the next n tokens simultaneously based on previous context tokens at each time step, encouraging the model to plan for future tokens and prevent overfitting on strong local correlations. Additionally, the paper extends the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

The paper pre-trains ProphetNet on two scale pre-trained datasets and achieves new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, the paper fine-tunes ProphetNet on several NLG tasks and achieves the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

The related works in this field include Universal Conditional Masked Language Pre-training, BANG, JANUS, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, MPNet, XLNet, and Attention Is All You Need. These works have proposed various pre-training models and methods to improve the performance of NLP tasks. However, ProphetNet stands out by introducing a novel self-supervised objective future n-gram prediction and extending the two-stream self-attention to n-stream self-attention. 

The rest of the paper is organized as follows. Section 2 provides a detailed description of the proposed ProphetNet model and its training process. Section 3 presents the experimental results on various NLG tasks. Section 4 discusses the related works in this field. Finally, Section 5 concludes the paper and outlines future research directions.