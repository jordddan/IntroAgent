Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and speech recognition. In this paper, we address the problem of pre-training language models, which has been shown to be a crucial step in achieving state-of-the-art performance on various downstream tasks. Our research question is how to design a pre-training method that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations.

Our main contribution is the proposal of a new pre-training method called Masked and Permuted Language Modeling (MPNet). MPNet splits the tokens in a sequence into non-predicted and predicted parts, allowing it to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-trained MPNet on a large-scale text corpus and fine-tuned it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our experiments show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Our work builds upon previous research in the field of pre-training language models. CeMAT proposes a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. ProphetNet introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. BANG bridges autoregressive and non-autoregressive generation by designing a novel model structure for large-scale pretraining. JANUS is a joint autoregressive and non-autoregressive training method using an auxiliary loss to enhance the model performance in both AR and NAR manner simultaneously. CMLMC proposes the Conditional Masked Language Model with Correction that addresses the problems of indistinguishability of tokens and mismatch between training and inference. XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autoregressive formulation. 

The rest of the paper is organized as follows. Section 2 provides a detailed description of the proposed MPNet method. Section 3 presents the experimental setup and results. Section 4 discusses related work in the field of pre-training language models. Finally, Section 5 concludes the paper and outlines future research directions.