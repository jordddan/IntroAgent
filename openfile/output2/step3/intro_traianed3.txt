Machine translation has been a long-standing challenge in natural language processing, with significant progress made in recent years through the development of autoregressive (AR) and non-autoregressive (NAR) models. While AR models have achieved excellent performance, they suffer from slow decoding speed due to their sequential nature. On the other hand, NAR models can generate translations in parallel, but their performance lags behind AR models, especially without distillation data. To address these limitations, this paper proposes a novel method called JANUS, which combines the strengths of both AR and NAR models while avoiding their weaknesses.

The related works in this field are diverse, with various pre-training models proposed to improve the performance of NMT. For example, CeMAT is a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora, while BANG bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pre-training. Other works, such as CMLMC, investigate the reasons behind the performance gap between AR and NAR models and propose a conditional masked language model with correction to address these issues. MPNet leverages the dependency among predicted tokens through permuted language modeling, while XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order.

The proposed JANUS method is motivated by the need to improve the performance of both AR and NAR models while avoiding their limitations. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. The method is evaluated on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Furthermore, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

In summary, this paper proposes a novel method called JANUS that combines the strengths of both AR and NAR models while avoiding their weaknesses. The proposed method is evaluated on multiple NMT datasets and pretraining models, achieving significant improvements in performance. The contributions of this paper include introducing JANUS, proposing an auxiliary distribution P AUX, demonstrating the effectiveness of JANUS, and exceeding the non-autoregressive pretraining model BANG. The rest of the paper is organized as follows: Section 2 describes the proposed method in detail, Section 3 presents the experimental setup and results, and Section 4 concludes the paper.