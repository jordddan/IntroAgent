Machine learning has revolutionized the field of natural language processing (NLP) by enabling the development of models that can learn from large amounts of data and perform a wide range of tasks. One of the key challenges in NLP is pre-training models that can effectively capture the complex relationships between words in a sentence. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations.

We provide a comprehensive overview of related works in the field, including BERT, XLNet, and RoBERTa, and highlight their strengths and weaknesses. We also discuss recent approaches that aim to bridge the gap between autoregressive (AR) and non-autoregressive (NAR) generation, such as BANG and JANUS. We explain the importance of addressing the problem of pre-training models that can effectively capture the complex relationships between words in a sentence and provide a clear motivation for the proposed MPNet approach.

MPNet introduces a new approach that splits the tokens in a sequence into non-predicted and predicted parts, which allows the model to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-train MPNet on a large-scale text corpus and fine-tune it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Experimental results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

In summary, the main contributions of this paper are: proposing a new pre-training method called MPNet that unifies the advantages of both MLM and PLM while addressing their limitations, introducing a new approach that splits the tokens in a sequence into non-predicted and predicted parts, and pre-training MPNet on a large-scale text corpus and fine-tuning it on various downstream benchmark tasks, achieving state-of-the-art results. The rest of the paper is organized as follows: Section 2 provides a detailed description of the proposed MPNet approach, Section 3 presents the experimental setup and results, and Section 4 concludes the paper and discusses future work.