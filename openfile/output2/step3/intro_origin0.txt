Machine translation has become an essential tool in today's globalized world, enabling communication across different languages and cultures. However, achieving high-quality machine translation remains a challenging task, especially for low-resource languages. In this paper, we propose a novel pre-training model for machine translation, called CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.

Our work builds upon and extends previous research in the field of machine translation. Previous pre-training models have typically adopted an unidirectional decoder, while CeMAT demonstrates that pre-training a sequence-to-sequence model with a bidirectional decoder can produce notable performance gains for both AT and NAT. Other related works have proposed joint autoregressive and non-autoregressive training methods, bridging autoregressive and non-autoregressive generation, and improving non-autoregressive translation models without distillation. Our work contributes to this body of research by introducing novel techniques for enhancing the training of bidirectional decoders and demonstrating the effectiveness of our approach on a range of machine translation tasks.

In the following sections, we provide a detailed description of our proposed CeMAT model, the aligned code-switching & masking and dynamic dual-masking techniques, and the experimental results on various machine translation tasks. We conclude with a discussion of the implications of our work and directions for future research.