Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and robotics. In particular, sequence generation tasks, such as machine translation and text summarization, have benefited greatly from the recent advances in deep learning. However, there are still challenges in achieving high-quality generation while maintaining fast inference speed.

This paper addresses the problem of improving the performance of autoregressive (AR) and non-autoregressive (NAR) generation models while avoiding their weaknesses. To this end, we propose JANUS, a method that combines the strengths of both AR and NAR models and introduces an auxiliary distribution to bridge the discrepancy between them. Our approach achieves similar results to the state-of-the-art NAR model without distillation data and improves the AR model performance by more than 1.5 BLEU scores on average.

We compare our approach with several related works, including CeMAT, ProphetNet, BANG, CMLMC, MPNet, XLNet, and the Transformer model. CeMAT pre-trains a sequence-to-sequence model with a bidirectional decoder, while ProphetNet introduces a novel self-supervised objective for n-step ahead prediction. BANG bridges AR and NAR generation by designing a novel model structure for large-scale pretraining, while CMLMC addresses the problem of indistinguishability of tokens and mismatch between training and inference. MPNet leverages the dependency among predicted tokens through permuted language modeling, while XLNet enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. Finally, the Transformer model is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

In summary, our work proposes a novel approach to improve the performance of AR and NAR generation models, achieving state-of-the-art results on multiple datasets. The rest of the paper is organized as follows: Section 2 describes the related works in more detail, Section 3 presents the proposed JANUS method, Section 4 reports the experimental results, and Section 5 concludes the paper and discusses future work.