Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and robotics. In this paper, we focus on the problem of bridging the gap between autoregressive (AR) and non-autoregressive (NAR) generation, which has been a challenging task in the field of sequence generation. 

Our main objective is to propose a large-scale pretraining model, BANG, designed explicitly for NAR and semi-NAR generation. BANG is pretrained using an efficient cross-stream visible n-stream decoder to realize parallelization, which supports predicting tokens with arbitrary previous golden tokens or [MASK]. Moreover, BANG supports NAR, semi-NAR, and AR finetuning to meet different requirements with the same pretrained model structure. 

To the best of our knowledge, BANG is the first large-scale pretraining model designed explicitly for NAR and semi-NAR generation. Our experiments on question generation, summarization, and dialogue generation show that BANG improves NAR and semi-NAR performance significantly and attains comparable performance with strong AR pretrained models. 

In addition to our contribution, we also provide a brief overview of related works, including MPNet, XLNet, Universal Conditional Masked Language Pre-training, Attention Is All You Need, Improving Non-Autoregressive Translation Models Without Distillation, ProphetNet, and JANUS. These works have made significant contributions to the field of sequence generation and have inspired our approach. 

The rest of the paper is organized as follows. In Section 2, we provide a detailed description of the proposed BANG model. In Section 3, we present our experimental results and compare them with the state-of-the-art models. Finally, we conclude the paper in Section 4 and discuss future research directions.