Machine translation has become an essential tool for breaking down language barriers in today's globalized world. However, achieving high-quality translations remains a challenging task, especially for low-resource languages. In this paper, we propose a novel pre-training model for machine translation, CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages.

Our experiments demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. We compare our approach with several related works, including MPNet, XLNet, Attention Is All You Need, BANG, IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION, ProphetNet, and JANUS. Our results show that CeMAT outperforms these models in terms of translation quality and efficiency.

The rest of the paper is organized as follows. Section 2 provides a detailed description of the proposed CeMAT model and the aligned code-switching & masking and dynamic dual-masking techniques. Section 3 presents the experimental setup and results, followed by a discussion of the findings in Section 4. Finally, Section 5 concludes the paper and outlines future research directions.