Machine translation has become an essential tool for breaking down language barriers in today's globalized world. However, achieving high-quality translations remains a challenging task, especially for low-resource languages. In this paper, we propose a novel pre-training model for machine translation, CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. 

To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. 

We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes. Our proposed model outperforms the state-of-the-art pre-training models, including BERT, XLNet, and RoBERTa, under the same model setting. 

In summary, this paper presents a novel pre-training model, CeMAT, for machine translation that achieves state-of-the-art performance on both AT and NAT tasks. The proposed aligned code-switching & masking and dynamic dual-masking techniques further enhance the model training. The rest of the paper is organized as follows: Section 2 provides a detailed description of the proposed model and training techniques. Section 3 presents the experimental setup and results. Finally, Section 4 concludes the paper and discusses future work.