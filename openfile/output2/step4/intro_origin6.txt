Machine learning has become an essential tool in various fields, including natural language processing (NLP), computer vision, and robotics. In NLP, sequence generation tasks, such as machine translation and text summarization, have been widely studied. However, traditional autoregressive (AR) models that generate one token at a time can be time-consuming, especially for long sequences. Non-autoregressive (NAR) models that generate tokens in parallel have been proposed to address this issue, but they often lag behind AR models in terms of performance. 

In this paper, we propose JANUS, a method that combines the strengths of both AR and NAR models while avoiding their weaknesses to improve performance. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Furthermore, we exceed the non-autoregressive pretraining model BANG on the same GLGE tasks and achieve comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

To the best of our knowledge, JANUS is the first method that combines AR and NAR models in a joint training framework. Our approach can transfer the AR knowledge to NAR and alleviate the problem of distribution discrepancy. We compare our approach with related works, including MPNet, XLNet, CMLMC, ProphetNet, Attention Is All You Need, and Universal Conditional Masked Language Pre-training, and show that JANUS achieves state-of-the-art performance on multiple generation tasks. 

The rest of the paper is organized as follows. Section 2 provides a brief overview of related works. Section 3 describes the proposed JANUS method in detail. Section 4 presents the experimental results and analysis. Finally, Section 5 concludes the paper and discusses future work.