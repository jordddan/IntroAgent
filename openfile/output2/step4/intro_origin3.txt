Machine learning has become an essential tool in various fields, including natural language processing (NLP). In NLP, sequence-to-sequence (Seq2Seq) models have shown remarkable performance in tasks such as summarization and question generation. However, pre-training these models on large-scale datasets is computationally expensive and time-consuming. In this paper, we introduce a new large-scale pre-trained Seq2Seq model called ProphetNet, which achieves state-of-the-art results on several NLP benchmarks.

ProphetNet is pre-trained using a novel self-supervised objective called future n-gram prediction, which encourages the model to plan for future tokens and prevents overfitting on strong local correlations. We also develop a method to simultaneously predict the future n-gram at each time step during the training phase, which further improves the model's performance. Additionally, we extend the two-stream self-attention proposed in XLNet to n-stream self-attention, which includes a main stream self-attention and n extra self-attention predicting streams for future n-gram prediction.

We pre-train ProphetNet on two scale pre-trained datasets and achieve new state-of-the-art results on CNN/DailyMail and Gigaword, using only about 1/3 pre-training epochs of BART and about 1/5 pre-training corpus of T5 and PEGASUS. Furthermore, we fine-tune ProphetNet on several NLG tasks and achieve the best performance on CNN/DailyMail, Gigaword, and SQuAD 1.1 question generation tasks compared to the models using the same base scale pre-training dataset.

Related works in this field include MPNet, XLNet, and Universal Conditional Masked Language Pre-training, which have all shown significant improvements in pre-training Seq2Seq models. However, ProphetNet introduces a novel self-supervised objective and n-stream self-attention mechanism, which sets it apart from previous works. 

In this paper, we provide a detailed description of ProphetNet's architecture, pre-training, and fine-tuning procedures. We also present experimental results on several benchmarks, demonstrating the effectiveness of our approach. The rest of the paper is organized as follows: Section 2 provides an overview of related works, Section 3 describes the ProphetNet model in detail, Section 4 presents the pre-training and fine-tuning procedures, Section 5 reports experimental results, and Section 6 concludes the paper.