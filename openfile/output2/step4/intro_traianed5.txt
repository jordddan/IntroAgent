Machine translation is a fundamental task in natural language processing, with the aim of translating text from one language to another. Despite significant progress in recent years, machine translation still faces challenges in handling low-resource languages and achieving high-quality translations. In this paper, we propose a novel pre-training model for machine translation, called CeMAT, which addresses these challenges by leveraging both monolingual and bilingual corpora. 

The main contribution of this paper is threefold. Firstly, we introduce CeMAT, a pre-training model that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks. Secondly, we introduce aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders. These techniques are applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. Thirdly, we demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.

To motivate our approach, we review related work in the field of machine translation. We discuss the limitations of existing pre-training models, such as BERT and XLNet, which neglect dependency among predicted tokens and suffer from a pretrain-finetune discrepancy. We also review recent work on non-autoregressive translation models, which translate blocks of tokens in parallel to accelerate inference. However, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. We argue that our proposed approach addresses these limitations and provides a unified model for fine-tuning on both NMT tasks.

In this paper, we conduct extensive experiments to evaluate the effectiveness of CeMAT on various machine translation tasks. We show that CeMAT achieves significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low-resource and +7.9 BLEU on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. We also compare our approach with state-of-the-art models and show that CeMAT outperforms them consistently. 

In summary, this paper proposes a novel pre-training model for machine translation, which leverages both monolingual and bilingual corpora and introduces aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training. Our approach achieves significant performance improvement on various machine translation tasks and provides a unified model for fine-tuning on both NMT tasks. The rest of the paper is organized as follows: Section 2 describes the proposed approach in detail. Section 3 presents the experimental setup and results. Section 4 discusses the experimental results and provides an analysis. Finally, Section 5 concludes the paper and discusses future work.