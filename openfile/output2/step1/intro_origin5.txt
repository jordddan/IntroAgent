Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and robotics. In recent years, pre-training methods have shown remarkable success in improving the performance of machine learning models. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet) that unifies the advantages of both Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while addressing their limitations.

Our work addresses the limitations of previous pre-training methods by introducing a new approach that splits the tokens in a sequence into non-predicted and predicted parts. This approach allows MPNet to take into consideration the dependency among the predicted tokens through permuted language modeling and the position information of all tokens to alleviate the position discrepancy of XLNet. We pre-trained MPNet on a large-scale text corpus and fine-tuned it on various downstream benchmark tasks, including GLUE, SQuAD, RACE, and IMDB. Our results show that MPNet outperforms MLM and PLM by a large margin and previous well-known models BERT, XLNet, and RoBERTa by 4.8, 3.4, and 1.5 points, respectively, on GLUE dev sets under the same model setting.

Our work builds upon previous research in the field of pre-training, including XLNet, BERT, and ProphetNet. XLNet introduced permuted language modeling to address the limitations of BERT, while ProphetNet proposed a new self-supervised objective named future n-gram prediction. Our work extends these approaches by unifying the advantages of MLM and PLM and introducing a new approach that splits the tokens in a sequence into non-predicted and predicted parts.

The rest of the paper is organized as follows. Section 2 provides a detailed description of the MPNet pre-training method. Section 3 presents the experimental setup and results. Section 4 discusses the limitations and future directions of our work. Finally, Section 5 concludes the paper.