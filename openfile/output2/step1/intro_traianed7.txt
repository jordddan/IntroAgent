Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and robotics. In recent years, there has been a growing interest in developing more efficient and accurate machine translation models. The main objective of this paper is to propose a pre-training model for machine translation that can improve the performance of both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

The main contribution of this paper is the development of CeMAT, a pre-training model that consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both AT and NAT tasks. Additionally, this paper introduces aligned code-switching & masking and dynamic dual-masking techniques to enhance the model training under the setting of bidirectional decoders.

To provide a comprehensive overview of related work, this paper discusses several existing methods, including JANUS, XLNet, BANG, Attention Is All You Need, ProphetNet, MPNet, and CMLMC. The limitations of these methods are highlighted, and the proposed approach is compared to them. The proposed approach differs from existing methods in that it pre-trains a unified model for fine-tuning on both NMT tasks.

The proposed method is explained in detail, including the methodology used and the experimental setup. The effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings is demonstrated. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.

In conclusion, this paper proposes a pre-training model for machine translation that can improve the performance of both AT and NAT tasks. The proposed approach differs from existing methods in that it pre-trains a unified model for fine-tuning on both NMT tasks. The experimental results demonstrate the effectiveness of the proposed approach in improving machine translation performance.