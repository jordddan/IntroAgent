Machine translation has become an essential tool for breaking down language barriers in today's globalized world. However, achieving high-quality translations remains a challenging task, especially for low-resource languages. In this paper, we propose a novel pre-training model for machine translation, called CeMAT, which consists of a bidirectional encoder, a bidirectional decoder, and a cross-attention module for bridging them. CeMAT is pre-trained on both monolingual and bilingual corpora and can provide unified initialization parameters for both Autoregressive NMT (AT) and Non-autoregressive NMT (NAT) tasks.

To enhance the model training under the setting of bidirectional decoders, we introduce aligned code-switching & masking and dynamic dual-masking techniques. Aligned code-switching & masking is applied based on a multilingual translation dictionary and word alignment between source and target sentences, while dynamic dual-masking is used to mask the contents of the source and target languages. We demonstrate the effectiveness of CeMAT in improving machine translation performance on both low-resource and high-resource settings. CeMAT achieves consistent improvements over strong competitors in both AT and NAT tasks with data of varied sizes.

Our work builds upon recent advances in machine translation, including Transformer-based autoregressive and non-autoregressive models, generalized autoregressive pretraining, and masked language modeling. However, to the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages. 

The rest of the paper is organized as follows. Section 2 provides an overview of related work in machine translation and pre-training models. Section 3 describes the proposed CeMAT model in detail, including the aligned code-switching & masking and dynamic dual-masking techniques. Section 4 presents the experimental setup and results, followed by a discussion of the findings in Section 5. Finally, Section 6 concludes the paper and outlines directions for future research.