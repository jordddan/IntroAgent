Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and robotics. In recent years, there has been a growing interest in developing machine learning models that can generate sequences of data, such as text, speech, and music. One of the most popular approaches for sequence generation is autoregressive (AR) models, which generate one token at a time conditioned on the previously generated tokens. However, AR models suffer from slow inference speed and cannot generate sequences in parallel. Non-autoregressive (NAR) models, on the other hand, can generate sequences in parallel but often sacrifice the quality of the generated sequences.

In this paper, we propose a novel method called JANUS, which combines the strengths of both AR and NAR models while avoiding their weaknesses. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. We demonstrate the effectiveness of JANUS on multiple neural machine translation (NMT) datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Moreover, JANUS exceeds the non-autoregressive pretraining model BANG on the same GLGE tasks and achieves comparable performance with the AR manner at least two times speedup based on the iterative inference mechanism.

Our proposed method builds upon the existing literature on AR and NAR models. XLNet and BERT are two popular AR models that have achieved state-of-the-art performance on various NLP tasks. However, these models suffer from slow inference speed and cannot generate sequences in parallel. BANG is a recent NAR model that bridges the gap between AR and NAR generation by designing a novel model structure for large-scale pretraining. Attention Is All You Need is a seminal work that introduced the Transformer architecture, which has become the backbone of many state-of-the-art NMT models. ProphetNet and MPNet are two recent pre-training models that have achieved state-of-the-art performance on various NLP tasks. IMPROVING NON-AUTOREGRESSIVE TRANSLATION MODELS WITHOUT DISTILLATION and Universal Conditional Masked Language Pre-training are two recent works that have proposed novel methods to improve the performance of NAR models.

In this paper, we propose a novel method that combines the strengths of AR and NAR models while avoiding their weaknesses. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, which benefits AR and NAR to learn from each other. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving state-of-the-art performance without distillation data. Our proposed method advances the field of machine learning by providing a new approach to sequence generation that combines the strengths of AR and NAR models.