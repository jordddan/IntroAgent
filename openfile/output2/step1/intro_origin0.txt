Machine learning has become an essential tool in various fields, including natural language processing, computer vision, and robotics. In particular, sequence generation tasks, such as machine translation and text summarization, have benefited greatly from the development of autoregressive (AR) and non-autoregressive (NAR) models. AR models generate sequences one token at a time, while NAR models generate sequences in parallel, resulting in faster inference times. However, both models have their limitations, with AR models suffering from slow inference times and NAR models struggling to match the performance of AR models.

In this paper, we propose a novel method called JANUS, which combines the strengths of both AR and NAR models while avoiding their weaknesses. JANUS introduces an auxiliary distribution P AUX to bridge the discrepancy between P AR and P NAR, allowing AR and NAR models to learn from each other. We demonstrate the effectiveness of JANUS on multiple NMT datasets and autoregressive pretraining models, achieving similar results to the state-of-the-art NAR model without distillation data and improving the AR model performance by more than 1.5 BLEU scores on average. Additionally, we exceed the non-autoregressive pretraining model BANG on the same GLGE tasks and achieve comparable performance with the AR model at least two times speedup based on the iterative inference mechanism.

Our work builds upon previous research in the field, including XLNet, BANG, Attention Is All You Need, ProphetNet, MPNet, and Universal Conditional Masked Language Pre-training. XLNet introduced a generalized autoregressive pretraining method that overcomes the limitations of BERT, while BANG bridged the gap between AR and NAR generation. Attention Is All You Need proposed a new network architecture based solely on attention mechanisms, while ProphetNet introduced a novel self-supervised objective named future n-gram prediction. MPNet leveraged the dependency among predicted tokens through permuted language modeling, and Universal Conditional Masked Language Pre-training pre-trained a sequence-to-sequence model with a bidirectional decoder. Our work extends these approaches by introducing JANUS, a method that combines the strengths of AR and NAR models while avoiding their weaknesses.

The rest of the paper is organized as follows. Section 2 provides an overview of related work in the field of sequence generation. Section 3 describes the proposed JANUS method in detail. Section 4 presents experimental results on multiple NMT datasets and pretraining models. Finally, Section 5 concludes the paper and discusses future directions for research.