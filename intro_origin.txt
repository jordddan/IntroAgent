The field of natural language processing has seen significant advancements in recent years, with pre-training methods such as Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) leading the way. However, these methods have their limitations, such as the position discrepancy of XLNet and the lack of consideration for token dependencies in MLM. In this paper, we propose a new pre-training method called Masked and Permuted Language Modeling (MPNet), which unifies the advantages of both MLM and PLM while addressing their limitations. We pre-trained MPNet on a large-scale text corpus and fine-tuned it on various benchmark tasks, showing that it outperforms MLM and PLM by a large margin and also outperforms previous well-known models such as BERT, XLNet, and RoBERTa on the GLUE dev sets. Our contribution lies in the development of a novel pre-training method that takes into consideration both token dependencies and position information, leading to improved performance on downstream tasks. The rest of the paper is organized as follows: Section 2 provides a literature review, Section 3 describes the proposed MPNet method, Section 4 presents the experimental setup and results, and Section 5 concludes the paper.